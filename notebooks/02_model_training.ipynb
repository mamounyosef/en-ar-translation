{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "22ffe200",
      "metadata": {},
      "source": [
        "# EN-AR Model Training\n",
        "\n",
        "## Objective\n",
        "- Train a bidirectional EN <-> AR encoder-decoder model from random initialization.\n",
        "- Use the cleaned combined dataset exported by Notebook 01.\n",
        "\n",
        "## Scope\n",
        "- Notebook-first, micro-step implementation (1-2 short cells per step).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c46cad66",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: c:\\My Projects\\en-ar-translation\n",
            "Dataset path: c:\\My Projects\\en-ar-translation\\artifacts\\eda\\final_cleaned_combined_dataset.parquet\n",
            "CUDA available: True\n",
            "TrainConfig(max_seq_len=128, vocab_size=32000, train_ratio=0.9, val_ratio=0.05, test_ratio=0.05)\n"
          ]
        }
      ],
      "source": [
        "# Setup: imports and training constants\n",
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "candidate_roots = [Path.cwd(), Path.cwd().parent]\n",
        "PROJECT_ROOT = next((r for r in candidate_roots if (r / \"artifacts\").exists()), Path.cwd())\n",
        "DATA_PATH = PROJECT_ROOT / \"artifacts\" / \"eda\" / \"final_cleaned_combined_dataset.parquet\"\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    max_seq_len: int = 128\n",
        "    vocab_size: int = 32_000\n",
        "    train_ratio: float = 0.90\n",
        "    val_ratio: float = 0.05\n",
        "    test_ratio: float = 0.05\n",
        "\n",
        "config = TrainConfig()\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"Dataset path: {DATA_PATH}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "caa92828",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded dataset: c:\\My Projects\\en-ar-translation\\artifacts\\eda\\final_cleaned_combined_dataset.parquet\n",
            "Shape: (827576, 2)\n",
            "Columns: ['en', 'ar']\n"
          ]
        }
      ],
      "source": [
        "# Micro-step 2: load cleaned dataset and validate schema\n",
        "assert DATA_PATH.exists(), f\"Cleaned dataset not found: {DATA_PATH}\"\n",
        "df = pd.read_parquet(DATA_PATH)\n",
        "\n",
        "required_columns = [\"en\", \"ar\"]\n",
        "missing = [c for c in required_columns if c not in df.columns]\n",
        "assert not missing, f\"Missing required columns: {missing}\"\n",
        "\n",
        "print(f\"Loaded dataset: {DATA_PATH}\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c6e9cec1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows before cleaning: 827,576\n",
            "Rows after cleaning: 827,546\n",
            "Rows removed: 30\n"
          ]
        }
      ],
      "source": [
        "# Micro-step 3: remove invalid/empty rows and report before/after\n",
        "rows_before = len(df)\n",
        "\n",
        "df = df.dropna(subset=[\"en\", \"ar\"]).copy()\n",
        "df[\"en\"] = df[\"en\"].astype(str).str.strip()\n",
        "df[\"ar\"] = df[\"ar\"].astype(str).str.strip()\n",
        "df = df[(df[\"en\"] != \"\") & (df[\"ar\"] != \"\")].reset_index(drop=True)\n",
        "\n",
        "rows_after = len(df)\n",
        "rows_removed = rows_before - rows_after\n",
        "print(f\"Rows before cleaning: {rows_before:,}\")\n",
        "print(f\"Rows after cleaning: {rows_after:,}\")\n",
        "print(f\"Rows removed: {rows_removed:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5032acd3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>en</th>\n",
              "      <th>ar</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>he was killed in the plane crash</td>\n",
              "      <td>.. مات في حادثة تحطم الطائرة</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>where did you get it</td>\n",
              "      <td>من أين حصلتم عليه ؟</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>an illustration of a gorilla flying through th...</td>\n",
              "      <td>« مثل غوريلا تطير في الهواء وخلفه سماء المدينة ».</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>He narrowly lost the final to Dieter Baumann o...</td>\n",
              "      <td>خسر المباراة بفارق ضئيل أمام ديتر بومان من ألم...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>oh i see what you mean sir</td>\n",
              "      <td>فهمت ما تقصد</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>It was the last album recorded by the group be...</td>\n",
              "      <td>كان هذا آخر ألبوم قامت المجموعة بتسجيله قبل تف...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Now you can not worry that the Christmas tree ...</td>\n",
              "      <td>الآن لا يمكنك أن تقلق بشأن شجرة عيد الميلاد لت...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>former world champions</td>\n",
              "      <td>بطولات وطنية سابقا.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Third-party tools enable one to build a variet...</td>\n",
              "      <td>أدوات الطرف الثالث تمكن المرء من بناء مجموعة م...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>not a one</td>\n",
              "      <td>ولا واحد</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  en  \\\n",
              "0                   he was killed in the plane crash   \n",
              "1                               where did you get it   \n",
              "2  an illustration of a gorilla flying through th...   \n",
              "3  He narrowly lost the final to Dieter Baumann o...   \n",
              "4                         oh i see what you mean sir   \n",
              "5  It was the last album recorded by the group be...   \n",
              "6  Now you can not worry that the Christmas tree ...   \n",
              "7                             former world champions   \n",
              "8  Third-party tools enable one to build a variet...   \n",
              "9                                          not a one   \n",
              "\n",
              "                                                  ar  \n",
              "0                       .. مات في حادثة تحطم الطائرة  \n",
              "1                                من أين حصلتم عليه ؟  \n",
              "2  « مثل غوريلا تطير في الهواء وخلفه سماء المدينة ».  \n",
              "3  خسر المباراة بفارق ضئيل أمام ديتر بومان من ألم...  \n",
              "4                                       فهمت ما تقصد  \n",
              "5  كان هذا آخر ألبوم قامت المجموعة بتسجيله قبل تف...  \n",
              "6  الآن لا يمكنك أن تقلق بشأن شجرة عيد الميلاد لت...  \n",
              "7                                بطولات وطنية سابقا.  \n",
              "8  أدوات الطرف الثالث تمكن المرء من بناء مجموعة م...  \n",
              "9                                           ولا واحد  "
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Quick check: 10 random rows from current dataset\n",
        "df.sample(n=10)[[\"en\", \"ar\"]].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f1f6f191",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>split</th>\n",
              "      <th>rows</th>\n",
              "      <th>ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>train</td>\n",
              "      <td>744926</td>\n",
              "      <td>0.9002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>val</td>\n",
              "      <td>41445</td>\n",
              "      <td>0.0501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>test</td>\n",
              "      <td>41175</td>\n",
              "      <td>0.0498</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   split    rows   ratio\n",
              "0  train  744926  0.9002\n",
              "1    val   41445  0.0501\n",
              "2   test   41175  0.0498"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Micro-step 4: deterministic hash split (90/5/5) with leakage guard\n",
        "assert abs((config.train_ratio + config.val_ratio + config.test_ratio) - 1.0) < 1e-9, \"Split ratios must sum to 1.0\"\n",
        "\n",
        "pair_hash = pd.util.hash_pandas_object(df[[\"en\", \"ar\"]], index=False).astype(\"uint64\")\n",
        "u = pair_hash / np.float64(2**64)\n",
        "\n",
        "train_cut = config.train_ratio\n",
        "val_cut = config.train_ratio + config.val_ratio\n",
        "df[\"split\"] = np.where(u < train_cut, \"train\", np.where(u < val_cut, \"val\", \"test\"))\n",
        "\n",
        "leak_count = int((df.groupby([\"en\", \"ar\"])[\"split\"].nunique() > 1).sum())\n",
        "assert leak_count == 0, f\"Leakage detected across splits for {leak_count} pairs\"\n",
        "\n",
        "split_counts = df[\"split\"].value_counts().rename_axis(\"split\").reset_index(name=\"rows\")\n",
        "split_counts[\"ratio\"] = (split_counts[\"rows\"] / len(df)).round(4)\n",
        "split_counts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "97237a4c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base rows: 827,546\n",
            "Bidirectional rows: 1,655,092\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>split</th>\n",
              "      <th>direction</th>\n",
              "      <th>rows</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>test</td>\n",
              "      <td>ar_to_en</td>\n",
              "      <td>41175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>test</td>\n",
              "      <td>en_to_ar</td>\n",
              "      <td>41175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>train</td>\n",
              "      <td>ar_to_en</td>\n",
              "      <td>744926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>train</td>\n",
              "      <td>en_to_ar</td>\n",
              "      <td>744926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>val</td>\n",
              "      <td>ar_to_en</td>\n",
              "      <td>41445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>val</td>\n",
              "      <td>en_to_ar</td>\n",
              "      <td>41445</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   split direction    rows\n",
              "0   test  ar_to_en   41175\n",
              "1   test  en_to_ar   41175\n",
              "2  train  ar_to_en  744926\n",
              "3  train  en_to_ar  744926\n",
              "4    val  ar_to_en   41445\n",
              "5    val  en_to_ar   41445"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Micro-step 5: build bidirectional rows with direction tokens\n",
        "required_split_cols = [\"en\", \"ar\", \"split\"]\n",
        "missing_split_cols = [c for c in required_split_cols if c not in df.columns]\n",
        "assert not missing_split_cols, f\"Missing columns before bidirectional build: {missing_split_cols}\"\n",
        "\n",
        "df_en_to_ar = pd.DataFrame({\n",
        "    \"source_text\": \"<2ar> \" + df[\"en\"],\n",
        "    \"target_text\": df[\"ar\"],\n",
        "    \"direction\": \"en_to_ar\",\n",
        "    \"split\": df[\"split\"],\n",
        "})\n",
        "\n",
        "df_ar_to_en = pd.DataFrame({\n",
        "    \"source_text\": \"<2en> \" + df[\"ar\"],\n",
        "    \"target_text\": df[\"en\"],\n",
        "    \"direction\": \"ar_to_en\",\n",
        "    \"split\": df[\"split\"],\n",
        "})\n",
        "\n",
        "df_bi = pd.concat([df_en_to_ar, df_ar_to_en], ignore_index=True)\n",
        "\n",
        "print(f\"Base rows: {len(df):,}\")\n",
        "print(f\"Bidirectional rows: {len(df_bi):,}\")\n",
        "\n",
        "direction_split_counts = (\n",
        "    df_bi.groupby([\"split\", \"direction\"]).size().reset_index(name=\"rows\")\n",
        ")\n",
        "direction_split_counts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "89f344a6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train rows: 1,489,852\n",
            "val rows: 82,890\n",
            "test rows: 82,350\n"
          ]
        }
      ],
      "source": [
        "# Micro-step 6: create train/val/test views from bidirectional dataset\n",
        "required_bi_cols = [\"source_text\", \"target_text\", \"direction\", \"split\"]\n",
        "missing_bi_cols = [c for c in required_bi_cols if c not in df_bi.columns]\n",
        "assert not missing_bi_cols, f\"Missing columns in df_bi: {missing_bi_cols}\"\n",
        "\n",
        "train_df = df_bi[df_bi[\"split\"] == \"train\"].reset_index(drop=True)\n",
        "val_df = df_bi[df_bi[\"split\"] == \"val\"].reset_index(drop=True)\n",
        "test_df = df_bi[df_bi[\"split\"] == \"test\"].reset_index(drop=True)\n",
        "\n",
        "assert len(train_df) + len(val_df) + len(test_df) == len(df_bi), \"Split size mismatch\"\n",
        "assert len(train_df) > 0 and len(val_df) > 0 and len(test_df) > 0, \"One split is empty\"\n",
        "\n",
        "print(f\"train rows: {len(train_df):,}\")\n",
        "print(f\"val rows: {len(val_df):,}\")\n",
        "print(f\"test rows: {len(test_df):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "78765424",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer output path: c:\\My Projects\\en-ar-translation\\artifacts\\tokenizer\\en_ar_bpe_tokenizer.json\n",
            "Special tokens: ['<pad>', '<s>', '</s>', '<unk>', '<2ar>', '<2en>']\n",
            "Tokenizer mode: ByteLevel BPE\n",
            "Train rows for tokenizer: 1,489,852\n",
            "Approx lines seen by tokenizer iterator: 2,979,704\n"
          ]
        }
      ],
      "source": [
        "# Micro-step 7: tokenizer setup (train split only)\n",
        "try:\n",
        "    from tokenizers import Tokenizer, decoders, models, pre_tokenizers, trainers\n",
        "except ImportError as e:\n",
        "    raise ImportError(\"`tokenizers` is required. Install with: pip install tokenizers\") from e\n",
        "\n",
        "TOKENIZER_DIR = PROJECT_ROOT / \"artifacts\" / \"tokenizer\"\n",
        "TOKENIZER_DIR.mkdir(parents=True, exist_ok=True)\n",
        "TOKENIZER_PATH = TOKENIZER_DIR / \"en_ar_bpe_tokenizer.json\"\n",
        "\n",
        "SPECIAL_TOKENS = [\"<pad>\", \"<s>\", \"</s>\", \"<unk>\", \"<2ar>\", \"<2en>\"]\n",
        "\n",
        "def train_corpus_iterator(df_in):\n",
        "    for row in df_in.itertuples(index=False):\n",
        "        yield row.source_text\n",
        "        yield row.target_text\n",
        "\n",
        "print(f\"Tokenizer output path: {TOKENIZER_PATH}\")\n",
        "print(f\"Special tokens: {SPECIAL_TOKENS}\")\n",
        "print(\"Tokenizer mode: ByteLevel BPE\")\n",
        "print(f\"Train rows for tokenizer: {len(train_df):,}\")\n",
        "print(f\"Approx lines seen by tokenizer iterator: {len(train_df) * 2:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "4cb5dac7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trained tokenizer vocab size: 32,000\n",
            "Saved tokenizer to: c:\\My Projects\\en-ar-translation\\artifacts\\tokenizer\\en_ar_bpe_tokenizer.json\n"
          ]
        }
      ],
      "source": [
        "# Micro-step 8: train and save shared ByteLevel BPE tokenizer\n",
        "tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
        "tokenizer.decoder = decoders.ByteLevel()\n",
        "\n",
        "trainer = trainers.BpeTrainer(\n",
        "    vocab_size=config.vocab_size,\n",
        "    special_tokens=SPECIAL_TOKENS,\n",
        "    min_frequency=2,\n",
        "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n",
        ")\n",
        "\n",
        "tokenizer.train_from_iterator(train_corpus_iterator(train_df), trainer=trainer)\n",
        "tokenizer.save(str(TOKENIZER_PATH))\n",
        "\n",
        "vocab_size_trained = tokenizer.get_vocab_size()\n",
        "print(f\"Trained tokenizer vocab size: {vocab_size_trained:,}\")\n",
        "print(f\"Saved tokenizer to: {TOKENIZER_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "c7e854cc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Special token IDs: {'<pad>': 0, '<s>': 1, '</s>': 2, '<unk>': 3, '<2ar>': 4, '<2en>': 5}\n",
            "Direction token probes OK (<2ar>/<2en> preserved).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>metric</th>\n",
              "      <th>value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sample_rows</td>\n",
              "      <td>20000.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>trained_vocab_size</td>\n",
              "      <td>32000.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>src_len_p50</td>\n",
              "      <td>9.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>src_len_p90</td>\n",
              "      <td>18.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>src_len_p95</td>\n",
              "      <td>21.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>src_len_p99</td>\n",
              "      <td>28.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>tgt_len_p50</td>\n",
              "      <td>8.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>tgt_len_p90</td>\n",
              "      <td>17.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>tgt_len_p95</td>\n",
              "      <td>20.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>tgt_len_p99</td>\n",
              "      <td>28.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>src_trunc_ratio_gt_max_len</td>\n",
              "      <td>0.0001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>tgt_trunc_ratio_gt_max_len</td>\n",
              "      <td>0.0001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>either_trunc_ratio_gt_max_len</td>\n",
              "      <td>0.0002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>src_unk_tokens</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>tgt_unk_tokens</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           metric       value\n",
              "0                     sample_rows  20000.0000\n",
              "1              trained_vocab_size  32000.0000\n",
              "2                     src_len_p50      9.0000\n",
              "3                     src_len_p90     18.0000\n",
              "4                     src_len_p95     21.0000\n",
              "5                     src_len_p99     28.0000\n",
              "6                     tgt_len_p50      8.0000\n",
              "7                     tgt_len_p90     17.0000\n",
              "8                     tgt_len_p95     20.0000\n",
              "9                     tgt_len_p99     28.0000\n",
              "10     src_trunc_ratio_gt_max_len      0.0001\n",
              "11     tgt_trunc_ratio_gt_max_len      0.0001\n",
              "12  either_trunc_ratio_gt_max_len      0.0002\n",
              "13                 src_unk_tokens      0.0000\n",
              "14                 tgt_unk_tokens      0.0000"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Micro-step 9: tokenizer audit (critical checks)\n",
        "from collections import Counter\n",
        "\n",
        "# Ensure tokenizer object is available (reload from disk if needed)\n",
        "if \"tokenizer\" not in globals():\n",
        "    tokenizer = Tokenizer.from_file(str(TOKENIZER_PATH))\n",
        "\n",
        "# 1) Special-token integrity\n",
        "special_token_ids = {tok: tokenizer.token_to_id(tok) for tok in SPECIAL_TOKENS}\n",
        "missing_special = [tok for tok, tid in special_token_ids.items() if tid is None]\n",
        "assert not missing_special, f\"Missing special tokens in tokenizer vocab: {missing_special}\"\n",
        "\n",
        "id_2ar = special_token_ids[\"<2ar>\"]\n",
        "id_2en = special_token_ids[\"<2en>\"]\n",
        "probe_2ar = tokenizer.encode(\"<2ar> this is a test\")\n",
        "probe_2en = tokenizer.encode(\"<2en> english direction probe\")\n",
        "assert len(probe_2ar.ids) > 0 and probe_2ar.ids[0] == id_2ar, \"<2ar> is not preserved as first token\"\n",
        "assert len(probe_2en.ids) > 0 and probe_2en.ids[0] == id_2en, \"<2en> is not preserved as first token\"\n",
        "\n",
        "# 2) Sample-based token-length and truncation audit (real tokenizer lengths)\n",
        "audit_sample_size = min(20000, len(df_bi))\n",
        "audit_df = df_bi.sample(n=audit_sample_size, random_state=RANDOM_SEED) if len(df_bi) > audit_sample_size else df_bi\n",
        "\n",
        "src_lengths = []\n",
        "tgt_lengths = []\n",
        "unk_counter = Counter()\n",
        "unk_id = special_token_ids[\"<unk>\"]\n",
        "\n",
        "for row in audit_df.itertuples(index=False):\n",
        "    src_ids = tokenizer.encode(row.source_text).ids\n",
        "    tgt_ids = tokenizer.encode(row.target_text).ids\n",
        "    src_lengths.append(len(src_ids))\n",
        "    tgt_lengths.append(len(tgt_ids))\n",
        "    unk_counter[\"src_unk_tokens\"] += sum(1 for i in src_ids if i == unk_id)\n",
        "    unk_counter[\"tgt_unk_tokens\"] += sum(1 for i in tgt_ids if i == unk_id)\n",
        "\n",
        "src_lengths = np.array(src_lengths, dtype=np.int32)\n",
        "tgt_lengths = np.array(tgt_lengths, dtype=np.int32)\n",
        "\n",
        "tokenizer_audit = pd.DataFrame({\n",
        "    \"metric\": [\n",
        "        \"sample_rows\",\n",
        "        \"trained_vocab_size\",\n",
        "        \"src_len_p50\", \"src_len_p90\", \"src_len_p95\", \"src_len_p99\",\n",
        "        \"tgt_len_p50\", \"tgt_len_p90\", \"tgt_len_p95\", \"tgt_len_p99\",\n",
        "        \"src_trunc_ratio_gt_max_len\",\n",
        "        \"tgt_trunc_ratio_gt_max_len\",\n",
        "        \"either_trunc_ratio_gt_max_len\",\n",
        "        \"src_unk_tokens\",\n",
        "        \"tgt_unk_tokens\",\n",
        "    ],\n",
        "    \"value\": [\n",
        "        int(len(audit_df)),\n",
        "        int(tokenizer.get_vocab_size()),\n",
        "        int(np.percentile(src_lengths, 50)), int(np.percentile(src_lengths, 90)), int(np.percentile(src_lengths, 95)), int(np.percentile(src_lengths, 99)),\n",
        "        int(np.percentile(tgt_lengths, 50)), int(np.percentile(tgt_lengths, 90)), int(np.percentile(tgt_lengths, 95)), int(np.percentile(tgt_lengths, 99)),\n",
        "        float((src_lengths > config.max_seq_len).mean()),\n",
        "        float((tgt_lengths > config.max_seq_len).mean()),\n",
        "        float(((src_lengths > config.max_seq_len) | (tgt_lengths > config.max_seq_len)).mean()),\n",
        "        int(unk_counter[\"src_unk_tokens\"]),\n",
        "        int(unk_counter[\"tgt_unk_tokens\"]),\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"Special token IDs:\", special_token_ids)\n",
        "print(\"Direction token probes OK (<2ar>/<2en> preserved).\")\n",
        "tokenizer_audit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ff47333",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved HF tokenizer to: c:\\My Projects\\en-ar-translation\\artifacts\\tokenizer\\hf_tokenizer\n",
            "Token IDs: {'<pad>': 0, '<s>': 1, '</s>': 2, '<unk>': 3, '<2ar>': 4, '<2en>': 5}\n",
            "HF vocab size: 32,000\n"
          ]
        }
      ],
      "source": [
        "# Micro-step 10: build and save Hugging Face compatible fast tokenizer\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "HF_TOKENIZER_DIR = TOKENIZER_DIR / \"hf_tokenizer\"\n",
        "HF_TOKENIZER_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "hf_tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_file=str(TOKENIZER_PATH),\n",
        "    bos_token=\"<s>\",\n",
        "    eos_token=\"</s>\",\n",
        "    unk_token=\"<unk>\",\n",
        "    pad_token=\"<pad>\",\n",
        "    additional_special_tokens=[\"<2ar>\", \"<2en>\"],\n",
        ")\n",
        "\n",
        "# Verify essential token ids exist\n",
        "required_tokens = [\"<pad>\", \"<s>\", \"</s>\", \"<unk>\", \"<2ar>\", \"<2en>\"]\n",
        "token_id_map = {tok: hf_tokenizer.convert_tokens_to_ids(tok) for tok in required_tokens}\n",
        "missing_ids = [tok for tok, tid in token_id_map.items() if tid is None or tid < 0]\n",
        "assert not missing_ids, f\"Missing token ids in hf_tokenizer: {missing_ids}\"\n",
        "\n",
        "hf_tokenizer.save_pretrained(str(HF_TOKENIZER_DIR))\n",
        "\n",
        "print(f\"Saved HF tokenizer to: {HF_TOKENIZER_DIR}\")\n",
        "print(\"Token IDs:\", token_id_map)\n",
        "print(f\"HF vocab size: {hf_tokenizer.vocab_size:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "21169324",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2607401e4a034cc5b8c2f8d732abe8a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train:   0%|          | 0/1489852 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e38bb5a4780d4f9ca2979ba07f5d260a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing val:   0%|          | 0/82890 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3f4e8b03a5f45ac92a8a900345b3f85",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing test:   0%|          | 0/82350 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized train rows: 1,489,852\n",
            "Tokenized val rows: 82,890\n",
            "Tokenized test rows: 82,350\n",
            "dict_keys(['source_text', 'target_text', 'direction', 'split', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n"
          ]
        }
      ],
      "source": [
        "# Micro-step 11: tokenize train/val/test (truncation only, no static padding)\n",
        "from datasets import Dataset\n",
        "\n",
        "def tokenize_batch(batch):\n",
        "    src = hf_tokenizer(\n",
        "        batch[\"source_text\"],\n",
        "        truncation=True,\n",
        "        max_length=config.max_seq_len,\n",
        "        padding=False,\n",
        "    )\n",
        "    tgt = hf_tokenizer(\n",
        "        batch[\"target_text\"],\n",
        "        truncation=True,\n",
        "        max_length=config.max_seq_len,\n",
        "        padding=False,\n",
        "    )\n",
        "    src[\"labels\"] = tgt[\"input_ids\"]\n",
        "    return src\n",
        "\n",
        "train_ds = Dataset.from_pandas(train_df[[\"source_text\", \"target_text\", \"direction\", \"split\"]], preserve_index=False)\n",
        "val_ds = Dataset.from_pandas(val_df[[\"source_text\", \"target_text\", \"direction\", \"split\"]], preserve_index=False)\n",
        "test_ds = Dataset.from_pandas(test_df[[\"source_text\", \"target_text\", \"direction\", \"split\"]], preserve_index=False)\n",
        "\n",
        "train_tok = train_ds.map(tokenize_batch, batched=True, desc=\"Tokenizing train\")\n",
        "val_tok = val_ds.map(tokenize_batch, batched=True, desc=\"Tokenizing val\")\n",
        "test_tok = test_ds.map(tokenize_batch, batched=True, desc=\"Tokenizing test\")\n",
        "\n",
        "print(f\"Tokenized train rows: {len(train_tok):,}\")\n",
        "print(f\"Tokenized val rows: {len(val_tok):,}\")\n",
        "print(f\"Tokenized test rows: {len(test_tok):,}\")\n",
        "print(train_tok[0].keys())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "b4c5041c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probe batch size: 32\n",
            "input_ids shape: (32, 74)\n",
            "attention_mask shape: (32, 74)\n",
            "labels shape: (32, 37)\n",
            "Dynamic padded source length (batch max): 74\n",
            "Dynamic padded target length (batch max): 37\n",
            "Configured truncation cap: 128\n"
          ]
        }
      ],
      "source": [
        "# Micro-step 12: dynamic padding collator + one-batch sanity check\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "model_input_cols = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
        "train_tok_model = train_tok.remove_columns([c for c in train_tok.column_names if c not in model_input_cols])\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=hf_tokenizer,\n",
        "    model=None,\n",
        "    padding=\"longest\",\n",
        "    label_pad_token_id=-100,\n",
        ")\n",
        "\n",
        "batch_size_probe = min(32, len(train_tok_model))\n",
        "probe_ds = train_tok_model.shuffle(seed=RANDOM_SEED).select(range(batch_size_probe))\n",
        "probe_features = [probe_ds[i] for i in range(len(probe_ds))]\n",
        "probe_batch = data_collator(probe_features)\n",
        "\n",
        "print(f\"Probe batch size: {batch_size_probe}\")\n",
        "print(f\"input_ids shape: {tuple(probe_batch['input_ids'].shape)}\")\n",
        "print(f\"attention_mask shape: {tuple(probe_batch['attention_mask'].shape)}\")\n",
        "print(f\"labels shape: {tuple(probe_batch['labels'].shape)}\")\n",
        "print(f\"Dynamic padded source length (batch max): {probe_batch['input_ids'].shape[1]}\")\n",
        "print(f\"Dynamic padded target length (batch max): {probe_batch['labels'].shape[1]}\")\n",
        "print(f\"Configured truncation cap: {config.max_seq_len}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "59accefc",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>direction</th>\n",
              "      <th>source_text</th>\n",
              "      <th>target_text</th>\n",
              "      <th>source_len</th>\n",
              "      <th>target_len</th>\n",
              "      <th>source_ids</th>\n",
              "      <th>target_ids</th>\n",
              "      <th>decoded_source_from_ids</th>\n",
              "      <th>decoded_target_from_ids</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ar_to_en</td>\n",
              "      <td>&lt;2en&gt; وقد أصيب جمهور نيويورك بالصدمة ولكنهم ما...</td>\n",
              "      <td>New York audiences were shocked but still atte...</td>\n",
              "      <td>20</td>\n",
              "      <td>15</td>\n",
              "      <td>[5, 3113, 15546, 11104, 5004, 457, 2635, 596, ...</td>\n",
              "      <td>[11837, 4674, 19580, 3875, 690, 25314, 883, 17...</td>\n",
              "      <td>&lt;2en&gt; وقد أصيب جمهور نيويورك بالصدمة ولكنهم ما...</td>\n",
              "      <td>New York audiences were shocked but still atte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ar_to_en</td>\n",
              "      <td>&lt;2en&gt; أظن أن هذا ما يهم توقعتم العكس</td>\n",
              "      <td>you know im gonna just assume thats implied</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>[5, 3755, 404, 504, 546, 7117, 20641, 606, 18926]</td>\n",
              "      <td>[781, 673, 587, 1354, 798, 26047, 1241, 14061,...</td>\n",
              "      <td>&lt;2en&gt; أظن أن هذا ما يهم توقعتم العكس</td>\n",
              "      <td>you know im gonna just assume thats implied</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ar_to_en</td>\n",
              "      <td>&lt;2en&gt; مرحبا (بيل )</td>\n",
              "      <td>hey bill</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>[5, 3295, 564, 3674, 2153]</td>\n",
              "      <td>[2715, 5751]</td>\n",
              "      <td>&lt;2en&gt; مرحبا (بيل )</td>\n",
              "      <td>hey bill</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>en_to_ar</td>\n",
              "      <td>&lt;2ar&gt; He also helped the girl to remember her ...</td>\n",
              "      <td>كما ساعد الفتاة على تذكر اسمها الحقيقي: إميلي ...</td>\n",
              "      <td>18</td>\n",
              "      <td>14</td>\n",
              "      <td>[4, 1050, 778, 8449, 295, 1217, 342, 3660, 693...</td>\n",
              "      <td>[1941, 6114, 5954, 378, 7557, 8747, 9087, 31, ...</td>\n",
              "      <td>&lt;2ar&gt; He also helped the girl to remember her ...</td>\n",
              "      <td>كما ساعد الفتاة على تذكر اسمها الحقيقي: إميلي ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>en_to_ar</td>\n",
              "      <td>&lt;2ar&gt; theyre even in his butt crack</td>\n",
              "      <td>وهم حتى في بلده بعقب الكراك.</td>\n",
              "      <td>7</td>\n",
              "      <td>10</td>\n",
              "      <td>[4, 3530, 1850, 326, 632, 15474, 12257]</td>\n",
              "      <td>[17421, 1658, 325, 1387, 2412, 3444, 1744, 563...</td>\n",
              "      <td>&lt;2ar&gt; theyre even in his butt crack</td>\n",
              "      <td>وهم حتى في بلده بعقب الكراك.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  direction                                        source_text  \\\n",
              "0  ar_to_en  <2en> وقد أصيب جمهور نيويورك بالصدمة ولكنهم ما...   \n",
              "1  ar_to_en               <2en> أظن أن هذا ما يهم توقعتم العكس   \n",
              "2  ar_to_en                                 <2en> مرحبا (بيل )   \n",
              "3  en_to_ar  <2ar> He also helped the girl to remember her ...   \n",
              "4  en_to_ar                <2ar> theyre even in his butt crack   \n",
              "\n",
              "                                         target_text  source_len  target_len  \\\n",
              "0  New York audiences were shocked but still atte...          20          15   \n",
              "1        you know im gonna just assume thats implied           9           9   \n",
              "2                                           hey bill           5           2   \n",
              "3  كما ساعد الفتاة على تذكر اسمها الحقيقي: إميلي ...          18          14   \n",
              "4                       وهم حتى في بلده بعقب الكراك.           7          10   \n",
              "\n",
              "                                          source_ids  \\\n",
              "0  [5, 3113, 15546, 11104, 5004, 457, 2635, 596, ...   \n",
              "1  [5, 3755, 404, 504, 546, 7117, 20641, 606, 18926]   \n",
              "2                         [5, 3295, 564, 3674, 2153]   \n",
              "3  [4, 1050, 778, 8449, 295, 1217, 342, 3660, 693...   \n",
              "4            [4, 3530, 1850, 326, 632, 15474, 12257]   \n",
              "\n",
              "                                          target_ids  \\\n",
              "0  [11837, 4674, 19580, 3875, 690, 25314, 883, 17...   \n",
              "1  [781, 673, 587, 1354, 798, 26047, 1241, 14061,...   \n",
              "2                                       [2715, 5751]   \n",
              "3  [1941, 6114, 5954, 378, 7557, 8747, 9087, 31, ...   \n",
              "4  [17421, 1658, 325, 1387, 2412, 3444, 1744, 563...   \n",
              "\n",
              "                             decoded_source_from_ids  \\\n",
              "0  <2en> وقد أصيب جمهور نيويورك بالصدمة ولكنهم ما...   \n",
              "1               <2en> أظن أن هذا ما يهم توقعتم العكس   \n",
              "2                                 <2en> مرحبا (بيل )   \n",
              "3  <2ar> He also helped the girl to remember her ...   \n",
              "4                <2ar> theyre even in his butt crack   \n",
              "\n",
              "                             decoded_target_from_ids  \n",
              "0  New York audiences were shocked but still atte...  \n",
              "1        you know im gonna just assume thats implied  \n",
              "2                                           hey bill  \n",
              "3  كما ساعد الفتاة على تذكر اسمها الحقيقي: إميلي ...  \n",
              "4                       وهم حتى في بلده بعقب الكراك.  "
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Micro-step 13: manual audit of random final tokenized examples\n",
        "sample_n = min(5, len(train_tok))\n",
        "audit_ds = train_tok.shuffle().select(range(sample_n))\n",
        "\n",
        "audit_rows = []\n",
        "for item in audit_ds:\n",
        "    src_ids = item[\"input_ids\"]\n",
        "    lbl_ids = item[\"labels\"]\n",
        "    audit_rows.append({\n",
        "        \"direction\": item.get(\"direction\", \"\"),\n",
        "        \"source_text\": item[\"source_text\"],\n",
        "        \"target_text\": item[\"target_text\"],\n",
        "        \"source_len\": len(src_ids),\n",
        "        \"target_len\": len(lbl_ids),\n",
        "        \"source_ids\": src_ids,\n",
        "        \"target_ids\": lbl_ids,\n",
        "        \"decoded_source_from_ids\": hf_tokenizer.decode(src_ids, skip_special_tokens=False),\n",
        "        \"decoded_target_from_ids\": hf_tokenizer.decode(lbl_ids, skip_special_tokens=False),\n",
        "    })\n",
        "\n",
        "tokenized_audit_df = pd.DataFrame(audit_rows)\n",
        "tokenized_audit_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "986a8afd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Direction : ar_to_en\n",
            "Source    : <2en> وقد أصيب جمهور نيويورك بالصدمة ولكنهم ما زالوا يحضرون المسرحية وجعلوا العرض مشهورا.\n",
            "Target    : New York audiences were shocked but still attended and made the play popular.\n",
            "Input IDs : [5, 3113, 15546, 11104, 5004, 457, 2635, 596, 29822, 546, 9927, 523, 13691, 13528, 1081, 576, 523, 2886, 24474, 19]\n",
            "Labels    : [11837, 4674, 19580, 3875, 690, 25314, 883, 1762, 5712, 355, 1502, 295, 848, 3777, 19]\n",
            "Attn Mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Decoded S : <2en> وقد أصيب جمهور نيويورك بالصدمة ولكنهم ما زالوا يحضرون المسرحية وجعلوا العرض مشهورا.\n",
            "Decoded T : New York audiences were shocked but still attended and made the play popular.\n",
            "============================================================\n",
            "Direction : ar_to_en\n",
            "Source    : <2en> أظن أن هذا ما يهم توقعتم العكس\n",
            "Target    : you know im gonna just assume thats implied\n",
            "Input IDs : [5, 3755, 404, 504, 546, 7117, 20641, 606, 18926]\n",
            "Labels    : [781, 673, 587, 1354, 798, 26047, 1241, 14061, 1319]\n",
            "Attn Mask : [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Decoded S : <2en> أظن أن هذا ما يهم توقعتم العكس\n",
            "Decoded T : you know im gonna just assume thats implied\n",
            "============================================================\n",
            "Direction : ar_to_en\n",
            "Source    : <2en> مرحبا (بيل )\n",
            "Target    : hey bill\n",
            "Input IDs : [5, 3295, 564, 3674, 2153]\n",
            "Labels    : [2715, 5751]\n",
            "Attn Mask : [1, 1, 1, 1, 1]\n",
            "Decoded S : <2en> مرحبا (بيل )\n",
            "Decoded T : hey bill\n",
            "============================================================\n",
            "Direction : en_to_ar\n",
            "Source    : <2ar> He also helped the girl to remember her real name: Emily Blandish.\n",
            "Target    : كما ساعد الفتاة على تذكر اسمها الحقيقي: إميلي بلانديش.\n",
            "Input IDs : [4, 1050, 778, 8449, 295, 1217, 342, 3660, 693, 2500, 1547, 31, 10041, 1302, 600, 1220, 856, 19]\n",
            "Labels    : [1941, 6114, 5954, 378, 7557, 8747, 9087, 31, 375, 17501, 1387, 12984, 344, 19]\n",
            "Attn Mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Decoded S : <2ar> He also helped the girl to remember her real name: Emily Blandish.\n",
            "Decoded T : كما ساعد الفتاة على تذكر اسمها الحقيقي: إميلي بلانديش.\n",
            "============================================================\n",
            "Direction : en_to_ar\n",
            "Source    : <2ar> theyre even in his butt crack\n",
            "Target    : وهم حتى في بلده بعقب الكراك.\n",
            "Input IDs : [4, 3530, 1850, 326, 632, 15474, 12257]\n",
            "Labels    : [17421, 1658, 325, 1387, 2412, 3444, 1744, 563, 2702, 19]\n",
            "Attn Mask : [1, 1, 1, 1, 1, 1, 1]\n",
            "Decoded S : <2ar> theyre even in his butt crack\n",
            "Decoded T : وهم حتى في بلده بعقب الكراك.\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "for item in audit_ds:\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Direction : {item.get('direction', '')}\")\n",
        "    print(f\"Source    : {item['source_text']}\")\n",
        "    print(f\"Target    : {item['target_text']}\")\n",
        "    print(f\"Input IDs : {item['input_ids']}\")\n",
        "    print(f\"Labels    : {item['labels']}\")\n",
        "    print(f\"Attn Mask : {item['attention_mask']}\")\n",
        "    print(f\"Decoded S : {hf_tokenizer.decode(item['input_ids'], skip_special_tokens=False)}\")\n",
        "    print(f\"Decoded T : {hf_tokenizer.decode(item['labels'], skip_special_tokens=False)}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "a2f9979c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized random BART model (no pretrained weights).\n",
            "Vocab size: 32,000\n",
            "d_model: 512, enc_layers: 6, dec_layers: 6\n",
            "Total params: 60,659,712\n",
            "Trainable params: 60,659,712\n"
          ]
        }
      ],
      "source": [
        "# Micro-step 14: define and instantiate random-init BART model\n",
        "from transformers import BartConfig, BartForConditionalGeneration\n",
        "\n",
        "bart_config = BartConfig(\n",
        "    vocab_size=hf_tokenizer.vocab_size,\n",
        "    max_position_embeddings=config.max_seq_len + 2,\n",
        "    d_model=512,\n",
        "    encoder_layers=6,\n",
        "    decoder_layers=6,\n",
        "    encoder_attention_heads=8,\n",
        "    decoder_attention_heads=8,\n",
        "    encoder_ffn_dim=2048,\n",
        "    decoder_ffn_dim=2048,\n",
        "    dropout=0.1, # Main residual dropout (applied after attention & FFN blocks)\n",
        "    attention_dropout=0.1, # Dropout applied to attention probabilities\n",
        "    activation_dropout=0.0, # Dropout after FFN activation (kept 0 for stability)\n",
        "    pad_token_id=hf_tokenizer.pad_token_id,\n",
        "    bos_token_id=hf_tokenizer.bos_token_id,\n",
        "    eos_token_id=hf_tokenizer.eos_token_id,\n",
        "    decoder_start_token_id=hf_tokenizer.bos_token_id, # the decoder will start with <bos> to predict the first token\n",
        ")\n",
        "\n",
        "model = BartForConditionalGeneration(bart_config)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"Initialized random BART model (no pretrained weights).\")\n",
        "print(f\"Vocab size: {bart_config.vocab_size:,}\")\n",
        "print(f\"d_model: {bart_config.d_model}, enc_layers: {bart_config.encoder_layers}, dec_layers: {bart_config.decoder_layers}\")\n",
        "print(f\"Total params: {total_params:,}\")\n",
        "print(f\"Trainable params: {trainable_params:,}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
