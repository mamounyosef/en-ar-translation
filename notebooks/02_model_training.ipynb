{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "22ffe200",
      "metadata": {},
      "source": [
        "# EN-AR Model Training\n",
        "\n",
        "## Objective\n",
        "- Train a bidirectional EN <-> AR encoder-decoder model from random initialization.\n",
        "- Use the cleaned combined dataset exported by Notebook 01.\n",
        "\n",
        "## Scope\n",
        "- Notebook-first, micro-step implementation (1-2 short cells per step).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a1d5f8ca",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"PATH\"] = r\"C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.44.35207\\bin\\Hostx64\\x64\" + os.pathsep + os.environ.get(\"PATH\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c46cad66",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: c:\\My Projects\\en-ar-translation\n",
            "Dataset path: c:\\My Projects\\en-ar-translation\\artifacts\\eda\\final_cleaned_combined_dataset.parquet\n",
            "CUDA available: True\n",
            "TrainConfig(max_seq_len=128, vocab_size=32000, train_ratio=0.9, val_ratio=0.05, test_ratio=0.05)\n"
          ]
        }
      ],
      "source": [
        "# Setup: imports and training constants\n",
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "candidate_roots = [Path.cwd(), Path.cwd().parent]\n",
        "PROJECT_ROOT = next((r for r in candidate_roots if (r / \"artifacts\").exists()), Path.cwd())\n",
        "DATA_PATH = PROJECT_ROOT / \"artifacts\" / \"eda\" / \"final_cleaned_combined_dataset.parquet\"\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    max_seq_len: int = 128\n",
        "    vocab_size: int = 32_000\n",
        "    train_ratio: float = 0.90\n",
        "    val_ratio: float = 0.05\n",
        "    test_ratio: float = 0.05\n",
        "\n",
        "config = TrainConfig()\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"Dataset path: {DATA_PATH}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "caa92828",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded dataset: c:\\My Projects\\en-ar-translation\\artifacts\\eda\\final_cleaned_combined_dataset.parquet\n",
            "Shape: (827576, 2)\n",
            "Columns: ['en', 'ar']\n"
          ]
        }
      ],
      "source": [
        "# Micro-step 2: load cleaned dataset and validate schema\n",
        "assert DATA_PATH.exists(), f\"Cleaned dataset not found: {DATA_PATH}\"\n",
        "df = pd.read_parquet(DATA_PATH)\n",
        "\n",
        "required_columns = [\"en\", \"ar\"]\n",
        "missing = [c for c in required_columns if c not in df.columns]\n",
        "assert not missing, f\"Missing required columns: {missing}\"\n",
        "\n",
        "print(f\"Loaded dataset: {DATA_PATH}\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c6e9cec1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows before cleaning: 827,576\n",
            "Rows after cleaning: 827,546\n",
            "Rows removed: 30\n"
          ]
        }
      ],
      "source": [
        "# Micro-step 3: remove invalid/empty rows and report before/after\n",
        "rows_before = len(df)\n",
        "\n",
        "df = df.dropna(subset=[\"en\", \"ar\"]).copy()\n",
        "df[\"en\"] = df[\"en\"].astype(str).str.strip()\n",
        "df[\"ar\"] = df[\"ar\"].astype(str).str.strip()\n",
        "df = df[(df[\"en\"] != \"\") & (df[\"ar\"] != \"\")].reset_index(drop=True)\n",
        "\n",
        "rows_after = len(df)\n",
        "rows_removed = rows_before - rows_after\n",
        "print(f\"Rows before cleaning: {rows_before:,}\")\n",
        "print(f\"Rows after cleaning: {rows_after:,}\")\n",
        "print(f\"Rows removed: {rows_removed:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5032acd3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>en</th>\n",
              "      <th>ar</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>man with blue shirt standing in a gymnasium</td>\n",
              "      <td>رجل ذو قميص أزرق يقف في صالة رياضية</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hart did not run for public office again.</td>\n",
              "      <td>لم تبحث شركة Hart عن مقر عام لها مجددا.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>uh present day linda ronstadt</td>\n",
              "      <td>في الوقت الحاضر (ليندا رونستد)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The current mayor is Leonard Reed.</td>\n",
              "      <td>ليونارد ريد هو عمدة البلدية الحالي.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"It's for Annabelle.\"</td>\n",
              "      <td>إنها لأنابيل.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>thats not your problem</td>\n",
              "      <td>هذه ليست مشكلتك</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>leave me alone</td>\n",
              "      <td>كلا ! دعوني وشأني</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>a large silver filigree pendant on a white bac...</td>\n",
              "      <td>قلادة فضية كبيرة على خلفية بيضاء</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>a large brick building with a blue sign that r...</td>\n",
              "      <td>مبنى كبير من الطوب مع علامة زرقاء التي تقرأ'mo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>egyptian president muhammad morsi has started ...</td>\n",
              "      <td>بدا الرييس المصري محمد مرسي رسميا بالتغريد عبر...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  en  \\\n",
              "0        man with blue shirt standing in a gymnasium   \n",
              "1          Hart did not run for public office again.   \n",
              "2                      uh present day linda ronstadt   \n",
              "3                 The current mayor is Leonard Reed.   \n",
              "4                              \"It's for Annabelle.\"   \n",
              "5                             thats not your problem   \n",
              "6                                     leave me alone   \n",
              "7  a large silver filigree pendant on a white bac...   \n",
              "8  a large brick building with a blue sign that r...   \n",
              "9  egyptian president muhammad morsi has started ...   \n",
              "\n",
              "                                                  ar  \n",
              "0                رجل ذو قميص أزرق يقف في صالة رياضية  \n",
              "1            لم تبحث شركة Hart عن مقر عام لها مجددا.  \n",
              "2                     في الوقت الحاضر (ليندا رونستد)  \n",
              "3                ليونارد ريد هو عمدة البلدية الحالي.  \n",
              "4                                      إنها لأنابيل.  \n",
              "5                                    هذه ليست مشكلتك  \n",
              "6                                  كلا ! دعوني وشأني  \n",
              "7                   قلادة فضية كبيرة على خلفية بيضاء  \n",
              "8  مبنى كبير من الطوب مع علامة زرقاء التي تقرأ'mo...  \n",
              "9  بدا الرييس المصري محمد مرسي رسميا بالتغريد عبر...  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Quick check: 10 random rows from current dataset\n",
        "df.sample(n=10)[[\"en\", \"ar\"]].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f1f6f191",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>split</th>\n",
              "      <th>rows</th>\n",
              "      <th>ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>train</td>\n",
              "      <td>744926</td>\n",
              "      <td>0.9002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>val</td>\n",
              "      <td>41445</td>\n",
              "      <td>0.0501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>test</td>\n",
              "      <td>41175</td>\n",
              "      <td>0.0498</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   split    rows   ratio\n",
              "0  train  744926  0.9002\n",
              "1    val   41445  0.0501\n",
              "2   test   41175  0.0498"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Micro-step 4: deterministic hash split (90/5/5) with leakage guard\n",
        "assert abs((config.train_ratio + config.val_ratio + config.test_ratio) - 1.0) < 1e-9, \"Split ratios must sum to 1.0\"\n",
        "\n",
        "pair_hash = pd.util.hash_pandas_object(df[[\"en\", \"ar\"]], index=False).astype(\"uint64\")\n",
        "u = pair_hash / np.float64(2**64)\n",
        "\n",
        "train_cut = config.train_ratio\n",
        "val_cut = config.train_ratio + config.val_ratio\n",
        "df[\"split\"] = np.where(u < train_cut, \"train\", np.where(u < val_cut, \"val\", \"test\"))\n",
        "\n",
        "leak_count = int((df.groupby([\"en\", \"ar\"])[\"split\"].nunique() > 1).sum())\n",
        "assert leak_count == 0, f\"Leakage detected across splits for {leak_count} pairs\"\n",
        "\n",
        "split_counts = df[\"split\"].value_counts().rename_axis(\"split\").reset_index(name=\"rows\")\n",
        "split_counts[\"ratio\"] = (split_counts[\"rows\"] / len(df)).round(4)\n",
        "split_counts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "97237a4c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base rows: 827,546\n",
            "Bidirectional rows: 1,655,092\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>split</th>\n",
              "      <th>direction</th>\n",
              "      <th>rows</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>test</td>\n",
              "      <td>ar_to_en</td>\n",
              "      <td>41175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>test</td>\n",
              "      <td>en_to_ar</td>\n",
              "      <td>41175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>train</td>\n",
              "      <td>ar_to_en</td>\n",
              "      <td>744926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>train</td>\n",
              "      <td>en_to_ar</td>\n",
              "      <td>744926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>val</td>\n",
              "      <td>ar_to_en</td>\n",
              "      <td>41445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>val</td>\n",
              "      <td>en_to_ar</td>\n",
              "      <td>41445</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   split direction    rows\n",
              "0   test  ar_to_en   41175\n",
              "1   test  en_to_ar   41175\n",
              "2  train  ar_to_en  744926\n",
              "3  train  en_to_ar  744926\n",
              "4    val  ar_to_en   41445\n",
              "5    val  en_to_ar   41445"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Micro-step 5: build bidirectional rows with direction tokens\n",
        "required_split_cols = [\"en\", \"ar\", \"split\"]\n",
        "missing_split_cols = [c for c in required_split_cols if c not in df.columns]\n",
        "assert not missing_split_cols, f\"Missing columns before bidirectional build: {missing_split_cols}\"\n",
        "\n",
        "df_en_to_ar = pd.DataFrame({\n",
        "    \"source_text\": \"<2ar> \" + df[\"en\"],\n",
        "    \"target_text\": df[\"ar\"],\n",
        "    \"direction\": \"en_to_ar\",\n",
        "    \"split\": df[\"split\"],\n",
        "})\n",
        "\n",
        "df_ar_to_en = pd.DataFrame({\n",
        "    \"source_text\": \"<2en> \" + df[\"ar\"],\n",
        "    \"target_text\": df[\"en\"],\n",
        "    \"direction\": \"ar_to_en\",\n",
        "    \"split\": df[\"split\"],\n",
        "})\n",
        "\n",
        "df_bi = pd.concat([df_en_to_ar, df_ar_to_en], ignore_index=True)\n",
        "\n",
        "print(f\"Base rows: {len(df):,}\")\n",
        "print(f\"Bidirectional rows: {len(df_bi):,}\")\n",
        "\n",
        "direction_split_counts = (\n",
        "    df_bi.groupby([\"split\", \"direction\"]).size().reset_index(name=\"rows\")\n",
        ")\n",
        "direction_split_counts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "89f344a6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train rows: 1,489,852\n",
            "val rows: 82,890\n",
            "test rows: 82,350\n"
          ]
        }
      ],
      "source": [
        "# Micro-step 6: create train/val/test views from bidirectional dataset\n",
        "required_bi_cols = [\"source_text\", \"target_text\", \"direction\", \"split\"]\n",
        "missing_bi_cols = [c for c in required_bi_cols if c not in df_bi.columns]\n",
        "assert not missing_bi_cols, f\"Missing columns in df_bi: {missing_bi_cols}\"\n",
        "\n",
        "train_df = df_bi[df_bi[\"split\"] == \"train\"].reset_index(drop=True)\n",
        "val_df = df_bi[df_bi[\"split\"] == \"val\"].reset_index(drop=True)\n",
        "test_df = df_bi[df_bi[\"split\"] == \"test\"].reset_index(drop=True)\n",
        "\n",
        "assert len(train_df) + len(val_df) + len(test_df) == len(df_bi), \"Split size mismatch\"\n",
        "assert len(train_df) > 0 and len(val_df) > 0 and len(test_df) > 0, \"One split is empty\"\n",
        "\n",
        "print(f\"train rows: {len(train_df):,}\")\n",
        "print(f\"val rows: {len(val_df):,}\")\n",
        "print(f\"test rows: {len(test_df):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "78765424",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer output path: c:\\My Projects\\en-ar-translation\\artifacts\\tokenizer\\en_ar_bpe_tokenizer.json\n",
            "Special tokens: ['<pad>', '<s>', '</s>', '<unk>', '<2ar>', '<2en>']\n",
            "Tokenizer mode: ByteLevel BPE\n",
            "Train rows for tokenizer: 1,489,852\n",
            "Approx lines seen by tokenizer iterator: 2,979,704\n"
          ]
        }
      ],
      "source": [
        "# Micro-step 7: tokenizer setup (train split only)\n",
        "try:\n",
        "    from tokenizers import Tokenizer, decoders, models, pre_tokenizers, trainers\n",
        "except ImportError as e:\n",
        "    raise ImportError(\"`tokenizers` is required. Install with: pip install tokenizers\") from e\n",
        "\n",
        "TOKENIZER_DIR = PROJECT_ROOT / \"artifacts\" / \"tokenizer\"\n",
        "TOKENIZER_DIR.mkdir(parents=True, exist_ok=True)\n",
        "TOKENIZER_PATH = TOKENIZER_DIR / \"en_ar_bpe_tokenizer.json\"\n",
        "\n",
        "SPECIAL_TOKENS = [\"<pad>\", \"<s>\", \"</s>\", \"<unk>\", \"<2ar>\", \"<2en>\"]\n",
        "\n",
        "def train_corpus_iterator(df_in):\n",
        "    for row in df_in.itertuples(index=False):\n",
        "        yield row.source_text\n",
        "        yield row.target_text\n",
        "\n",
        "print(f\"Tokenizer output path: {TOKENIZER_PATH}\")\n",
        "print(f\"Special tokens: {SPECIAL_TOKENS}\")\n",
        "print(\"Tokenizer mode: ByteLevel BPE\")\n",
        "print(f\"Train rows for tokenizer: {len(train_df):,}\")\n",
        "print(f\"Approx lines seen by tokenizer iterator: {len(train_df) * 2:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4cb5dac7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trained tokenizer vocab size: 32,000\n",
            "Saved tokenizer to: c:\\My Projects\\en-ar-translation\\artifacts\\tokenizer\\en_ar_bpe_tokenizer.json\n"
          ]
        }
      ],
      "source": [
        "# Micro-step 8: train and save shared ByteLevel BPE tokenizer\n",
        "tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
        "tokenizer.decoder = decoders.ByteLevel()\n",
        "\n",
        "trainer = trainers.BpeTrainer(\n",
        "    vocab_size=config.vocab_size,\n",
        "    special_tokens=SPECIAL_TOKENS,\n",
        "    min_frequency=2,\n",
        "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n",
        ")\n",
        "\n",
        "tokenizer.train_from_iterator(train_corpus_iterator(train_df), trainer=trainer)\n",
        "tokenizer.save(str(TOKENIZER_PATH))\n",
        "\n",
        "vocab_size_trained = tokenizer.get_vocab_size()\n",
        "print(f\"Trained tokenizer vocab size: {vocab_size_trained:,}\")\n",
        "print(f\"Saved tokenizer to: {TOKENIZER_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c7e854cc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Special token IDs: {'<pad>': 0, '<s>': 1, '</s>': 2, '<unk>': 3, '<2ar>': 4, '<2en>': 5}\n",
            "Direction token probes OK (<2ar>/<2en> preserved).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>metric</th>\n",
              "      <th>value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sample_rows</td>\n",
              "      <td>20000.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>trained_vocab_size</td>\n",
              "      <td>32000.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>src_len_p50</td>\n",
              "      <td>9.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>src_len_p90</td>\n",
              "      <td>18.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>src_len_p95</td>\n",
              "      <td>21.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>src_len_p99</td>\n",
              "      <td>28.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>tgt_len_p50</td>\n",
              "      <td>8.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>tgt_len_p90</td>\n",
              "      <td>17.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>tgt_len_p95</td>\n",
              "      <td>20.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>tgt_len_p99</td>\n",
              "      <td>28.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>src_trunc_ratio_gt_max_len</td>\n",
              "      <td>0.0001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>tgt_trunc_ratio_gt_max_len</td>\n",
              "      <td>0.0001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>either_trunc_ratio_gt_max_len</td>\n",
              "      <td>0.0002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>src_unk_tokens</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>tgt_unk_tokens</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           metric       value\n",
              "0                     sample_rows  20000.0000\n",
              "1              trained_vocab_size  32000.0000\n",
              "2                     src_len_p50      9.0000\n",
              "3                     src_len_p90     18.0000\n",
              "4                     src_len_p95     21.0000\n",
              "5                     src_len_p99     28.0000\n",
              "6                     tgt_len_p50      8.0000\n",
              "7                     tgt_len_p90     17.0000\n",
              "8                     tgt_len_p95     20.0000\n",
              "9                     tgt_len_p99     28.0000\n",
              "10     src_trunc_ratio_gt_max_len      0.0001\n",
              "11     tgt_trunc_ratio_gt_max_len      0.0001\n",
              "12  either_trunc_ratio_gt_max_len      0.0002\n",
              "13                 src_unk_tokens      0.0000\n",
              "14                 tgt_unk_tokens      0.0000"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Micro-step 9: tokenizer audit (critical checks)\n",
        "from collections import Counter\n",
        "\n",
        "# Ensure tokenizer object is available (reload from disk if needed)\n",
        "if \"tokenizer\" not in globals():\n",
        "    tokenizer = Tokenizer.from_file(str(TOKENIZER_PATH))\n",
        "\n",
        "# 1) Special-token integrity\n",
        "special_token_ids = {tok: tokenizer.token_to_id(tok) for tok in SPECIAL_TOKENS}\n",
        "missing_special = [tok for tok, tid in special_token_ids.items() if tid is None]\n",
        "assert not missing_special, f\"Missing special tokens in tokenizer vocab: {missing_special}\"\n",
        "\n",
        "id_2ar = special_token_ids[\"<2ar>\"]\n",
        "id_2en = special_token_ids[\"<2en>\"]\n",
        "probe_2ar = tokenizer.encode(\"<2ar> this is a test\")\n",
        "probe_2en = tokenizer.encode(\"<2en> english direction probe\")\n",
        "assert len(probe_2ar.ids) > 0 and probe_2ar.ids[0] == id_2ar, \"<2ar> is not preserved as first token\"\n",
        "assert len(probe_2en.ids) > 0 and probe_2en.ids[0] == id_2en, \"<2en> is not preserved as first token\"\n",
        "\n",
        "# 2) Sample-based token-length and truncation audit (real tokenizer lengths)\n",
        "audit_sample_size = min(20000, len(df_bi))\n",
        "audit_df = df_bi.sample(n=audit_sample_size, random_state=RANDOM_SEED) if len(df_bi) > audit_sample_size else df_bi\n",
        "\n",
        "src_lengths = []\n",
        "tgt_lengths = []\n",
        "unk_counter = Counter()\n",
        "unk_id = special_token_ids[\"<unk>\"]\n",
        "\n",
        "for row in audit_df.itertuples(index=False):\n",
        "    src_ids = tokenizer.encode(row.source_text).ids\n",
        "    tgt_ids = tokenizer.encode(row.target_text).ids\n",
        "    src_lengths.append(len(src_ids))\n",
        "    tgt_lengths.append(len(tgt_ids))\n",
        "    unk_counter[\"src_unk_tokens\"] += sum(1 for i in src_ids if i == unk_id)\n",
        "    unk_counter[\"tgt_unk_tokens\"] += sum(1 for i in tgt_ids if i == unk_id)\n",
        "\n",
        "src_lengths = np.array(src_lengths, dtype=np.int32)\n",
        "tgt_lengths = np.array(tgt_lengths, dtype=np.int32)\n",
        "\n",
        "tokenizer_audit = pd.DataFrame({\n",
        "    \"metric\": [\n",
        "        \"sample_rows\",\n",
        "        \"trained_vocab_size\",\n",
        "        \"src_len_p50\", \"src_len_p90\", \"src_len_p95\", \"src_len_p99\",\n",
        "        \"tgt_len_p50\", \"tgt_len_p90\", \"tgt_len_p95\", \"tgt_len_p99\",\n",
        "        \"src_trunc_ratio_gt_max_len\",\n",
        "        \"tgt_trunc_ratio_gt_max_len\",\n",
        "        \"either_trunc_ratio_gt_max_len\",\n",
        "        \"src_unk_tokens\",\n",
        "        \"tgt_unk_tokens\",\n",
        "    ],\n",
        "    \"value\": [\n",
        "        int(len(audit_df)),\n",
        "        int(tokenizer.get_vocab_size()),\n",
        "        int(np.percentile(src_lengths, 50)), int(np.percentile(src_lengths, 90)), int(np.percentile(src_lengths, 95)), int(np.percentile(src_lengths, 99)),\n",
        "        int(np.percentile(tgt_lengths, 50)), int(np.percentile(tgt_lengths, 90)), int(np.percentile(tgt_lengths, 95)), int(np.percentile(tgt_lengths, 99)),\n",
        "        float((src_lengths > config.max_seq_len).mean()),\n",
        "        float((tgt_lengths > config.max_seq_len).mean()),\n",
        "        float(((src_lengths > config.max_seq_len) | (tgt_lengths > config.max_seq_len)).mean()),\n",
        "        int(unk_counter[\"src_unk_tokens\"]),\n",
        "        int(unk_counter[\"tgt_unk_tokens\"]),\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"Special token IDs:\", special_token_ids)\n",
        "print(\"Direction token probes OK (<2ar>/<2en> preserved).\")\n",
        "tokenizer_audit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5ff47333",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved HF tokenizer to: c:\\My Projects\\en-ar-translation\\artifacts\\tokenizer\\hf_tokenizer\n",
            "Token IDs: {'<pad>': 0, '<s>': 1, '</s>': 2, '<unk>': 3, '<2ar>': 4, '<2en>': 5}\n",
            "HF vocab size: 32,000\n"
          ]
        }
      ],
      "source": [
        "# Micro-step 10: build and save Hugging Face compatible fast tokenizer\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "HF_TOKENIZER_DIR = TOKENIZER_DIR / \"hf_tokenizer\"\n",
        "HF_TOKENIZER_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "hf_tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_file=str(TOKENIZER_PATH),\n",
        "    bos_token=\"<s>\",\n",
        "    eos_token=\"</s>\",\n",
        "    unk_token=\"<unk>\",\n",
        "    pad_token=\"<pad>\",\n",
        "    additional_special_tokens=[\"<2ar>\", \"<2en>\"],\n",
        ")\n",
        "\n",
        "# Verify essential token ids exist\n",
        "required_tokens = [\"<pad>\", \"<s>\", \"</s>\", \"<unk>\", \"<2ar>\", \"<2en>\"]\n",
        "token_id_map = {tok: hf_tokenizer.convert_tokens_to_ids(tok) for tok in required_tokens}\n",
        "missing_ids = [tok for tok, tid in token_id_map.items() if tid is None or tid < 0]\n",
        "assert not missing_ids, f\"Missing token ids in hf_tokenizer: {missing_ids}\"\n",
        "\n",
        "hf_tokenizer.save_pretrained(str(HF_TOKENIZER_DIR))\n",
        "\n",
        "print(f\"Saved HF tokenizer to: {HF_TOKENIZER_DIR}\")\n",
        "print(\"Token IDs:\", token_id_map)\n",
        "print(f\"HF vocab size: {hf_tokenizer.vocab_size:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "21169324",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41e50adba639482aa64f6b7caf3602b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train:   0%|          | 0/1489852 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "00bd61fc30cc4726b08ea0912698e078",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing val:   0%|          | 0/82890 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83558573ce534e5db546c250217466f2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing test:   0%|          | 0/82350 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized train rows: 1,489,852\n",
            "Tokenized val rows: 82,890\n",
            "Tokenized test rows: 82,350\n",
            "dict_keys(['source_text', 'target_text', 'direction', 'split', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n"
          ]
        }
      ],
      "source": [
        "# Micro-step 11: tokenize train/val/test (truncation only, no static padding)\n",
        "from datasets import Dataset\n",
        "\n",
        "def tokenize_batch(batch):\n",
        "    src = hf_tokenizer(\n",
        "        batch[\"source_text\"],\n",
        "        truncation=True,\n",
        "        max_length=config.max_seq_len,\n",
        "        padding=False,\n",
        "    )\n",
        "    tgt = hf_tokenizer(\n",
        "        batch[\"target_text\"],\n",
        "        truncation=True,\n",
        "        max_length=config.max_seq_len,\n",
        "        padding=False,\n",
        "    )\n",
        "    src[\"labels\"] = tgt[\"input_ids\"]\n",
        "    return src\n",
        "\n",
        "train_ds = Dataset.from_pandas(train_df[[\"source_text\", \"target_text\", \"direction\", \"split\"]], preserve_index=False)\n",
        "val_ds = Dataset.from_pandas(val_df[[\"source_text\", \"target_text\", \"direction\", \"split\"]], preserve_index=False)\n",
        "test_ds = Dataset.from_pandas(test_df[[\"source_text\", \"target_text\", \"direction\", \"split\"]], preserve_index=False)\n",
        "\n",
        "train_tok = train_ds.map(tokenize_batch, batched=True, desc=\"Tokenizing train\")\n",
        "val_tok = val_ds.map(tokenize_batch, batched=True, desc=\"Tokenizing val\")\n",
        "test_tok = test_ds.map(tokenize_batch, batched=True, desc=\"Tokenizing test\")\n",
        "\n",
        "print(f\"Tokenized train rows: {len(train_tok):,}\")\n",
        "print(f\"Tokenized val rows: {len(val_tok):,}\")\n",
        "print(f\"Tokenized test rows: {len(test_tok):,}\")\n",
        "print(train_tok[0].keys())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b4c5041c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probe batch size: 32\n",
            "input_ids shape: (32, 74)\n",
            "attention_mask shape: (32, 74)\n",
            "labels shape: (32, 37)\n",
            "Dynamic padded source length (batch max): 74\n",
            "Dynamic padded target length (batch max): 37\n",
            "Configured truncation cap: 128\n"
          ]
        }
      ],
      "source": [
        "# Micro-step 12: dynamic padding collator + one-batch sanity check\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "model_input_cols = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
        "train_tok_model = train_tok.remove_columns([c for c in train_tok.column_names if c not in model_input_cols])\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=hf_tokenizer,\n",
        "    model=None,\n",
        "    padding=\"longest\",\n",
        "    label_pad_token_id=-100,\n",
        ")\n",
        "\n",
        "batch_size_probe = min(32, len(train_tok_model))\n",
        "probe_ds = train_tok_model.shuffle(seed=RANDOM_SEED).select(range(batch_size_probe))\n",
        "probe_features = [probe_ds[i] for i in range(len(probe_ds))]\n",
        "probe_batch = data_collator(probe_features)\n",
        "\n",
        "print(f\"Probe batch size: {batch_size_probe}\")\n",
        "print(f\"input_ids shape: {tuple(probe_batch['input_ids'].shape)}\")\n",
        "print(f\"attention_mask shape: {tuple(probe_batch['attention_mask'].shape)}\")\n",
        "print(f\"labels shape: {tuple(probe_batch['labels'].shape)}\")\n",
        "print(f\"Dynamic padded source length (batch max): {probe_batch['input_ids'].shape[1]}\")\n",
        "print(f\"Dynamic padded target length (batch max): {probe_batch['labels'].shape[1]}\")\n",
        "print(f\"Configured truncation cap: {config.max_seq_len}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "59accefc",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>direction</th>\n",
              "      <th>source_text</th>\n",
              "      <th>target_text</th>\n",
              "      <th>source_len</th>\n",
              "      <th>target_len</th>\n",
              "      <th>source_ids</th>\n",
              "      <th>target_ids</th>\n",
              "      <th>decoded_source_from_ids</th>\n",
              "      <th>decoded_target_from_ids</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ar_to_en</td>\n",
              "      <td>&lt;2en&gt; فقد نفذ مني البروبان تقريبا</td>\n",
              "      <td>im nearly out of propane</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>[5, 3071, 18650, 4714, 1520, 16742, 4348]</td>\n",
              "      <td>[431, 9763, 684, 333, 4298, 2385]</td>\n",
              "      <td>&lt;2en&gt; فقد نفذ مني البروبان تقريبا</td>\n",
              "      <td>im nearly out of propane</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>en_to_ar</td>\n",
              "      <td>&lt;2ar&gt; another glass for the lady</td>\n",
              "      <td>أتريدين كأسا آخر؟</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>[4, 2200, 1849, 434, 295, 5523]</td>\n",
              "      <td>[27912, 17415, 1714, 411]</td>\n",
              "      <td>&lt;2ar&gt; another glass for the lady</td>\n",
              "      <td>أتريدين كأسا آخر؟</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ar_to_en</td>\n",
              "      <td>&lt;2en&gt; أنت مضحك</td>\n",
              "      <td>mm youre funny</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>[5, 895, 10872]</td>\n",
              "      <td>[3683, 1151, 5910]</td>\n",
              "      <td>&lt;2en&gt; أنت مضحك</td>\n",
              "      <td>mm youre funny</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ar_to_en</td>\n",
              "      <td>&lt;2en&gt; رغم ذلك، فإن هذه المعتقدات لها ما يبررها...</td>\n",
              "      <td>However, these beliefs are clearly justified.</td>\n",
              "      <td>17</td>\n",
              "      <td>10</td>\n",
              "      <td>[5, 10193, 569, 360, 3970, 687, 332, 1144, 335...</td>\n",
              "      <td>[3799, 17, 1614, 18768, 88, 450, 13120, 798, 4...</td>\n",
              "      <td>&lt;2en&gt; رغم ذلك، فإن هذه المعتقدات لها ما يبررها...</td>\n",
              "      <td>However, these beliefs are clearly justified.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ar_to_en</td>\n",
              "      <td>&lt;2en&gt; قراءة والمشاركة في مدونة الاصوات الصاعدة...</td>\n",
              "      <td>read and participate on the rising voices blog...</td>\n",
              "      <td>39</td>\n",
              "      <td>35</td>\n",
              "      <td>[5, 10144, 1723, 12101, 325, 7438, 2178, 29307...</td>\n",
              "      <td>[4776, 355, 18161, 372, 295, 13727, 2129, 2799...</td>\n",
              "      <td>&lt;2en&gt; قراءة والمشاركة في مدونة الاصوات الصاعدة...</td>\n",
              "      <td>read and participate on the rising voices blog...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  direction                                        source_text  \\\n",
              "0  ar_to_en                  <2en> فقد نفذ مني البروبان تقريبا   \n",
              "1  en_to_ar                   <2ar> another glass for the lady   \n",
              "2  ar_to_en                                     <2en> أنت مضحك   \n",
              "3  ar_to_en  <2en> رغم ذلك، فإن هذه المعتقدات لها ما يبررها...   \n",
              "4  ar_to_en  <2en> قراءة والمشاركة في مدونة الاصوات الصاعدة...   \n",
              "\n",
              "                                         target_text  source_len  target_len  \\\n",
              "0                           im nearly out of propane           7           6   \n",
              "1                                  أتريدين كأسا آخر؟           6           4   \n",
              "2                                     mm youre funny           3           3   \n",
              "3      However, these beliefs are clearly justified.          17          10   \n",
              "4  read and participate on the rising voices blog...          39          35   \n",
              "\n",
              "                                          source_ids  \\\n",
              "0          [5, 3071, 18650, 4714, 1520, 16742, 4348]   \n",
              "1                    [4, 2200, 1849, 434, 295, 5523]   \n",
              "2                                    [5, 895, 10872]   \n",
              "3  [5, 10193, 569, 360, 3970, 687, 332, 1144, 335...   \n",
              "4  [5, 10144, 1723, 12101, 325, 7438, 2178, 29307...   \n",
              "\n",
              "                                          target_ids  \\\n",
              "0                  [431, 9763, 684, 333, 4298, 2385]   \n",
              "1                          [27912, 17415, 1714, 411]   \n",
              "2                                 [3683, 1151, 5910]   \n",
              "3  [3799, 17, 1614, 18768, 88, 450, 13120, 798, 4...   \n",
              "4  [4776, 355, 18161, 372, 295, 13727, 2129, 2799...   \n",
              "\n",
              "                             decoded_source_from_ids  \\\n",
              "0                  <2en> فقد نفذ مني البروبان تقريبا   \n",
              "1                   <2ar> another glass for the lady   \n",
              "2                                     <2en> أنت مضحك   \n",
              "3  <2en> رغم ذلك، فإن هذه المعتقدات لها ما يبررها...   \n",
              "4  <2en> قراءة والمشاركة في مدونة الاصوات الصاعدة...   \n",
              "\n",
              "                             decoded_target_from_ids  \n",
              "0                           im nearly out of propane  \n",
              "1                                  أتريدين كأسا آخر؟  \n",
              "2                                     mm youre funny  \n",
              "3      However, these beliefs are clearly justified.  \n",
              "4  read and participate on the rising voices blog...  "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Micro-step 13: manual audit of random final tokenized examples\n",
        "sample_n = min(5, len(train_tok))\n",
        "audit_ds = train_tok.shuffle().select(range(sample_n))\n",
        "\n",
        "audit_rows = []\n",
        "for item in audit_ds:\n",
        "    src_ids = item[\"input_ids\"]\n",
        "    lbl_ids = item[\"labels\"]\n",
        "    audit_rows.append({\n",
        "        \"direction\": item.get(\"direction\", \"\"),\n",
        "        \"source_text\": item[\"source_text\"],\n",
        "        \"target_text\": item[\"target_text\"],\n",
        "        \"source_len\": len(src_ids),\n",
        "        \"target_len\": len(lbl_ids),\n",
        "        \"source_ids\": src_ids,\n",
        "        \"target_ids\": lbl_ids,\n",
        "        \"decoded_source_from_ids\": hf_tokenizer.decode(src_ids, skip_special_tokens=False),\n",
        "        \"decoded_target_from_ids\": hf_tokenizer.decode(lbl_ids, skip_special_tokens=False),\n",
        "    })\n",
        "\n",
        "tokenized_audit_df = pd.DataFrame(audit_rows)\n",
        "tokenized_audit_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "986a8afd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Direction : ar_to_en\n",
            "Source    : <2en> فقد نفذ مني البروبان تقريبا\n",
            "Target    : im nearly out of propane\n",
            "Input IDs : [5, 3071, 18650, 4714, 1520, 16742, 4348]\n",
            "Labels    : [431, 9763, 684, 333, 4298, 2385]\n",
            "Attn Mask : [1, 1, 1, 1, 1, 1, 1]\n",
            "Decoded S : <2en> فقد نفذ مني البروبان تقريبا\n",
            "Decoded T : im nearly out of propane\n",
            "============================================================\n",
            "Direction : en_to_ar\n",
            "Source    : <2ar> another glass for the lady\n",
            "Target    : أتريدين كأسا آخر؟\n",
            "Input IDs : [4, 2200, 1849, 434, 295, 5523]\n",
            "Labels    : [27912, 17415, 1714, 411]\n",
            "Attn Mask : [1, 1, 1, 1, 1, 1]\n",
            "Decoded S : <2ar> another glass for the lady\n",
            "Decoded T : أتريدين كأسا آخر؟\n",
            "============================================================\n",
            "Direction : ar_to_en\n",
            "Source    : <2en> أنت مضحك\n",
            "Target    : mm youre funny\n",
            "Input IDs : [5, 895, 10872]\n",
            "Labels    : [3683, 1151, 5910]\n",
            "Attn Mask : [1, 1, 1]\n",
            "Decoded S : <2en> أنت مضحك\n",
            "Decoded T : mm youre funny\n",
            "============================================================\n",
            "Direction : ar_to_en\n",
            "Source    : <2en> رغم ذلك، فإن هذه المعتقدات لها ما يبررها بوضوح.\n",
            "Target    : However, these beliefs are clearly justified.\n",
            "Input IDs : [5, 10193, 569, 360, 3970, 687, 332, 1144, 335, 1949, 546, 324, 516, 269, 383, 15302, 19]\n",
            "Labels    : [3799, 17, 1614, 18768, 88, 450, 13120, 798, 4290, 19]\n",
            "Attn Mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Decoded S : <2en> رغم ذلك، فإن هذه المعتقدات لها ما يبررها بوضوح.\n",
            "Decoded T : However, these beliefs are clearly justified.\n",
            "============================================================\n",
            "Direction : ar_to_en\n",
            "Source    : <2en> قراءة والمشاركة في مدونة الاصوات الصاعدة لمتابعة اخبار الحايزين على المنح ودراسات عن المشاريع القايمة واخبار عن مجموعات العمل يرجة ترك تعليق في حالة وجود شيء حاز الاهتمام\n",
            "Target    : read and participate on the rising voices blog for news about grantees case studies of projects and news from our working groups please leave a comment if there is something you find interesting.\n",
            "Input IDs : [5, 10144, 1723, 12101, 325, 7438, 2178, 29307, 14231, 4010, 15854, 465, 1275, 4937, 378, 6814, 312, 7220, 2536, 543, 12734, 532, 1275, 596, 17588, 1321, 543, 8283, 1484, 15945, 274, 2727, 15503, 325, 4306, 3848, 1165, 29917, 14272]\n",
            "Labels    : [4776, 355, 18161, 372, 295, 13727, 2129, 2799, 434, 3653, 849, 11423, 7041, 3035, 11132, 333, 8794, 355, 3653, 639, 1350, 2579, 6796, 2091, 2877, 275, 9586, 1415, 903, 368, 1550, 353, 1791, 9482, 19]\n",
            "Attn Mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Decoded S : <2en> قراءة والمشاركة في مدونة الاصوات الصاعدة لمتابعة اخبار الحايزين على المنح ودراسات عن المشاريع القايمة واخبار عن مجموعات العمل يرجة ترك تعليق في حالة وجود شيء حاز الاهتمام\n",
            "Decoded T : read and participate on the rising voices blog for news about grantees case studies of projects and news from our working groups please leave a comment if there is something you find interesting.\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "for item in audit_ds:\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Direction : {item.get('direction', '')}\")\n",
        "    print(f\"Source    : {item['source_text']}\")\n",
        "    print(f\"Target    : {item['target_text']}\")\n",
        "    print(f\"Input IDs : {item['input_ids']}\")\n",
        "    print(f\"Labels    : {item['labels']}\")\n",
        "    print(f\"Attn Mask : {item['attention_mask']}\")\n",
        "    print(f\"Decoded S : {hf_tokenizer.decode(item['input_ids'], skip_special_tokens=False)}\")\n",
        "    print(f\"Decoded T : {hf_tokenizer.decode(item['labels'], skip_special_tokens=False)}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "a2f9979c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized random BART model (no pretrained weights).\n",
            "Vocab size: 32,000\n",
            "d_model: 512, enc_layers: 6, dec_layers: 6\n",
            "Total params: 60,659,712\n",
            "Trainable params: 60,659,712\n"
          ]
        }
      ],
      "source": [
        "# Micro-step 14: define and instantiate random-init BART model\n",
        "from transformers import BartConfig, BartForConditionalGeneration\n",
        "\n",
        "bart_config = BartConfig(\n",
        "    vocab_size=hf_tokenizer.vocab_size,\n",
        "    max_position_embeddings=config.max_seq_len + 2,\n",
        "    d_model=512,\n",
        "    encoder_layers=6,\n",
        "    decoder_layers=6,\n",
        "    encoder_attention_heads=8,\n",
        "    decoder_attention_heads=8,\n",
        "    encoder_ffn_dim=2048,\n",
        "    decoder_ffn_dim=2048,\n",
        "    dropout=0.1, # Main residual dropout (applied after attention & FFN blocks)\n",
        "    attention_dropout=0.1, # Dropout applied to attention probabilities\n",
        "    activation_dropout=0.0, # Dropout after FFN activation (kept 0 for stability)\n",
        "    pad_token_id=hf_tokenizer.pad_token_id,\n",
        "    bos_token_id=hf_tokenizer.bos_token_id,\n",
        "    eos_token_id=hf_tokenizer.eos_token_id,\n",
        "    decoder_start_token_id=hf_tokenizer.bos_token_id, # the decoder will start with <bos> to predict the first token\n",
        ")\n",
        "\n",
        "model = BartForConditionalGeneration(bart_config)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"Initialized random BART model (no pretrained weights).\")\n",
        "print(f\"Vocab size: {bart_config.vocab_size:,}\")\n",
        "print(f\"d_model: {bart_config.d_model}, enc_layers: {bart_config.encoder_layers}, dec_layers: {bart_config.decoder_layers}\")\n",
        "print(f\"Total params: {total_params:,}\")\n",
        "print(f\"Trainable params: {trainable_params:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "3e17c187",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Probe batch size: 8\n",
            "Loss: 10.462287\n",
            "Logits shape: (8, 17, 32000)\n",
            "tensor([[[ 0.0000e+00,  4.7105e+00,  7.3093e-01,  ..., -1.7524e-01,\n",
            "          -4.8019e-01, -2.6668e-01],\n",
            "         [ 0.0000e+00,  8.7805e-04, -7.4898e-01,  ...,  1.4209e-01,\n",
            "           7.2610e-02, -9.1130e-02],\n",
            "         [ 0.0000e+00, -1.0881e-01,  3.3885e-01,  ..., -6.3099e-01,\n",
            "          -1.9324e-01, -2.8855e-01],\n",
            "         ...,\n",
            "         [ 0.0000e+00, -1.6474e-01,  3.3509e-01,  ..., -8.8127e-02,\n",
            "          -6.1816e-01, -4.6103e-01],\n",
            "         [ 0.0000e+00,  1.9915e-01,  2.0464e-01,  ...,  2.3678e-01,\n",
            "           2.9234e-01, -7.0112e-01],\n",
            "         [ 0.0000e+00, -7.7394e-01, -3.0749e-01,  ..., -1.1064e-01,\n",
            "          -6.7551e-01, -4.5202e-01]],\n",
            "\n",
            "        [[ 0.0000e+00,  4.4847e+00,  1.6182e-01,  ...,  7.3620e-01,\n",
            "          -7.9925e-01,  6.1794e-02],\n",
            "         [ 0.0000e+00,  1.7318e-01, -5.4782e-01,  ...,  8.3050e-01,\n",
            "           3.1044e-01, -4.5127e-02],\n",
            "         [ 0.0000e+00,  3.1265e-01,  2.2879e-01,  ...,  3.6026e-01,\n",
            "          -1.7683e-01,  1.7019e-01],\n",
            "         ...,\n",
            "         [ 0.0000e+00, -4.4010e-01,  3.3349e-01,  ..., -8.5355e-02,\n",
            "          -7.3155e-01, -6.4830e-01],\n",
            "         [ 0.0000e+00,  6.2435e-02,  1.1837e-01,  ...,  9.6366e-01,\n",
            "          -3.9614e-02, -5.8873e-01],\n",
            "         [ 0.0000e+00, -6.7187e-01, -4.7464e-01,  ...,  4.9651e-01,\n",
            "          -8.4196e-01, -7.7626e-02]],\n",
            "\n",
            "        [[ 0.0000e+00,  4.8843e+00,  2.0838e-01,  ...,  2.2584e-01,\n",
            "          -3.7806e-01, -2.3811e-01],\n",
            "         [ 0.0000e+00,  9.2063e-01,  2.2336e-01,  ...,  1.8629e-01,\n",
            "           8.8773e-02,  5.5645e-03],\n",
            "         [ 0.0000e+00,  2.7102e-02,  5.2305e-01,  ..., -3.4762e-01,\n",
            "          -2.6219e-02,  1.2230e-01],\n",
            "         ...,\n",
            "         [ 0.0000e+00, -1.1504e-01,  2.4596e-01,  ...,  5.7914e-02,\n",
            "          -6.5451e-01, -5.2203e-01],\n",
            "         [ 0.0000e+00,  7.5917e-01,  2.2537e-01,  ...,  2.7616e-01,\n",
            "           2.4535e-01, -6.2797e-01],\n",
            "         [ 0.0000e+00, -5.1467e-01, -3.8168e-01,  ...,  2.2677e-01,\n",
            "          -7.1851e-01, -3.7719e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0000e+00,  4.7134e+00,  5.6617e-01,  ...,  2.2364e-01,\n",
            "          -4.5150e-01, -7.3673e-02],\n",
            "         [ 0.0000e+00, -1.6841e-01, -6.0212e-01,  ...,  5.8750e-01,\n",
            "          -7.5917e-01, -2.7252e-01],\n",
            "         [ 0.0000e+00,  6.4972e-02,  1.6391e-01,  ..., -3.6285e-01,\n",
            "          -2.0282e-01, -2.0664e-01],\n",
            "         ...,\n",
            "         [ 0.0000e+00, -2.3461e-01,  6.3549e-01,  ..., -3.0435e-01,\n",
            "          -7.3439e-01, -2.9811e-01],\n",
            "         [ 0.0000e+00,  5.0897e-01,  3.7496e-01,  ...,  1.3678e-02,\n",
            "           4.2935e-01, -8.0202e-01],\n",
            "         [ 0.0000e+00, -3.9714e-01, -2.0764e-01,  ...,  2.9859e-01,\n",
            "          -1.0282e+00, -3.4558e-01]],\n",
            "\n",
            "        [[ 0.0000e+00,  4.5708e+00,  5.2437e-01,  ...,  5.1877e-01,\n",
            "          -2.9951e-01, -1.2510e-02],\n",
            "         [ 0.0000e+00,  4.0698e-01, -2.7595e-01,  ...,  1.1461e+00,\n",
            "          -3.3476e-02,  2.6396e-01],\n",
            "         [ 0.0000e+00,  3.8511e-01,  4.8728e-01,  ...,  3.9550e-01,\n",
            "          -1.3272e-01,  4.3644e-01],\n",
            "         ...,\n",
            "         [ 0.0000e+00, -3.2063e-01,  4.0171e-01,  ..., -3.3170e-01,\n",
            "          -1.0531e+00, -7.2565e-01],\n",
            "         [ 0.0000e+00,  3.4164e-01,  1.7781e-01,  ...,  5.8501e-01,\n",
            "           4.4499e-01, -2.2578e-01],\n",
            "         [ 0.0000e+00, -3.4405e-01, -4.3470e-01,  ...,  1.6675e-01,\n",
            "          -1.1106e+00, -2.6546e-02]],\n",
            "\n",
            "        [[ 0.0000e+00,  4.4977e+00,  2.9502e-02,  ...,  3.4828e-01,\n",
            "          -5.3841e-01, -2.8886e-01],\n",
            "         [ 0.0000e+00,  2.0281e-01, -1.5963e-01,  ...,  6.0572e-01,\n",
            "          -5.1705e-01,  9.3550e-01],\n",
            "         [ 0.0000e+00,  5.5271e-01,  3.3655e-01,  ...,  2.8731e-01,\n",
            "          -7.7673e-02,  3.5543e-01],\n",
            "         ...,\n",
            "         [ 0.0000e+00, -4.7293e-01, -2.0923e-01,  ..., -4.6760e-01,\n",
            "          -9.5080e-01, -8.1991e-03],\n",
            "         [ 0.0000e+00,  5.3015e-01,  1.9770e-01,  ...,  6.9023e-01,\n",
            "           7.4364e-02, -4.7247e-01],\n",
            "         [ 0.0000e+00, -9.2879e-01, -5.4029e-01,  ...,  6.0400e-01,\n",
            "          -4.2455e-01, -2.9122e-01]]], device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Micro-step 15: one-batch forward-pass sanity check\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "model.train()\n",
        "\n",
        "probe_size = min(8, len(train_tok_model))\n",
        "probe_ds = train_tok_model.shuffle(seed=RANDOM_SEED).select(range(probe_size))\n",
        "probe_features = [probe_ds[i] for i in range(len(probe_ds))]\n",
        "probe_batch = data_collator(probe_features)\n",
        "\n",
        "batch_on_device = {k: v.to(device) for k, v in probe_batch.items()}\n",
        "outputs = model(**batch_on_device)\n",
        "\n",
        "loss_value = float(outputs.loss.detach().cpu().item())\n",
        "assert np.isfinite(loss_value), f\"Non-finite loss detected: {loss_value}\"\n",
        "\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Probe batch size: {probe_size}\")\n",
        "print(f\"Loss: {loss_value:.6f}\")\n",
        "print(f\"Logits shape: {tuple(outputs.logits.shape)}\")\n",
        "print(outputs.logits)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "b0393e92",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max learning_rate: 0.0003\n",
            "min_lr_rate ratio: 0.10\n",
            "min_lr value: 2.9999999999999997e-05\n",
            "weight_decay: 0.01\n",
            "adam_betas: (0.9, 0.98)\n",
            "max_steps: 32,000\n",
            "num_warmup_steps: 480\n"
          ]
        }
      ],
      "source": [
        "# Micro-step 16: training hyperparameters + optimizer/scheduler setup\n",
        "from transformers import get_cosine_with_min_lr_schedule_with_warmup\n",
        "\n",
        "assert \"model\" in globals(), \"Model must be initialized before optimizer setup\"\n",
        "\n",
        "learning_rate = 3e-4\n",
        "weight_decay = 0.01\n",
        "adam_betas = (0.9, 0.98)\n",
        "adam_eps = 1e-8\n",
        "\n",
        "max_steps = 32_000\n",
        "warmup_ratio = 0.015\n",
        "num_warmup_steps = int(max_steps * warmup_ratio)\n",
        "\n",
        "# Cosine decay floor: keep LR at >=10% of initial LR\n",
        "min_lr_rate = 0.10\n",
        "min_lr = learning_rate * min_lr_rate\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=learning_rate,\n",
        "    betas=adam_betas,\n",
        "    eps=adam_eps,\n",
        "    weight_decay=weight_decay,\n",
        ")\n",
        "\n",
        "lr_scheduler = get_cosine_with_min_lr_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    num_training_steps=max_steps,\n",
        "    min_lr_rate=min_lr_rate,\n",
        ")\n",
        "\n",
        "print(f\"max learning_rate: {learning_rate}\")\n",
        "print(f\"min_lr_rate ratio: {min_lr_rate:.2f}\")\n",
        "print(f\"min_lr value: {min_lr}\")\n",
        "print(f\"weight_decay: {weight_decay}\")\n",
        "print(f\"adam_betas: {adam_betas}\")\n",
        "print(f\"max_steps: {max_steps:,}\")\n",
        "print(f\"num_warmup_steps: {num_warmup_steps:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "05f8e977",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enabling TF32 for matmul and cuDNN (Ampere+ GPUs)\n",
            "use_gradient_checkpointing: False\n",
            "use_fp16: True\n",
            "per_device_train_batch_size: 24\n",
            "gradient_accumulation_steps: 24\n",
            "effective_batch_size (examples/update): 576\n"
          ]
        }
      ],
      "source": [
        "# Micro-step 17: runtime memory config (checkpointing OFF by default)\n",
        "assert \"model\" in globals(), \"Model must exist before runtime memory config\"\n",
        "\n",
        "# Start simple: no gradient checkpointing unless we face OOM\n",
        "use_gradient_checkpointing = False\n",
        "if use_gradient_checkpointing:\n",
        "    model.gradient_checkpointing_enable()\n",
        "else:\n",
        "    model.gradient_checkpointing_disable()\n",
        "\n",
        "# Mixed precision for 8GB VRAM training efficiency\n",
        "use_fp16 = torch.cuda.is_available()\n",
        "try:\n",
        "    grad_scaler = torch.amp.GradScaler(\"cuda\", enabled=use_fp16)\n",
        "except TypeError:\n",
        "    # Fallback for older torch versions\n",
        "    grad_scaler = torch.cuda.amp.GradScaler(enabled=use_fp16)\n",
        "\n",
        "# Initial batch settings (can be tuned after smoke run)\n",
        "per_device_train_batch_size = 24\n",
        "gradient_accumulation_steps = 24\n",
        "effective_batch_size = per_device_train_batch_size * gradient_accumulation_steps\n",
        "\n",
        "# Optional acceleration on Ampere+ GPUs\n",
        "if torch.cuda.is_available() and hasattr(torch.backends.cuda.matmul, \"allow_tf32\"):\n",
        "    print(\"Enabling TF32 for matmul and cuDNN (Ampere+ GPUs)\")\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "print(f\"use_gradient_checkpointing: {use_gradient_checkpointing}\")\n",
        "print(f\"use_fp16: {use_fp16}\")\n",
        "print(f\"per_device_train_batch_size: {per_device_train_batch_size}\")\n",
        "print(f\"gradient_accumulation_steps: {gradient_accumulation_steps}\")\n",
        "print(f\"effective_batch_size (examples/update): {effective_batch_size}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "371820c6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training-volume summary:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_c7293\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_c7293_level0_col0\" class=\"col_heading level0 col0\" >metric</th>\n",
              "      <th id=\"T_c7293_level0_col1\" class=\"col_heading level0 col1\" >value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_c7293_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_c7293_row0_col0\" class=\"data row0 col0\" >examples_per_micro_batch</td>\n",
              "      <td id=\"T_c7293_row0_col1\" class=\"data row0 col1\" >24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c7293_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_c7293_row1_col0\" class=\"data row1 col0\" >gradient_accumulation_steps</td>\n",
              "      <td id=\"T_c7293_row1_col1\" class=\"data row1 col1\" >24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c7293_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_c7293_row2_col0\" class=\"data row2 col0\" >examples_per_optimizer_step</td>\n",
              "      <td id=\"T_c7293_row2_col1\" class=\"data row2 col1\" >576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c7293_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_c7293_row3_col0\" class=\"data row3 col0\" >max_steps</td>\n",
              "      <td id=\"T_c7293_row3_col1\" class=\"data row3 col1\" >32,000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c7293_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_c7293_row4_col0\" class=\"data row4 col0\" >examples_seen_total</td>\n",
              "      <td id=\"T_c7293_row4_col1\" class=\"data row4 col1\" >18,432,000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c7293_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "      <td id=\"T_c7293_row5_col0\" class=\"data row5 col0\" >train_examples</td>\n",
              "      <td id=\"T_c7293_row5_col1\" class=\"data row5 col1\" >1,489,852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c7293_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
              "      <td id=\"T_c7293_row6_col0\" class=\"data row6 col0\" >approx_epochs_over_train_split</td>\n",
              "      <td id=\"T_c7293_row6_col1\" class=\"data row6 col1\" >12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c7293_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
              "      <td id=\"T_c7293_row7_col0\" class=\"data row7 col0\" >coverage_percent_of_train_split %</td>\n",
              "      <td id=\"T_c7293_row7_col1\" class=\"data row7 col1\" >1,237</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x1c59b0c0d10>"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Micro-step 18: dynamic training-volume analytics\n",
        "assert \"per_device_train_batch_size\" in globals(), \"Run runtime config cell first\"\n",
        "assert \"gradient_accumulation_steps\" in globals(), \"Run runtime config cell first\"\n",
        "assert \"max_steps\" in globals(), \"Run optimizer/scheduler cell first\"\n",
        "assert \"train_df\" in globals(), \"Run split/bidirectional cells first\"\n",
        "\n",
        "examples_per_micro_batch = int(per_device_train_batch_size)\n",
        "examples_per_optimizer_step = int(per_device_train_batch_size * gradient_accumulation_steps)\n",
        "examples_seen_total = int(max_steps * examples_per_optimizer_step)\n",
        "train_examples = int(len(train_df))\n",
        "approx_epochs = (examples_seen_total / train_examples) if train_examples else 0.0\n",
        "\n",
        "analytics_df = pd.DataFrame([\n",
        "    {\"metric\": \"examples_per_micro_batch\", \"value\": examples_per_micro_batch},\n",
        "    {\"metric\": \"gradient_accumulation_steps\", \"value\": int(gradient_accumulation_steps)},\n",
        "    {\"metric\": \"examples_per_optimizer_step\", \"value\": examples_per_optimizer_step},\n",
        "    {\"metric\": \"max_steps\", \"value\": int(max_steps)},\n",
        "    {\"metric\": \"examples_seen_total\", \"value\": examples_seen_total},\n",
        "    {\"metric\": \"train_examples\", \"value\": train_examples},\n",
        "    {\"metric\": \"approx_epochs_over_train_split\", \"value\": round(approx_epochs, 4)},\n",
        "    {\"metric\": \"coverage_percent_of_train_split %\", \"value\": round(approx_epochs * 100, 2)},\n",
        "])\n",
        "\n",
        "analytics_df = analytics_df.astype({\"value\": float})\n",
        "\n",
        "\n",
        "print(\"Training-volume summary:\")\n",
        "analytics_df.style.format({\"value\": \"{:,.0f}\"})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "b488c21d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max_steps: 32,000\n",
            "sec_per_step_estimate: 1.500\n",
            "estimated_total_seconds: 48,000\n",
            "estimated_total_hours: 13.33\n",
            "if 1.0s/step -> 8.89 hours\n",
            "if 1.5s/step -> 13.33 hours\n",
            "if 2.0s/step -> 17.78 hours\n",
            "if 2.5s/step -> 22.22 hours\n",
            "if 3.0s/step -> 26.67 hours\n"
          ]
        }
      ],
      "source": [
        "# Micro-step 19: quick training-time estimate for current max_steps\n",
        "assert \"max_steps\" in globals(), \"Run optimizer/scheduler setup cell first\"\n",
        "\n",
        "# Set this after a short timed run (seconds per optimizer step)\n",
        "sec_per_step_estimate = 1.5\n",
        "\n",
        "total_seconds = max_steps * sec_per_step_estimate\n",
        "total_hours = total_seconds / 3600\n",
        "\n",
        "print(f\"max_steps: {max_steps:,}\")\n",
        "print(f\"sec_per_step_estimate: {sec_per_step_estimate:.3f}\")\n",
        "print(f\"estimated_total_seconds: {total_seconds:,.0f}\")\n",
        "print(f\"estimated_total_hours: {total_hours:.2f}\")\n",
        "\n",
        "for s in [1.0, 1.5, 2.0, 2.5, 3.0]:\n",
        "    h = (max_steps * s) / 3600\n",
        "    print(f\"if {s:.1f}s/step -> {h:.2f} hours\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "268a968a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.compile: skipped (CUDA or torch.compile unavailable)\n"
          ]
        }
      ],
      "source": [
        "# Micro-step 19b: optional torch.compile acceleration\n",
        "use_torch_compile = False #torch.cuda.is_available() and hasattr(torch, \"compile\")\n",
        "\n",
        "if use_torch_compile:\n",
        "    try:\n",
        "        # `reduce-overhead` is a practical starting mode for training loops\n",
        "        model = torch.compile(model, backend=\"eager\", dynamic=True)\n",
        "        print(\"torch.compile: enabled\")\n",
        "    except Exception as e:\n",
        "        print(f\"torch.compile: failed, continuing without compile. reason={e}\")\n",
        "else:\n",
        "    print(\"torch.compile: skipped (CUDA or torch.compile unavailable)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "9ff79e6f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Micro-step 20: 50-step smoke training run\n",
        "# import time\n",
        "# from torch.utils.data import DataLoader\n",
        "\n",
        "# smoke_steps = 50\n",
        "# assert smoke_steps > 0, \"smoke_steps must be positive\"\n",
        "\n",
        "# train_tok_model = train_tok.remove_columns([c for c in train_tok.column_names if c not in [\"input_ids\", \"attention_mask\", \"labels\"]])\n",
        "# val_tok_model = val_tok.remove_columns([c for c in val_tok.column_names if c not in [\"input_ids\", \"attention_mask\", \"labels\"]])\n",
        "\n",
        "# train_loader = DataLoader(\n",
        "#     train_tok_model,\n",
        "#     batch_size=per_device_train_batch_size,\n",
        "#     shuffle=True,\n",
        "#     collate_fn=data_collator,\n",
        "#     drop_last=False,\n",
        "#     num_workers=2,\n",
        "#     pin_memory=True,\n",
        "# )\n",
        "# val_loader = DataLoader(\n",
        "#     val_tok_model,\n",
        "#     batch_size=per_device_train_batch_size,\n",
        "#     shuffle=False,\n",
        "#     collate_fn=data_collator,\n",
        "#     drop_last=False,\n",
        "#     num_workers=2,\n",
        "#     pin_memory=True,\n",
        "# )\n",
        "\n",
        "# model.train()\n",
        "# optimizer.zero_grad(set_to_none=True)\n",
        "# start = time.time()\n",
        "# running_loss = 0.0\n",
        "# optimizer_steps_done = 0\n",
        "\n",
        "# for micro_step, batch in enumerate(train_loader, start=1):\n",
        "#     if optimizer_steps_done >= smoke_steps:\n",
        "#         break\n",
        "\n",
        "#     batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "#     if use_fp16:\n",
        "#         with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "#             out = model(**batch)\n",
        "#             loss = out.loss / gradient_accumulation_steps\n",
        "#         grad_scaler.scale(loss).backward()\n",
        "#     else:\n",
        "#         out = model(**batch)\n",
        "#         loss = out.loss / gradient_accumulation_steps\n",
        "#         loss.backward()\n",
        "\n",
        "#     running_loss += float(out.loss.detach().cpu().item())\n",
        "\n",
        "#     if micro_step % gradient_accumulation_steps == 0:\n",
        "#         if use_fp16:\n",
        "#             grad_scaler.step(optimizer)\n",
        "#             grad_scaler.update()\n",
        "#         else:\n",
        "#             optimizer.step()\n",
        "#         optimizer.zero_grad(set_to_none=True)\n",
        "#         lr_scheduler.step()\n",
        "#         optimizer_steps_done += 1\n",
        "\n",
        "#         if optimizer_steps_done % 10 == 0 or optimizer_steps_done == 1:\n",
        "#             avg_loss = running_loss / micro_step\n",
        "#             current_lr = optimizer.param_groups[0][\"lr\"]\n",
        "#             print(f\"step={optimizer_steps_done:>3} avg_train_loss={avg_loss:.4f} lr={current_lr:.6g}\")\n",
        "\n",
        "# # Quick single-batch validation check\n",
        "# model.eval()\n",
        "# with torch.no_grad():\n",
        "#     val_batch = next(iter(val_loader))\n",
        "#     val_batch = {k: v.to(device) for k, v in val_batch.items()}\n",
        "#     if use_fp16:\n",
        "#         with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "#             val_out = model(**val_batch)\n",
        "#     else:\n",
        "#         val_out = model(**val_batch)\n",
        "\n",
        "# elapsed = time.time() - start\n",
        "# steps_per_sec = optimizer_steps_done / elapsed if elapsed > 0 else 0.0\n",
        "# sec_per_step_estimate = 1.0 / steps_per_sec if steps_per_sec > 0 else float(\"inf\")\n",
        "\n",
        "# print(\"-\" * 60)\n",
        "# print(f\"smoke optimizer steps completed: {optimizer_steps_done}\")\n",
        "# print(f\"elapsed_sec: {elapsed:.2f}\")\n",
        "# print(f\"steps_per_sec: {steps_per_sec:.4f}\")\n",
        "# print(f\"sec_per_step_estimate: {sec_per_step_estimate:.4f}\")\n",
        "# print(f\"val_loss_single_batch: {float(val_out.loss.detach().cpu().item()):.4f}\")\n",
        "# if torch.cuda.is_available():\n",
        "#     peak_mem_gb = torch.cuda.max_memory_allocated(device) / (1024**3)\n",
        "#     print(f\"peak_cuda_memory_gb: {peak_mem_gb:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "7db6cb69",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Micro-step 21a: helper functions for eval, metrics, and checkpointing\n",
        "import math\n",
        "\n",
        "def _load_text_metrics(enable_comet=False):\n",
        "    try:\n",
        "        import evaluate\n",
        "    except ImportError as e:\n",
        "        raise ImportError(\"`evaluate` is required for BLEU/chrF metrics. Install: pip install evaluate sacrebleu\") from e\n",
        "\n",
        "    _bleu_metric = evaluate.load(\"sacrebleu\")\n",
        "    _chrf_metric = evaluate.load(\"chrf\")\n",
        "    _comet_metric = None\n",
        "    if enable_comet:\n",
        "        try:\n",
        "            _comet_metric = evaluate.load(\"comet\")\n",
        "        except Exception as e:\n",
        "            print(f\"[warn] COMET unavailable; continuing without COMET. Reason: {e}\")\n",
        "            _comet_metric = None\n",
        "    return _bleu_metric, _chrf_metric, _comet_metric\n",
        "\n",
        "def _eval_val_loss(_model, _val_loader, _device, _use_fp16, _max_batches=None):\n",
        "    _model.eval()\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for b_idx, batch in enumerate(_val_loader):\n",
        "            if _max_batches is not None and b_idx >= _max_batches:\n",
        "                break\n",
        "            batch = {k: v.to(_device) for k, v in batch.items()}\n",
        "            if _use_fp16:\n",
        "                with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "                    out = _model(**batch)\n",
        "            else:\n",
        "                out = _model(**batch)\n",
        "            losses.append(float(out.loss.detach().cpu().item()))\n",
        "    _model.train()\n",
        "    return float(np.mean(losses)) if losses else float(\"nan\")\n",
        "\n",
        "def _loss_to_ppl(_loss):\n",
        "    if _loss is None or not np.isfinite(_loss):\n",
        "        return float(\"nan\")\n",
        "    # Cap exponent to avoid overflow while keeping PPL monotonic in loss.\n",
        "    return float(math.exp(min(float(_loss), 20.0)))\n",
        "\n",
        "def _eval_text_metrics(\n",
        "    _model,\n",
        "    _tokenizer,\n",
        "    _eval_df,\n",
        "    _device,\n",
        "    _batch_size,\n",
        "    _max_batches,\n",
        "    _num_beams,\n",
        "    _seed,\n",
        "    _bleu_metric,\n",
        "    _chrf_metric,\n",
        "    _comet_metric=None,\n",
        "):\n",
        "\n",
        "    _model.eval()\n",
        "    if _eval_df is None or len(_eval_df) == 0:\n",
        "        _model.train()\n",
        "        return {\n",
        "            \"num_samples\": 0,\n",
        "            \"num_batches\": 0,\n",
        "            \"bleu\": float(\"nan\"),\n",
        "            \"chrf\": float(\"nan\"),\n",
        "            \"comet\": float(\"nan\"),\n",
        "            \"bleu_en_to_ar\": float(\"nan\"),\n",
        "            \"bleu_ar_to_en\": float(\"nan\"),\n",
        "            \"chrf_en_to_ar\": float(\"nan\"),\n",
        "            \"chrf_ar_to_en\": float(\"nan\"),\n",
        "        }\n",
        "\n",
        "    if _max_batches is None:\n",
        "        sample_df = _eval_df.reset_index(drop=True)\n",
        "    else:\n",
        "        max_samples = int(_max_batches * _batch_size)\n",
        "        max_samples = min(max_samples, len(_eval_df))\n",
        "        if max_samples <= 0:\n",
        "            _model.train()\n",
        "            return {\n",
        "                \"num_samples\": 0,\n",
        "                \"num_batches\": 0,\n",
        "                \"bleu\": float(\"nan\"),\n",
        "                \"chrf\": float(\"nan\"),\n",
        "                \"comet\": float(\"nan\"),\n",
        "                \"bleu_en_to_ar\": float(\"nan\"),\n",
        "                \"bleu_ar_to_en\": float(\"nan\"),\n",
        "                \"chrf_en_to_ar\": float(\"nan\"),\n",
        "                \"chrf_ar_to_en\": float(\"nan\"),\n",
        "            }\n",
        "        sample_df = _eval_df.sample(n=max_samples, random_state=int(_seed)).reset_index(drop=True)\n",
        "\n",
        "    preds = []\n",
        "    refs = []\n",
        "    srcs = []\n",
        "    dirs = []\n",
        "    with torch.no_grad():\n",
        "        for start in range(0, len(sample_df), _batch_size):\n",
        "            chunk = sample_df.iloc[start : start + _batch_size]\n",
        "            src_batch = chunk[\"source_text\"].astype(str).tolist()\n",
        "            ref_batch = chunk[\"target_text\"].astype(str).tolist()\n",
        "            dir_batch = chunk[\"direction\"].astype(str).tolist()\n",
        "\n",
        "            enc = _tokenizer(\n",
        "                src_batch,\n",
        "                truncation=True,\n",
        "                max_length=config.max_seq_len,\n",
        "                padding=True,\n",
        "                return_token_type_ids=False,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "            enc.pop(\"token_type_ids\", None)\n",
        "            enc = {k: v.to(_device) for k, v in enc.items()}\n",
        "            gen_ids = _model.generate(\n",
        "                **enc,\n",
        "                max_new_tokens=config.max_seq_len,\n",
        "                num_beams=_num_beams,\n",
        "                do_sample=False,\n",
        "            )\n",
        "            pred_batch = _tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
        "\n",
        "            preds.extend([p.strip() for p in pred_batch])\n",
        "            refs.extend([r.strip() for r in ref_batch])\n",
        "            srcs.extend(src_batch)\n",
        "            dirs.extend(dir_batch)\n",
        "\n",
        "    _model.train()\n",
        "\n",
        "    if len(preds) == 0:\n",
        "        return {\n",
        "            \"num_samples\": 0,\n",
        "            \"num_batches\": 0,\n",
        "            \"bleu\": float(\"nan\"),\n",
        "            \"chrf\": float(\"nan\"),\n",
        "            \"comet\": float(\"nan\"),\n",
        "            \"bleu_en_to_ar\": float(\"nan\"),\n",
        "            \"bleu_ar_to_en\": float(\"nan\"),\n",
        "            \"chrf_en_to_ar\": float(\"nan\"),\n",
        "            \"chrf_ar_to_en\": float(\"nan\"),\n",
        "        }\n",
        "\n",
        "    def _score_subset(_preds, _refs):\n",
        "        if len(_preds) == 0:\n",
        "            return float(\"nan\"), float(\"nan\")\n",
        "        _bleu = float(_bleu_metric.compute(predictions=_preds, references=[[r] for r in _refs])[\"score\"])\n",
        "        _chrf = float(_chrf_metric.compute(predictions=_preds, references=_refs)[\"score\"])\n",
        "        return _bleu, _chrf\n",
        "\n",
        "    bleu_all, chrf_all = _score_subset(preds, refs)\n",
        "    idx_en_to_ar = [i for i, d in enumerate(dirs) if d == \"en_to_ar\"]\n",
        "    idx_ar_to_en = [i for i, d in enumerate(dirs) if d == \"ar_to_en\"]\n",
        "    bleu_en_to_ar, chrf_en_to_ar = _score_subset([preds[i] for i in idx_en_to_ar], [refs[i] for i in idx_en_to_ar])\n",
        "    bleu_ar_to_en, chrf_ar_to_en = _score_subset([preds[i] for i in idx_ar_to_en], [refs[i] for i in idx_ar_to_en])\n",
        "\n",
        "    comet_score = float(\"nan\")\n",
        "    if _comet_metric is not None:\n",
        "        try:\n",
        "            comet_out = _comet_metric.compute(predictions=preds, references=refs, sources=srcs)\n",
        "            comet_score = float(comet_out.get(\"mean_score\", comet_out.get(\"score\", float(\"nan\"))))\n",
        "        except Exception as e:\n",
        "            print(f\"[warn] COMET scoring failed at runtime; continuing without COMET value. Reason: {e}\")\n",
        "\n",
        "    return {\n",
        "        \"num_samples\": int(len(preds)),\n",
        "        \"num_batches\": int(math.ceil(len(preds) / _batch_size)),\n",
        "        \"bleu\": bleu_all,\n",
        "        \"chrf\": chrf_all,\n",
        "        \"comet\": comet_score,\n",
        "        \"bleu_en_to_ar\": bleu_en_to_ar,\n",
        "        \"bleu_ar_to_en\": bleu_ar_to_en,\n",
        "        \"chrf_en_to_ar\": chrf_en_to_ar,\n",
        "        \"chrf_ar_to_en\": chrf_ar_to_en,\n",
        "    }\n",
        "\n",
        "def _save_checkpoint(_model, _tokenizer, _optimizer, _scheduler, _scaler, _step, _val_loss, _target_dir):\n",
        "    _target_dir.mkdir(parents=True, exist_ok=True)\n",
        "    _model.save_pretrained(str(_target_dir))\n",
        "    _tokenizer.save_pretrained(str(_target_dir / \"tokenizer\"))\n",
        "    torch.save({\n",
        "        \"step\": int(_step),\n",
        "        \"val_loss\": float(_val_loss) if _val_loss is not None else None,\n",
        "        \"optimizer\": _optimizer.state_dict(),\n",
        "        \"scheduler\": _scheduler.state_dict(),\n",
        "        \"scaler\": _scaler.state_dict() if _scaler is not None else None,\n",
        "    }, _target_dir / \"trainer_state.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "cd5a226d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Micro-step 21: full training loop with interval logging, checkpoints, and best-model tracking\n",
        "import json as pyjson\n",
        "import shutil\n",
        "import time\n",
        "from datetime import datetime\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# -----------------------------\n",
        "# Tunable run hyperparameters\n",
        "# -----------------------------\n",
        "log_every_steps = 40\n",
        "eval_every_steps = 400\n",
        "save_every_steps = 500\n",
        "keep_last_n_checkpoints = 10\n",
        "max_val_batches = 300  # set None to use full val loader\n",
        "text_metric_num_beams = 1\n",
        "max_text_metric_batches_log = 4   # lightweight text-metric pass at log intervals\n",
        "max_text_metric_batches_eval = 32  # stronger text-metric pass at eval intervals\n",
        "enable_comet = False\n",
        "train_num_workers = 2\n",
        "val_num_workers = 2\n",
        "pin_memory = True\n",
        "\n",
        "assert max_steps > 0, \"max_steps must be > 0\"\n",
        "assert gradient_accumulation_steps > 0, \"gradient_accumulation_steps must be > 0\"\n",
        "\n",
        "# Build train/val model-input datasets (dynamic padding via collator)\n",
        "model_input_cols = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
        "train_tok_model = train_tok.remove_columns([c for c in train_tok.column_names if c not in model_input_cols])\n",
        "val_tok_model = val_tok.remove_columns([c for c in val_tok.column_names if c not in model_input_cols])\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_tok_model,\n",
        "    batch_size=per_device_train_batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        "    num_workers=train_num_workers,\n",
        "    pin_memory=pin_memory,\n",
        "    persistent_workers=(train_num_workers > 0),\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_tok_model,\n",
        "    batch_size=per_device_train_batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        "    num_workers=val_num_workers,\n",
        "    pin_memory=pin_memory,\n",
        "    persistent_workers=(val_num_workers > 0),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "0f3c46ba",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run_id: 20260218_003516\n",
            "run_dir: c:\\My Projects\\en-ar-translation\\artifacts\\runs\\20260218_003516\n",
            "ckpt_root: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\n"
          ]
        }
      ],
      "source": [
        "# Run dirs and paths\n",
        "run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "run_dir = PROJECT_ROOT / \"artifacts\" / \"runs\" / run_id\n",
        "ckpt_root = PROJECT_ROOT / \"checkpoints\" / run_id\n",
        "best_dir = ckpt_root / \"best_model\"\n",
        "run_dir.mkdir(parents=True, exist_ok=True)\n",
        "ckpt_root.mkdir(parents=True, exist_ok=True)\n",
        "best_dir.mkdir(parents=True, exist_ok=True)\n",
        "train_metrics_path = run_dir / \"train_metrics.csv\"\n",
        "eval_metrics_path = run_dir / \"eval_metrics.csv\"\n",
        "config_path = run_dir / \"run_config.json\"\n",
        "\n",
        "run_config = {\n",
        "    \"run_id\": run_id,\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"min_lr_rate\": min_lr_rate,\n",
        "    \"weight_decay\": weight_decay,\n",
        "    \"adam_betas\": list(adam_betas),\n",
        "    \"adam_eps\": adam_eps,\n",
        "    \"max_steps\": max_steps,\n",
        "    \"warmup_ratio\": warmup_ratio,\n",
        "    \"num_warmup_steps\": num_warmup_steps,\n",
        "    \"per_device_train_batch_size\": per_device_train_batch_size,\n",
        "    \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
        "    \"effective_batch_size\": effective_batch_size,\n",
        "    \"use_fp16\": bool(use_fp16),\n",
        "    \"use_gradient_checkpointing\": bool(use_gradient_checkpointing),\n",
        "    \"log_every_steps\": log_every_steps,\n",
        "    \"eval_every_steps\": eval_every_steps,\n",
        "    \"save_every_steps\": save_every_steps,\n",
        "    \"keep_last_n_checkpoints\": keep_last_n_checkpoints,\n",
        "    \"max_val_batches\": max_val_batches,\n",
        "    \"text_metric_num_beams\": text_metric_num_beams,\n",
        "    \"max_text_metric_batches_log\": max_text_metric_batches_log,\n",
        "    \"max_text_metric_batches_eval\": max_text_metric_batches_eval,\n",
        "    \"enable_comet\": bool(enable_comet),\n",
        "}\n",
        "config_path.write_text(pyjson.dumps(run_config, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print(f\"run_id: {run_id}\")\n",
        "print(f\"run_dir: {run_dir}\")\n",
        "print(f\"ckpt_root: {ckpt_root}\")\n",
        "\n",
        "assert \"_load_text_metrics\" in globals(), \"Run helper-functions cell (Micro-step 21a) first\"\n",
        "assert \"_eval_val_loss\" in globals(), \"Run helper-functions cell (Micro-step 21a) first\"\n",
        "assert \"_eval_text_metrics\" in globals(), \"Run helper-functions cell (Micro-step 21a) first\"\n",
        "assert \"_save_checkpoint\" in globals(), \"Run helper-functions cell (Micro-step 21a) first\"\n",
        "bleu_metric, chrf_metric, comet_metric = _load_text_metrics(enable_comet=enable_comet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "49000b79",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================================================================\n",
            "step      1/32000 | avg_loss=10.4623 | train_ppl=34970.1665 | lr=6.25e-07 | grad_norm=1.3755\n",
            "interval_sec=32.61 | steps/sec=0.0307 | sec/step=32.6076\n",
            "examples_seen=128 | approx_epochs=0.0001\n",
            "batch_shapes input=(16, 18) labels=(16, 13)\n",
            "cuda_mem_alloc_gb=0.822 | cuda_mem_peak_gb=1.575\n",
            "[text@log] samples=64 | bleu=0.000 | chrf=0.000 | comet=nan | text_eval_sec=6.15\n",
            "========================================================================================\n",
            "step     40/32000 | avg_loss=10.2586 | train_ppl=28528.2377 | lr=2.5e-05 | grad_norm=1.3175\n",
            "interval_sec=22.50 | steps/sec=1.7332 | sec/step=0.5770\n",
            "examples_seen=5,120 | approx_epochs=0.0034\n",
            "batch_shapes input=(16, 22) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.531\n",
            "[text@log] samples=64 | bleu=0.016 | chrf=0.127 | comet=nan | text_eval_sec=5.40\n",
            "========================================================================================\n",
            "step     80/32000 | avg_loss=9.6433 | train_ppl=15417.5080 | lr=5e-05 | grad_norm=1.2453\n",
            "interval_sec=21.21 | steps/sec=1.8855 | sec/step=0.5304\n",
            "examples_seen=10,240 | approx_epochs=0.0069\n",
            "batch_shapes input=(16, 20) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.743\n",
            "[text@log] samples=64 | bleu=0.012 | chrf=1.543 | comet=nan | text_eval_sec=5.96\n",
            "========================================================================================\n",
            "step    120/32000 | avg_loss=9.0242 | train_ppl=8301.7600 | lr=7.5e-05 | grad_norm=0.9573\n",
            "interval_sec=21.94 | steps/sec=1.8235 | sec/step=0.5484\n",
            "examples_seen=15,360 | approx_epochs=0.0103\n",
            "batch_shapes input=(16, 24) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.845 | cuda_mem_peak_gb=2.743\n",
            "[text@log] samples=64 | bleu=0.055 | chrf=1.555 | comet=nan | text_eval_sec=5.22\n",
            "========================================================================================\n",
            "step    160/32000 | avg_loss=8.4615 | train_ppl=4729.2823 | lr=0.0001 | grad_norm=0.7915\n",
            "interval_sec=23.56 | steps/sec=1.6979 | sec/step=0.5889\n",
            "examples_seen=20,480 | approx_epochs=0.0137\n",
            "batch_shapes input=(16, 19) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.843 | cuda_mem_peak_gb=2.745\n",
            "[text@log] samples=64 | bleu=0.012 | chrf=2.474 | comet=nan | text_eval_sec=4.63\n",
            "========================================================================================\n",
            "step    200/32000 | avg_loss=8.1098 | train_ppl=3327.0405 | lr=0.000125 | grad_norm=1.0375\n",
            "interval_sec=21.79 | steps/sec=1.8359 | sec/step=0.5447\n",
            "examples_seen=25,600 | approx_epochs=0.0172\n",
            "batch_shapes input=(16, 22) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.745\n",
            "[text@log] samples=64 | bleu=0.044 | chrf=1.611 | comet=nan | text_eval_sec=7.99\n",
            "========================================================================================\n",
            "step    240/32000 | avg_loss=7.8720 | train_ppl=2622.9189 | lr=0.00015 | grad_norm=0.8838\n",
            "interval_sec=25.34 | steps/sec=1.5786 | sec/step=0.6335\n",
            "examples_seen=30,720 | approx_epochs=0.0206\n",
            "batch_shapes input=(16, 15) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.745\n",
            "[text@log] samples=64 | bleu=0.040 | chrf=1.806 | comet=nan | text_eval_sec=6.12\n",
            "========================================================================================\n",
            "step    280/32000 | avg_loss=7.6441 | train_ppl=2088.3644 | lr=0.000175 | grad_norm=1.5937\n",
            "interval_sec=20.00 | steps/sec=2.0003 | sec/step=0.4999\n",
            "examples_seen=35,840 | approx_epochs=0.0241\n",
            "batch_shapes input=(16, 21) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.745\n",
            "[text@log] samples=64 | bleu=0.033 | chrf=2.604 | comet=nan | text_eval_sec=4.68\n",
            "========================================================================================\n",
            "step    320/32000 | avg_loss=7.4589 | train_ppl=1735.2418 | lr=0.0002 | grad_norm=0.9265\n",
            "interval_sec=24.33 | steps/sec=1.6441 | sec/step=0.6082\n",
            "examples_seen=40,960 | approx_epochs=0.0275\n",
            "batch_shapes input=(16, 13) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.745\n",
            "[text@log] samples=64 | bleu=0.025 | chrf=3.066 | comet=nan | text_eval_sec=4.88\n",
            "========================================================================================\n",
            "step    360/32000 | avg_loss=7.3731 | train_ppl=1592.5228 | lr=0.000225 | grad_norm=1.0748\n",
            "interval_sec=22.65 | steps/sec=1.7663 | sec/step=0.5661\n",
            "examples_seen=46,080 | approx_epochs=0.0309\n",
            "batch_shapes input=(16, 16) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.745\n",
            "[text@log] samples=64 | bleu=0.027 | chrf=2.820 | comet=nan | text_eval_sec=4.68\n",
            "========================================================================================\n",
            "step    400/32000 | avg_loss=7.2419 | train_ppl=1396.7788 | lr=0.00025 | grad_norm=1.4053\n",
            "interval_sec=24.55 | steps/sec=1.6295 | sec/step=0.6137\n",
            "examples_seen=51,200 | approx_epochs=0.0344\n",
            "batch_shapes input=(16, 13) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.745\n",
            "[text@log] samples=64 | bleu=0.056 | chrf=4.097 | comet=nan | text_eval_sec=5.91\n",
            "[eval] step=400 val_loss=7.1735 | val_ppl=1304.4506 | bleu=0.040 | chrf=4.137 | comet=nan | text_eval_sec=41.71\n",
            "[best] new best val_loss=7.1735 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step    440/32000 | avg_loss=7.1122 | train_ppl=1226.8219 | lr=0.000275 | grad_norm=1.0710\n",
            "interval_sec=103.87 | steps/sec=0.3851 | sec/step=2.5967\n",
            "examples_seen=56,320 | approx_epochs=0.0378\n",
            "batch_shapes input=(16, 25) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.745\n",
            "[text@log] samples=64 | bleu=0.046 | chrf=3.571 | comet=nan | text_eval_sec=6.19\n",
            "========================================================================================\n",
            "step    480/32000 | avg_loss=7.0097 | train_ppl=1107.3254 | lr=0.0003 | grad_norm=1.1376\n",
            "interval_sec=22.72 | steps/sec=1.7608 | sec/step=0.5679\n",
            "examples_seen=61,440 | approx_epochs=0.0412\n",
            "batch_shapes input=(16, 16) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.745\n",
            "[text@log] samples=64 | bleu=0.070 | chrf=4.291 | comet=nan | text_eval_sec=4.58\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_000500\n",
            "========================================================================================\n",
            "step    520/32000 | avg_loss=6.9364 | train_ppl=1029.0534 | lr=0.000299999 | grad_norm=1.1603\n",
            "interval_sec=22.71 | steps/sec=1.7612 | sec/step=0.5678\n",
            "examples_seen=66,560 | approx_epochs=0.0447\n",
            "batch_shapes input=(16, 31) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.844 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.073 | chrf=4.855 | comet=nan | text_eval_sec=5.10\n",
            "========================================================================================\n",
            "step    560/32000 | avg_loss=6.8161 | train_ppl=912.4147 | lr=0.000299996 | grad_norm=1.2196\n",
            "interval_sec=23.38 | steps/sec=1.7106 | sec/step=0.5846\n",
            "examples_seen=71,680 | approx_epochs=0.0481\n",
            "batch_shapes input=(16, 22) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.029 | chrf=4.365 | comet=nan | text_eval_sec=5.91\n",
            "========================================================================================\n",
            "step    600/32000 | avg_loss=6.7337 | train_ppl=840.2612 | lr=0.00029999 | grad_norm=1.1100\n",
            "interval_sec=21.37 | steps/sec=1.8717 | sec/step=0.5343\n",
            "examples_seen=76,800 | approx_epochs=0.0515\n",
            "batch_shapes input=(16, 16) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.843 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.081 | chrf=6.154 | comet=nan | text_eval_sec=4.36\n",
            "========================================================================================\n",
            "step    640/32000 | avg_loss=6.6137 | train_ppl=745.2716 | lr=0.000299983 | grad_norm=1.2464\n",
            "interval_sec=21.64 | steps/sec=1.8486 | sec/step=0.5410\n",
            "examples_seen=81,920 | approx_epochs=0.0550\n",
            "batch_shapes input=(16, 19) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.050 | chrf=5.097 | comet=nan | text_eval_sec=4.43\n",
            "========================================================================================\n",
            "step    680/32000 | avg_loss=6.5750 | train_ppl=716.9485 | lr=0.000299973 | grad_norm=1.3164\n",
            "interval_sec=23.11 | steps/sec=1.7309 | sec/step=0.5777\n",
            "examples_seen=87,040 | approx_epochs=0.0584\n",
            "batch_shapes input=(16, 59) labels=(16, 54)\n",
            "cuda_mem_alloc_gb=0.902 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.237 | chrf=6.224 | comet=nan | text_eval_sec=6.45\n",
            "========================================================================================\n",
            "step    720/32000 | avg_loss=6.4516 | train_ppl=633.7446 | lr=0.000299961 | grad_norm=1.2222\n",
            "interval_sec=22.76 | steps/sec=1.7578 | sec/step=0.5689\n",
            "examples_seen=92,160 | approx_epochs=0.0619\n",
            "batch_shapes input=(16, 21) labels=(16, 14)\n",
            "cuda_mem_alloc_gb=0.824 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.139 | chrf=6.418 | comet=nan | text_eval_sec=5.68\n",
            "========================================================================================\n",
            "step    760/32000 | avg_loss=6.3681 | train_ppl=582.9599 | lr=0.000299947 | grad_norm=1.3918\n",
            "interval_sec=22.19 | steps/sec=1.8027 | sec/step=0.5547\n",
            "examples_seen=97,280 | approx_epochs=0.0653\n",
            "batch_shapes input=(16, 18) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.091 | chrf=5.901 | comet=nan | text_eval_sec=5.68\n",
            "========================================================================================\n",
            "step    800/32000 | avg_loss=6.3135 | train_ppl=551.9989 | lr=0.000299931 | grad_norm=1.3685\n",
            "interval_sec=24.06 | steps/sec=1.6628 | sec/step=0.6014\n",
            "examples_seen=102,400 | approx_epochs=0.0687\n",
            "batch_shapes input=(16, 16) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.160 | chrf=8.021 | comet=nan | text_eval_sec=4.45\n",
            "[eval] step=800 val_loss=6.2374 | val_ppl=511.5414 | bleu=0.142 | chrf=7.881 | comet=nan | text_eval_sec=43.84\n",
            "[best] new best val_loss=6.2374 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step    840/32000 | avg_loss=6.2943 | train_ppl=541.4935 | lr=0.000299913 | grad_norm=1.2378\n",
            "interval_sec=71.64 | steps/sec=0.5584 | sec/step=1.7909\n",
            "examples_seen=107,520 | approx_epochs=0.0722\n",
            "batch_shapes input=(16, 21) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.100 | chrf=6.723 | comet=nan | text_eval_sec=4.59\n",
            "========================================================================================\n",
            "step    880/32000 | avg_loss=6.2159 | train_ppl=500.6339 | lr=0.000299893 | grad_norm=1.3997\n",
            "interval_sec=23.86 | steps/sec=1.6764 | sec/step=0.5965\n",
            "examples_seen=112,640 | approx_epochs=0.0756\n",
            "batch_shapes input=(16, 23) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.233 | chrf=7.380 | comet=nan | text_eval_sec=6.11\n",
            "========================================================================================\n",
            "step    920/32000 | avg_loss=6.1458 | train_ppl=466.7629 | lr=0.00029987 | grad_norm=1.3319\n",
            "interval_sec=20.27 | steps/sec=1.9736 | sec/step=0.5067\n",
            "examples_seen=117,760 | approx_epochs=0.0790\n",
            "batch_shapes input=(16, 22) labels=(16, 26)\n",
            "cuda_mem_alloc_gb=0.847 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.290 | chrf=7.920 | comet=nan | text_eval_sec=6.52\n",
            "========================================================================================\n",
            "step    960/32000 | avg_loss=6.0916 | train_ppl=442.1115 | lr=0.000299846 | grad_norm=1.4326\n",
            "interval_sec=21.83 | steps/sec=1.8322 | sec/step=0.5458\n",
            "examples_seen=122,880 | approx_epochs=0.0825\n",
            "batch_shapes input=(16, 16) labels=(16, 14)\n",
            "cuda_mem_alloc_gb=0.824 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.198 | chrf=7.992 | comet=nan | text_eval_sec=5.89\n",
            "========================================================================================\n",
            "step   1000/32000 | avg_loss=5.9826 | train_ppl=396.4834 | lr=0.000299819 | grad_norm=1.4672\n",
            "interval_sec=21.34 | steps/sec=1.8748 | sec/step=0.5334\n",
            "examples_seen=128,000 | approx_epochs=0.0859\n",
            "batch_shapes input=(16, 23) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.223 | chrf=8.882 | comet=nan | text_eval_sec=4.80\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_001000\n",
            "========================================================================================\n",
            "step   1040/32000 | avg_loss=5.9658 | train_ppl=389.8752 | lr=0.00029979 | grad_norm=1.3883\n",
            "interval_sec=26.95 | steps/sec=1.4840 | sec/step=0.6738\n",
            "examples_seen=133,120 | approx_epochs=0.0894\n",
            "batch_shapes input=(16, 28) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.187 | chrf=7.173 | comet=nan | text_eval_sec=6.54\n",
            "========================================================================================\n",
            "step   1080/32000 | avg_loss=5.9143 | train_ppl=370.3111 | lr=0.000299759 | grad_norm=1.4747\n",
            "interval_sec=21.88 | steps/sec=1.8279 | sec/step=0.5471\n",
            "examples_seen=138,240 | approx_epochs=0.0928\n",
            "batch_shapes input=(16, 22) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.845 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.342 | chrf=9.458 | comet=nan | text_eval_sec=5.76\n",
            "========================================================================================\n",
            "step   1120/32000 | avg_loss=5.8807 | train_ppl=358.0583 | lr=0.000299725 | grad_norm=1.5149\n",
            "interval_sec=20.66 | steps/sec=1.9359 | sec/step=0.5165\n",
            "examples_seen=143,360 | approx_epochs=0.0962\n",
            "batch_shapes input=(16, 25) labels=(16, 26)\n",
            "cuda_mem_alloc_gb=0.847 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.472 | chrf=9.588 | comet=nan | text_eval_sec=7.03\n",
            "========================================================================================\n",
            "step   1160/32000 | avg_loss=5.7647 | train_ppl=318.8518 | lr=0.00029969 | grad_norm=1.4093\n",
            "interval_sec=24.78 | steps/sec=1.6144 | sec/step=0.6194\n",
            "examples_seen=148,480 | approx_epochs=0.0997\n",
            "batch_shapes input=(16, 21) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.843 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.405 | chrf=10.132 | comet=nan | text_eval_sec=6.11\n",
            "========================================================================================\n",
            "step   1200/32000 | avg_loss=5.7580 | train_ppl=316.7149 | lr=0.000299653 | grad_norm=1.6020\n",
            "interval_sec=19.53 | steps/sec=2.0477 | sec/step=0.4884\n",
            "examples_seen=153,600 | approx_epochs=0.1031\n",
            "batch_shapes input=(16, 19) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.330 | chrf=9.716 | comet=nan | text_eval_sec=6.92\n",
            "[eval] step=1200 val_loss=5.6926 | val_ppl=296.6718 | bleu=0.350 | chrf=9.136 | comet=nan | text_eval_sec=43.79\n",
            "[best] new best val_loss=5.6926 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step   1240/32000 | avg_loss=5.7589 | train_ppl=317.0107 | lr=0.000299613 | grad_norm=1.5446\n",
            "interval_sec=75.14 | steps/sec=0.5323 | sec/step=1.8786\n",
            "examples_seen=158,720 | approx_epochs=0.1065\n",
            "batch_shapes input=(16, 19) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.499 | chrf=9.600 | comet=nan | text_eval_sec=5.39\n",
            "========================================================================================\n",
            "step   1280/32000 | avg_loss=5.6813 | train_ppl=293.3349 | lr=0.000299571 | grad_norm=1.7912\n",
            "interval_sec=20.28 | steps/sec=1.9722 | sec/step=0.5070\n",
            "examples_seen=163,840 | approx_epochs=0.1100\n",
            "batch_shapes input=(16, 24) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.260 | chrf=9.400 | comet=nan | text_eval_sec=4.93\n",
            "========================================================================================\n",
            "step   1320/32000 | avg_loss=5.6923 | train_ppl=296.5754 | lr=0.000299527 | grad_norm=1.5227\n",
            "interval_sec=21.25 | steps/sec=1.8828 | sec/step=0.5311\n",
            "examples_seen=168,960 | approx_epochs=0.1134\n",
            "batch_shapes input=(16, 14) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.682 | chrf=10.746 | comet=nan | text_eval_sec=4.49\n",
            "========================================================================================\n",
            "step   1360/32000 | avg_loss=5.6068 | train_ppl=272.2630 | lr=0.000299481 | grad_norm=1.4697\n",
            "interval_sec=22.27 | steps/sec=1.7959 | sec/step=0.5568\n",
            "examples_seen=174,080 | approx_epochs=0.1168\n",
            "batch_shapes input=(16, 24) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.364 | chrf=9.384 | comet=nan | text_eval_sec=4.39\n",
            "========================================================================================\n",
            "step   1400/32000 | avg_loss=5.5560 | train_ppl=258.7984 | lr=0.000299433 | grad_norm=1.6703\n",
            "interval_sec=21.75 | steps/sec=1.8393 | sec/step=0.5437\n",
            "examples_seen=179,200 | approx_epochs=0.1203\n",
            "batch_shapes input=(16, 17) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.543 | chrf=9.721 | comet=nan | text_eval_sec=4.40\n",
            "========================================================================================\n",
            "step   1440/32000 | avg_loss=5.5381 | train_ppl=254.1954 | lr=0.000299382 | grad_norm=1.6440\n",
            "interval_sec=22.63 | steps/sec=1.7676 | sec/step=0.5657\n",
            "examples_seen=184,320 | approx_epochs=0.1237\n",
            "batch_shapes input=(16, 16) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.259 | chrf=10.605 | comet=nan | text_eval_sec=6.33\n",
            "========================================================================================\n",
            "step   1480/32000 | avg_loss=5.5555 | train_ppl=258.6690 | lr=0.00029933 | grad_norm=1.6968\n",
            "interval_sec=21.49 | steps/sec=1.8612 | sec/step=0.5373\n",
            "examples_seen=189,440 | approx_epochs=0.1272\n",
            "batch_shapes input=(16, 23) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.582 | chrf=10.853 | comet=nan | text_eval_sec=4.80\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_001500\n",
            "========================================================================================\n",
            "step   1520/32000 | avg_loss=5.5209 | train_ppl=249.8577 | lr=0.000299275 | grad_norm=1.4953\n",
            "interval_sec=24.42 | steps/sec=1.6380 | sec/step=0.6105\n",
            "examples_seen=194,560 | approx_epochs=0.1306\n",
            "batch_shapes input=(16, 27) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.846 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.799 | chrf=11.015 | comet=nan | text_eval_sec=5.29\n",
            "========================================================================================\n",
            "step   1560/32000 | avg_loss=5.4566 | train_ppl=234.3027 | lr=0.000299219 | grad_norm=1.7231\n",
            "interval_sec=22.28 | steps/sec=1.7953 | sec/step=0.5570\n",
            "examples_seen=199,680 | approx_epochs=0.1340\n",
            "batch_shapes input=(16, 19) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.556 | chrf=10.089 | comet=nan | text_eval_sec=5.83\n",
            "========================================================================================\n",
            "step   1600/32000 | avg_loss=5.4122 | train_ppl=224.1290 | lr=0.00029916 | grad_norm=1.5433\n",
            "interval_sec=21.92 | steps/sec=1.8245 | sec/step=0.5481\n",
            "examples_seen=204,800 | approx_epochs=0.1375\n",
            "batch_shapes input=(16, 14) labels=(16, 14)\n",
            "cuda_mem_alloc_gb=0.824 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.677 | chrf=10.585 | comet=nan | text_eval_sec=6.15\n",
            "[eval] step=1600 val_loss=5.2871 | val_ppl=197.7765 | bleu=0.635 | chrf=11.106 | comet=nan | text_eval_sec=36.01\n",
            "[best] new best val_loss=5.2871 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step   1640/32000 | avg_loss=5.3616 | train_ppl=213.0716 | lr=0.000299099 | grad_norm=1.5821\n",
            "interval_sec=64.11 | steps/sec=0.6240 | sec/step=1.6027\n",
            "examples_seen=209,920 | approx_epochs=0.1409\n",
            "batch_shapes input=(16, 20) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.507 | chrf=12.196 | comet=nan | text_eval_sec=4.97\n",
            "========================================================================================\n",
            "step   1680/32000 | avg_loss=5.3031 | train_ppl=200.9600 | lr=0.000299036 | grad_norm=1.7282\n",
            "interval_sec=20.15 | steps/sec=1.9856 | sec/step=0.5036\n",
            "examples_seen=215,040 | approx_epochs=0.1443\n",
            "batch_shapes input=(16, 21) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.553 | chrf=11.350 | comet=nan | text_eval_sec=4.32\n",
            "========================================================================================\n",
            "step   1720/32000 | avg_loss=5.3211 | train_ppl=204.6152 | lr=0.00029897 | grad_norm=1.6399\n",
            "interval_sec=19.04 | steps/sec=2.1014 | sec/step=0.4759\n",
            "examples_seen=220,160 | approx_epochs=0.1478\n",
            "batch_shapes input=(16, 22) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.843 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.528 | chrf=11.538 | comet=nan | text_eval_sec=4.27\n",
            "========================================================================================\n",
            "step   1760/32000 | avg_loss=5.2844 | train_ppl=197.2457 | lr=0.000298903 | grad_norm=1.5813\n",
            "interval_sec=17.91 | steps/sec=2.2332 | sec/step=0.4478\n",
            "examples_seen=225,280 | approx_epochs=0.1512\n",
            "batch_shapes input=(16, 25) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.845 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.707 | chrf=12.536 | comet=nan | text_eval_sec=4.36\n",
            "========================================================================================\n",
            "step   1800/32000 | avg_loss=5.2583 | train_ppl=192.1522 | lr=0.000298833 | grad_norm=1.6672\n",
            "interval_sec=17.62 | steps/sec=2.2699 | sec/step=0.4405\n",
            "examples_seen=230,400 | approx_epochs=0.1546\n",
            "batch_shapes input=(16, 20) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.750 | chrf=12.763 | comet=nan | text_eval_sec=4.24\n",
            "========================================================================================\n",
            "step   1840/32000 | avg_loss=5.2379 | train_ppl=188.2754 | lr=0.000298762 | grad_norm=1.6872\n",
            "interval_sec=18.31 | steps/sec=2.1847 | sec/step=0.4577\n",
            "examples_seen=235,520 | approx_epochs=0.1581\n",
            "batch_shapes input=(16, 16) labels=(16, 12)\n",
            "cuda_mem_alloc_gb=0.820 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.514 | chrf=10.622 | comet=nan | text_eval_sec=4.48\n",
            "========================================================================================\n",
            "step   1880/32000 | avg_loss=5.1766 | train_ppl=177.0787 | lr=0.000298688 | grad_norm=1.7163\n",
            "interval_sec=18.87 | steps/sec=2.1198 | sec/step=0.4718\n",
            "examples_seen=240,640 | approx_epochs=0.1615\n",
            "batch_shapes input=(16, 20) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.711 | chrf=11.855 | comet=nan | text_eval_sec=4.36\n",
            "========================================================================================\n",
            "step   1920/32000 | avg_loss=5.1612 | train_ppl=174.3757 | lr=0.000298612 | grad_norm=1.7476\n",
            "interval_sec=19.24 | steps/sec=2.0785 | sec/step=0.4811\n",
            "examples_seen=245,760 | approx_epochs=0.1650\n",
            "batch_shapes input=(16, 23) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.827 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.593 | chrf=11.887 | comet=nan | text_eval_sec=4.39\n",
            "========================================================================================\n",
            "step   1960/32000 | avg_loss=5.0563 | train_ppl=157.0039 | lr=0.000298534 | grad_norm=1.6432\n",
            "interval_sec=18.88 | steps/sec=2.1182 | sec/step=0.4721\n",
            "examples_seen=250,880 | approx_epochs=0.1684\n",
            "batch_shapes input=(16, 34) labels=(16, 30)\n",
            "cuda_mem_alloc_gb=0.855 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.591 | chrf=12.477 | comet=nan | text_eval_sec=4.47\n",
            "========================================================================================\n",
            "step   2000/32000 | avg_loss=5.1036 | train_ppl=164.6190 | lr=0.000298454 | grad_norm=1.6939\n",
            "interval_sec=17.88 | steps/sec=2.2373 | sec/step=0.4470\n",
            "examples_seen=256,000 | approx_epochs=0.1718\n",
            "batch_shapes input=(16, 25) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.580 | chrf=10.453 | comet=nan | text_eval_sec=4.17\n",
            "[eval] step=2000 val_loss=5.0234 | val_ppl=151.9306 | bleu=0.719 | chrf=11.957 | comet=nan | text_eval_sec=36.64\n",
            "[best] new best val_loss=5.0234 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_002000\n",
            "========================================================================================\n",
            "step   2040/32000 | avg_loss=5.0834 | train_ppl=161.3195 | lr=0.000298371 | grad_norm=1.6528\n",
            "interval_sec=66.94 | steps/sec=0.5976 | sec/step=1.6734\n",
            "examples_seen=261,120 | approx_epochs=0.1753\n",
            "batch_shapes input=(16, 29) labels=(16, 28)\n",
            "cuda_mem_alloc_gb=0.851 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.674 | chrf=11.381 | comet=nan | text_eval_sec=5.77\n",
            "========================================================================================\n",
            "step   2080/32000 | avg_loss=5.0727 | train_ppl=159.6068 | lr=0.000298287 | grad_norm=1.7513\n",
            "interval_sec=18.09 | steps/sec=2.2107 | sec/step=0.4523\n",
            "examples_seen=266,240 | approx_epochs=0.1787\n",
            "batch_shapes input=(16, 24) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.728 | chrf=11.157 | comet=nan | text_eval_sec=4.49\n",
            "========================================================================================\n",
            "step   2120/32000 | avg_loss=5.0063 | train_ppl=149.3555 | lr=0.000298201 | grad_norm=1.6835\n",
            "interval_sec=18.28 | steps/sec=2.1882 | sec/step=0.4570\n",
            "examples_seen=271,360 | approx_epochs=0.1821\n",
            "batch_shapes input=(16, 69) labels=(16, 70)\n",
            "cuda_mem_alloc_gb=0.933 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.932 | chrf=12.608 | comet=nan | text_eval_sec=4.58\n",
            "========================================================================================\n",
            "step   2160/32000 | avg_loss=5.0179 | train_ppl=151.0935 | lr=0.000298112 | grad_norm=1.7283\n",
            "interval_sec=18.32 | steps/sec=2.1836 | sec/step=0.4580\n",
            "examples_seen=276,480 | approx_epochs=0.1856\n",
            "batch_shapes input=(16, 21) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.622 | chrf=13.245 | comet=nan | text_eval_sec=4.62\n",
            "========================================================================================\n",
            "step   2200/32000 | avg_loss=4.9866 | train_ppl=146.4383 | lr=0.000298021 | grad_norm=1.7793\n",
            "interval_sec=18.18 | steps/sec=2.2000 | sec/step=0.4545\n",
            "examples_seen=281,600 | approx_epochs=0.1890\n",
            "batch_shapes input=(16, 24) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.615 | chrf=11.361 | comet=nan | text_eval_sec=4.82\n",
            "========================================================================================\n",
            "step   2240/32000 | avg_loss=4.9097 | train_ppl=135.6051 | lr=0.000297928 | grad_norm=1.8010\n",
            "interval_sec=18.07 | steps/sec=2.2134 | sec/step=0.4518\n",
            "examples_seen=286,720 | approx_epochs=0.1924\n",
            "batch_shapes input=(16, 18) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.682 | chrf=12.365 | comet=nan | text_eval_sec=4.72\n",
            "========================================================================================\n",
            "step   2280/32000 | avg_loss=4.8630 | train_ppl=129.4158 | lr=0.000297833 | grad_norm=1.8996\n",
            "interval_sec=18.80 | steps/sec=2.1273 | sec/step=0.4701\n",
            "examples_seen=291,840 | approx_epochs=0.1959\n",
            "batch_shapes input=(16, 16) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.913 | chrf=15.799 | comet=nan | text_eval_sec=4.44\n",
            "========================================================================================\n",
            "step   2320/32000 | avg_loss=4.9411 | train_ppl=139.9174 | lr=0.000297736 | grad_norm=1.7879\n",
            "interval_sec=19.55 | steps/sec=2.0464 | sec/step=0.4887\n",
            "examples_seen=296,960 | approx_epochs=0.1993\n",
            "batch_shapes input=(16, 22) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.347 | chrf=12.047 | comet=nan | text_eval_sec=4.70\n",
            "========================================================================================\n",
            "step   2360/32000 | avg_loss=4.8354 | train_ppl=125.8839 | lr=0.000297637 | grad_norm=1.9120\n",
            "interval_sec=18.69 | steps/sec=2.1399 | sec/step=0.4673\n",
            "examples_seen=302,080 | approx_epochs=0.2028\n",
            "batch_shapes input=(16, 14) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.357 | chrf=9.194 | comet=nan | text_eval_sec=4.28\n",
            "========================================================================================\n",
            "step   2400/32000 | avg_loss=4.8949 | train_ppl=133.6022 | lr=0.000297536 | grad_norm=1.7565\n",
            "interval_sec=18.11 | steps/sec=2.2088 | sec/step=0.4527\n",
            "examples_seen=307,200 | approx_epochs=0.2062\n",
            "batch_shapes input=(16, 29) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.746 | chrf=10.523 | comet=nan | text_eval_sec=4.47\n",
            "[eval] step=2400 val_loss=4.6862 | val_ppl=108.4424 | bleu=0.824 | chrf=12.388 | comet=nan | text_eval_sec=36.33\n",
            "[best] new best val_loss=4.6862 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step   2440/32000 | avg_loss=4.7943 | train_ppl=120.8220 | lr=0.000297432 | grad_norm=1.7734\n",
            "interval_sec=68.16 | steps/sec=0.5868 | sec/step=1.7040\n",
            "examples_seen=312,320 | approx_epochs=0.2096\n",
            "batch_shapes input=(16, 16) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.865 | chrf=13.661 | comet=nan | text_eval_sec=4.43\n",
            "========================================================================================\n",
            "step   2480/32000 | avg_loss=4.7772 | train_ppl=118.7665 | lr=0.000297327 | grad_norm=1.8395\n",
            "interval_sec=18.02 | steps/sec=2.2196 | sec/step=0.4505\n",
            "examples_seen=317,440 | approx_epochs=0.2131\n",
            "batch_shapes input=(16, 19) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.794 | chrf=13.658 | comet=nan | text_eval_sec=4.26\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_002500\n",
            "========================================================================================\n",
            "step   2520/32000 | avg_loss=4.8030 | train_ppl=121.8730 | lr=0.000297219 | grad_norm=1.8291\n",
            "interval_sec=21.28 | steps/sec=1.8793 | sec/step=0.5321\n",
            "examples_seen=322,560 | approx_epochs=0.2165\n",
            "batch_shapes input=(16, 20) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.853 | chrf=13.380 | comet=nan | text_eval_sec=4.91\n",
            "========================================================================================\n",
            "step   2560/32000 | avg_loss=4.7706 | train_ppl=117.9892 | lr=0.000297109 | grad_norm=1.9222\n",
            "interval_sec=18.68 | steps/sec=2.1408 | sec/step=0.4671\n",
            "examples_seen=327,680 | approx_epochs=0.2199\n",
            "batch_shapes input=(16, 19) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.957 | chrf=15.164 | comet=nan | text_eval_sec=4.28\n",
            "========================================================================================\n",
            "step   2600/32000 | avg_loss=4.7869 | train_ppl=119.9248 | lr=0.000296997 | grad_norm=1.6964\n",
            "interval_sec=18.84 | steps/sec=2.1231 | sec/step=0.4710\n",
            "examples_seen=332,800 | approx_epochs=0.2234\n",
            "batch_shapes input=(16, 19) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.741 | chrf=12.567 | comet=nan | text_eval_sec=4.58\n",
            "========================================================================================\n",
            "step   2640/32000 | avg_loss=4.7955 | train_ppl=120.9603 | lr=0.000296884 | grad_norm=1.8075\n",
            "interval_sec=18.40 | steps/sec=2.1739 | sec/step=0.4600\n",
            "examples_seen=337,920 | approx_epochs=0.2268\n",
            "batch_shapes input=(16, 35) labels=(16, 27)\n",
            "cuda_mem_alloc_gb=0.850 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.543 | chrf=13.431 | comet=nan | text_eval_sec=4.29\n",
            "========================================================================================\n",
            "step   2680/32000 | avg_loss=4.6960 | train_ppl=109.5097 | lr=0.000296768 | grad_norm=1.6954\n",
            "interval_sec=17.78 | steps/sec=2.2496 | sec/step=0.4445\n",
            "examples_seen=343,040 | approx_epochs=0.2303\n",
            "batch_shapes input=(16, 18) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.870 | chrf=12.478 | comet=nan | text_eval_sec=4.21\n",
            "========================================================================================\n",
            "step   2720/32000 | avg_loss=4.6534 | train_ppl=104.9380 | lr=0.000296649 | grad_norm=1.9010\n",
            "interval_sec=18.23 | steps/sec=2.1948 | sec/step=0.4556\n",
            "examples_seen=348,160 | approx_epochs=0.2337\n",
            "batch_shapes input=(16, 16) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.942 | chrf=13.628 | comet=nan | text_eval_sec=4.25\n",
            "========================================================================================\n",
            "step   2760/32000 | avg_loss=4.7126 | train_ppl=111.3418 | lr=0.000296529 | grad_norm=1.8613\n",
            "interval_sec=18.19 | steps/sec=2.1986 | sec/step=0.4548\n",
            "examples_seen=353,280 | approx_epochs=0.2371\n",
            "batch_shapes input=(16, 16) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.873 | chrf=12.628 | comet=nan | text_eval_sec=4.47\n",
            "========================================================================================\n",
            "step   2800/32000 | avg_loss=4.6971 | train_ppl=109.6329 | lr=0.000296407 | grad_norm=1.8662\n",
            "interval_sec=18.42 | steps/sec=2.1717 | sec/step=0.4605\n",
            "examples_seen=358,400 | approx_epochs=0.2406\n",
            "batch_shapes input=(16, 24) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.909 | chrf=14.100 | comet=nan | text_eval_sec=4.50\n",
            "[eval] step=2800 val_loss=4.4863 | val_ppl=88.7914 | bleu=0.770 | chrf=13.229 | comet=nan | text_eval_sec=35.94\n",
            "[best] new best val_loss=4.4863 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step   2840/32000 | avg_loss=4.5891 | train_ppl=98.4082 | lr=0.000296282 | grad_norm=1.8916\n",
            "interval_sec=64.04 | steps/sec=0.6246 | sec/step=1.6011\n",
            "examples_seen=363,520 | approx_epochs=0.2440\n",
            "batch_shapes input=(16, 17) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.256 | chrf=16.550 | comet=nan | text_eval_sec=4.32\n",
            "========================================================================================\n",
            "step   2880/32000 | avg_loss=4.6699 | train_ppl=106.6828 | lr=0.000296156 | grad_norm=1.9371\n",
            "interval_sec=20.17 | steps/sec=1.9827 | sec/step=0.5044\n",
            "examples_seen=368,640 | approx_epochs=0.2474\n",
            "batch_shapes input=(16, 20) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.006 | chrf=13.984 | comet=nan | text_eval_sec=4.29\n",
            "========================================================================================\n",
            "step   2920/32000 | avg_loss=4.6210 | train_ppl=101.5953 | lr=0.000296027 | grad_norm=1.8532\n",
            "interval_sec=18.46 | steps/sec=2.1664 | sec/step=0.4616\n",
            "examples_seen=373,760 | approx_epochs=0.2509\n",
            "batch_shapes input=(16, 20) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.821 | chrf=12.990 | comet=nan | text_eval_sec=4.39\n",
            "========================================================================================\n",
            "step   2960/32000 | avg_loss=4.6085 | train_ppl=100.3346 | lr=0.000295897 | grad_norm=2.0137\n",
            "interval_sec=19.37 | steps/sec=2.0652 | sec/step=0.4842\n",
            "examples_seen=378,880 | approx_epochs=0.2543\n",
            "batch_shapes input=(16, 20) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.832 | chrf=12.361 | comet=nan | text_eval_sec=4.33\n",
            "========================================================================================\n",
            "step   3000/32000 | avg_loss=4.6179 | train_ppl=101.2814 | lr=0.000295764 | grad_norm=2.0910\n",
            "interval_sec=18.46 | steps/sec=2.1663 | sec/step=0.4616\n",
            "examples_seen=384,000 | approx_epochs=0.2577\n",
            "batch_shapes input=(16, 22) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.846 | chrf=13.599 | comet=nan | text_eval_sec=4.52\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_003000\n",
            "========================================================================================\n",
            "step   3040/32000 | avg_loss=4.5432 | train_ppl=93.9936 | lr=0.000295629 | grad_norm=2.0188\n",
            "interval_sec=19.88 | steps/sec=2.0126 | sec/step=0.4969\n",
            "examples_seen=389,120 | approx_epochs=0.2612\n",
            "batch_shapes input=(16, 22) labels=(16, 30)\n",
            "cuda_mem_alloc_gb=0.855 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.962 | chrf=16.160 | comet=nan | text_eval_sec=4.12\n",
            "========================================================================================\n",
            "step   3080/32000 | avg_loss=4.5448 | train_ppl=94.1408 | lr=0.000295492 | grad_norm=1.9116\n",
            "interval_sec=18.19 | steps/sec=2.1993 | sec/step=0.4547\n",
            "examples_seen=394,240 | approx_epochs=0.2646\n",
            "batch_shapes input=(16, 48) labels=(16, 49)\n",
            "cuda_mem_alloc_gb=0.892 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.558 | chrf=13.291 | comet=nan | text_eval_sec=4.21\n",
            "========================================================================================\n",
            "step   3120/32000 | avg_loss=4.5009 | train_ppl=90.1008 | lr=0.000295353 | grad_norm=2.0729\n",
            "interval_sec=18.83 | steps/sec=2.1238 | sec/step=0.4709\n",
            "examples_seen=399,360 | approx_epochs=0.2681\n",
            "batch_shapes input=(16, 21) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.843 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.881 | chrf=15.307 | comet=nan | text_eval_sec=4.38\n",
            "========================================================================================\n",
            "step   3160/32000 | avg_loss=4.5504 | train_ppl=94.6717 | lr=0.000295212 | grad_norm=1.7784\n",
            "interval_sec=18.34 | steps/sec=2.1807 | sec/step=0.4586\n",
            "examples_seen=404,480 | approx_epochs=0.2715\n",
            "batch_shapes input=(16, 19) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.927 | chrf=15.235 | comet=nan | text_eval_sec=4.41\n",
            "========================================================================================\n",
            "step   3200/32000 | avg_loss=4.4794 | train_ppl=88.1782 | lr=0.000295069 | grad_norm=2.0838\n",
            "interval_sec=19.00 | steps/sec=2.1053 | sec/step=0.4750\n",
            "examples_seen=409,600 | approx_epochs=0.2749\n",
            "batch_shapes input=(16, 15) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.755 | chrf=14.864 | comet=nan | text_eval_sec=4.48\n",
            "[eval] step=3200 val_loss=4.3632 | val_ppl=78.5045 | bleu=0.874 | chrf=14.380 | comet=nan | text_eval_sec=35.76\n",
            "[best] new best val_loss=4.3632 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step   3240/32000 | avg_loss=4.4314 | train_ppl=84.0454 | lr=0.000294924 | grad_norm=1.9311\n",
            "interval_sec=63.24 | steps/sec=0.6325 | sec/step=1.5810\n",
            "examples_seen=414,720 | approx_epochs=0.2784\n",
            "batch_shapes input=(16, 14) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.102 | chrf=16.582 | comet=nan | text_eval_sec=6.27\n",
            "========================================================================================\n",
            "step   3280/32000 | avg_loss=4.4536 | train_ppl=85.9356 | lr=0.000294777 | grad_norm=2.0896\n",
            "interval_sec=19.91 | steps/sec=2.0094 | sec/step=0.4977\n",
            "examples_seen=419,840 | approx_epochs=0.2818\n",
            "batch_shapes input=(16, 24) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.743 | chrf=13.758 | comet=nan | text_eval_sec=4.38\n",
            "========================================================================================\n",
            "step   3320/32000 | avg_loss=4.4022 | train_ppl=81.6272 | lr=0.000294628 | grad_norm=1.9990\n",
            "interval_sec=18.43 | steps/sec=2.1702 | sec/step=0.4608\n",
            "examples_seen=424,960 | approx_epochs=0.2852\n",
            "batch_shapes input=(16, 22) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.853 | chrf=14.871 | comet=nan | text_eval_sec=4.38\n",
            "========================================================================================\n",
            "step   3360/32000 | avg_loss=4.3915 | train_ppl=80.7642 | lr=0.000294476 | grad_norm=1.9525\n",
            "interval_sec=18.65 | steps/sec=2.1453 | sec/step=0.4661\n",
            "examples_seen=430,080 | approx_epochs=0.2887\n",
            "batch_shapes input=(16, 20) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.738 | chrf=13.036 | comet=nan | text_eval_sec=4.81\n",
            "========================================================================================\n",
            "step   3400/32000 | avg_loss=4.4099 | train_ppl=82.2622 | lr=0.000294323 | grad_norm=2.0192\n",
            "interval_sec=18.77 | steps/sec=2.1313 | sec/step=0.4692\n",
            "examples_seen=435,200 | approx_epochs=0.2921\n",
            "batch_shapes input=(16, 25) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.893 | chrf=14.085 | comet=nan | text_eval_sec=4.56\n",
            "========================================================================================\n",
            "step   3440/32000 | avg_loss=4.4133 | train_ppl=82.5433 | lr=0.000294167 | grad_norm=2.0415\n",
            "interval_sec=19.00 | steps/sec=2.1056 | sec/step=0.4749\n",
            "examples_seen=440,320 | approx_epochs=0.2955\n",
            "batch_shapes input=(16, 25) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.038 | chrf=14.336 | comet=nan | text_eval_sec=4.68\n",
            "========================================================================================\n",
            "step   3480/32000 | avg_loss=4.4251 | train_ppl=83.5186 | lr=0.00029401 | grad_norm=1.9963\n",
            "interval_sec=18.94 | steps/sec=2.1124 | sec/step=0.4734\n",
            "examples_seen=445,440 | approx_epochs=0.2990\n",
            "batch_shapes input=(16, 16) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.847 | chrf=16.381 | comet=nan | text_eval_sec=4.34\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_003500\n",
            "========================================================================================\n",
            "step   3520/32000 | avg_loss=4.3851 | train_ppl=80.2488 | lr=0.00029385 | grad_norm=1.8845\n",
            "interval_sec=21.65 | steps/sec=1.8477 | sec/step=0.5412\n",
            "examples_seen=450,560 | approx_epochs=0.3024\n",
            "batch_shapes input=(16, 28) labels=(16, 38)\n",
            "cuda_mem_alloc_gb=0.870 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.659 | chrf=14.105 | comet=nan | text_eval_sec=4.38\n",
            "========================================================================================\n",
            "step   3560/32000 | avg_loss=4.3868 | train_ppl=80.3809 | lr=0.000293689 | grad_norm=2.0639\n",
            "interval_sec=18.38 | steps/sec=2.1767 | sec/step=0.4594\n",
            "examples_seen=455,680 | approx_epochs=0.3059\n",
            "batch_shapes input=(16, 22) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.102 | chrf=14.540 | comet=nan | text_eval_sec=4.28\n",
            "========================================================================================\n",
            "step   3600/32000 | avg_loss=4.3552 | train_ppl=77.8834 | lr=0.000293525 | grad_norm=2.1100\n",
            "interval_sec=18.46 | steps/sec=2.1665 | sec/step=0.4616\n",
            "examples_seen=460,800 | approx_epochs=0.3093\n",
            "batch_shapes input=(16, 26) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.924 | chrf=16.667 | comet=nan | text_eval_sec=6.40\n",
            "[eval] step=3600 val_loss=4.2311 | val_ppl=68.7922 | bleu=0.975 | chrf=15.711 | comet=nan | text_eval_sec=35.55\n",
            "[best] new best val_loss=4.2311 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step   3640/32000 | avg_loss=4.3154 | train_ppl=74.8461 | lr=0.000293359 | grad_norm=2.0608\n",
            "interval_sec=61.56 | steps/sec=0.6498 | sec/step=1.5390\n",
            "examples_seen=465,920 | approx_epochs=0.3127\n",
            "batch_shapes input=(16, 80) labels=(16, 74)\n",
            "cuda_mem_alloc_gb=0.941 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.822 | chrf=14.688 | comet=nan | text_eval_sec=4.37\n",
            "========================================================================================\n",
            "step   3680/32000 | avg_loss=4.3591 | train_ppl=78.1851 | lr=0.000293192 | grad_norm=1.9781\n",
            "interval_sec=17.66 | steps/sec=2.2646 | sec/step=0.4416\n",
            "examples_seen=471,040 | approx_epochs=0.3162\n",
            "batch_shapes input=(16, 26) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.360 | chrf=16.988 | comet=nan | text_eval_sec=4.17\n",
            "========================================================================================\n",
            "step   3720/32000 | avg_loss=4.3068 | train_ppl=74.2009 | lr=0.000293022 | grad_norm=1.9418\n",
            "interval_sec=17.65 | steps/sec=2.2658 | sec/step=0.4413\n",
            "examples_seen=476,160 | approx_epochs=0.3196\n",
            "batch_shapes input=(16, 31) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.993 | chrf=16.973 | comet=nan | text_eval_sec=4.23\n",
            "========================================================================================\n",
            "step   3760/32000 | avg_loss=4.2758 | train_ppl=71.9408 | lr=0.00029285 | grad_norm=2.1213\n",
            "interval_sec=17.51 | steps/sec=2.2849 | sec/step=0.4377\n",
            "examples_seen=481,280 | approx_epochs=0.3230\n",
            "batch_shapes input=(16, 20) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.850 | chrf=16.293 | comet=nan | text_eval_sec=4.32\n",
            "========================================================================================\n",
            "step   3800/32000 | avg_loss=4.2631 | train_ppl=71.0326 | lr=0.000292676 | grad_norm=2.0409\n",
            "interval_sec=18.17 | steps/sec=2.2017 | sec/step=0.4542\n",
            "examples_seen=486,400 | approx_epochs=0.3265\n",
            "batch_shapes input=(16, 17) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.220 | chrf=16.171 | comet=nan | text_eval_sec=4.36\n",
            "========================================================================================\n",
            "step   3840/32000 | avg_loss=4.2461 | train_ppl=69.8308 | lr=0.0002925 | grad_norm=2.0109\n",
            "interval_sec=18.88 | steps/sec=2.1190 | sec/step=0.4719\n",
            "examples_seen=491,520 | approx_epochs=0.3299\n",
            "batch_shapes input=(16, 21) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.767 | chrf=14.250 | comet=nan | text_eval_sec=5.13\n",
            "========================================================================================\n",
            "step   3880/32000 | avg_loss=4.2354 | train_ppl=69.0881 | lr=0.000292322 | grad_norm=2.0655\n",
            "interval_sec=18.75 | steps/sec=2.1330 | sec/step=0.4688\n",
            "examples_seen=496,640 | approx_epochs=0.3333\n",
            "batch_shapes input=(16, 17) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.224 | chrf=17.428 | comet=nan | text_eval_sec=4.37\n",
            "========================================================================================\n",
            "step   3920/32000 | avg_loss=4.2514 | train_ppl=70.2035 | lr=0.000292142 | grad_norm=1.9095\n",
            "interval_sec=17.96 | steps/sec=2.2268 | sec/step=0.4491\n",
            "examples_seen=501,760 | approx_epochs=0.3368\n",
            "batch_shapes input=(16, 21) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.132 | chrf=16.426 | comet=nan | text_eval_sec=4.22\n",
            "========================================================================================\n",
            "step   3960/32000 | avg_loss=4.2367 | train_ppl=69.1826 | lr=0.00029196 | grad_norm=2.0025\n",
            "interval_sec=17.81 | steps/sec=2.2457 | sec/step=0.4453\n",
            "examples_seen=506,880 | approx_epochs=0.3402\n",
            "batch_shapes input=(16, 29) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.844 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.809 | chrf=16.161 | comet=nan | text_eval_sec=6.02\n",
            "========================================================================================\n",
            "step   4000/32000 | avg_loss=4.2522 | train_ppl=70.2564 | lr=0.000291776 | grad_norm=1.8898\n",
            "interval_sec=18.35 | steps/sec=2.1801 | sec/step=0.4587\n",
            "examples_seen=512,000 | approx_epochs=0.3437\n",
            "batch_shapes input=(16, 20) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.677 | chrf=16.314 | comet=nan | text_eval_sec=4.45\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=4000 val_loss=4.0798 | val_ppl=59.1310 | bleu=0.880 | chrf=16.845 | comet=nan | text_eval_sec=35.47\n",
            "[best] new best val_loss=4.0798 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_004000\n",
            "========================================================================================\n",
            "step   4040/32000 | avg_loss=4.1549 | train_ppl=63.7424 | lr=0.000291591 | grad_norm=2.2045\n",
            "interval_sec=63.76 | steps/sec=0.6273 | sec/step=1.5940\n",
            "examples_seen=517,120 | approx_epochs=0.3471\n",
            "batch_shapes input=(16, 20) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.032 | chrf=17.357 | comet=nan | text_eval_sec=4.71\n",
            "========================================================================================\n",
            "step   4080/32000 | avg_loss=4.1539 | train_ppl=63.6796 | lr=0.000291403 | grad_norm=2.0379\n",
            "interval_sec=19.10 | steps/sec=2.0939 | sec/step=0.4776\n",
            "examples_seen=522,240 | approx_epochs=0.3505\n",
            "batch_shapes input=(16, 24) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.204 | chrf=15.157 | comet=nan | text_eval_sec=4.48\n",
            "========================================================================================\n",
            "step   4120/32000 | avg_loss=4.2043 | train_ppl=66.9755 | lr=0.000291213 | grad_norm=2.1347\n",
            "interval_sec=18.44 | steps/sec=2.1688 | sec/step=0.4611\n",
            "examples_seen=527,360 | approx_epochs=0.3540\n",
            "batch_shapes input=(16, 27) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.844 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.856 | chrf=14.913 | comet=nan | text_eval_sec=4.41\n",
            "========================================================================================\n",
            "step   4160/32000 | avg_loss=4.1363 | train_ppl=62.5716 | lr=0.00029102 | grad_norm=2.2098\n",
            "interval_sec=19.33 | steps/sec=2.0697 | sec/step=0.4832\n",
            "examples_seen=532,480 | approx_epochs=0.3574\n",
            "batch_shapes input=(16, 21) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.864 | chrf=15.361 | comet=nan | text_eval_sec=4.40\n",
            "========================================================================================\n",
            "step   4200/32000 | avg_loss=4.0988 | train_ppl=60.2665 | lr=0.000290826 | grad_norm=2.1317\n",
            "interval_sec=18.50 | steps/sec=2.1621 | sec/step=0.4625\n",
            "examples_seen=537,600 | approx_epochs=0.3608\n",
            "batch_shapes input=(16, 15) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.271 | chrf=17.416 | comet=nan | text_eval_sec=4.64\n",
            "========================================================================================\n",
            "step   4240/32000 | avg_loss=4.1067 | train_ppl=60.7436 | lr=0.00029063 | grad_norm=2.0848\n",
            "interval_sec=18.17 | steps/sec=2.2010 | sec/step=0.4543\n",
            "examples_seen=542,720 | approx_epochs=0.3643\n",
            "batch_shapes input=(16, 19) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.841 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.136 | chrf=16.359 | comet=nan | text_eval_sec=4.41\n",
            "========================================================================================\n",
            "step   4280/32000 | avg_loss=4.1304 | train_ppl=62.2052 | lr=0.000290432 | grad_norm=2.3685\n",
            "interval_sec=19.00 | steps/sec=2.1053 | sec/step=0.4750\n",
            "examples_seen=547,840 | approx_epochs=0.3677\n",
            "batch_shapes input=(16, 16) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.875 | chrf=15.941 | comet=nan | text_eval_sec=4.38\n",
            "========================================================================================\n",
            "step   4320/32000 | avg_loss=4.1138 | train_ppl=61.1816 | lr=0.000290232 | grad_norm=2.2323\n",
            "interval_sec=18.37 | steps/sec=2.1772 | sec/step=0.4593\n",
            "examples_seen=552,960 | approx_epochs=0.3712\n",
            "batch_shapes input=(16, 18) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.907 | chrf=18.432 | comet=nan | text_eval_sec=4.78\n",
            "========================================================================================\n",
            "step   4360/32000 | avg_loss=4.1101 | train_ppl=60.9517 | lr=0.00029003 | grad_norm=2.1320\n",
            "interval_sec=18.70 | steps/sec=2.1387 | sec/step=0.4676\n",
            "examples_seen=558,080 | approx_epochs=0.3746\n",
            "batch_shapes input=(16, 21) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.022 | chrf=16.540 | comet=nan | text_eval_sec=4.63\n",
            "========================================================================================\n",
            "step   4400/32000 | avg_loss=4.1112 | train_ppl=61.0229 | lr=0.000289826 | grad_norm=2.2344\n",
            "interval_sec=18.78 | steps/sec=2.1303 | sec/step=0.4694\n",
            "examples_seen=563,200 | approx_epochs=0.3780\n",
            "batch_shapes input=(16, 24) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.040 | chrf=18.309 | comet=nan | text_eval_sec=4.46\n",
            "[eval] step=4400 val_loss=3.9882 | val_ppl=53.9555 | bleu=1.030 | chrf=16.450 | comet=nan | text_eval_sec=36.00\n",
            "[best] new best val_loss=3.9882 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step   4440/32000 | avg_loss=4.0449 | train_ppl=57.1030 | lr=0.000289621 | grad_norm=2.2726\n",
            "interval_sec=65.12 | steps/sec=0.6142 | sec/step=1.6281\n",
            "examples_seen=568,320 | approx_epochs=0.3815\n",
            "batch_shapes input=(16, 16) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.240 | chrf=13.834 | comet=nan | text_eval_sec=4.48\n",
            "========================================================================================\n",
            "step   4480/32000 | avg_loss=4.0561 | train_ppl=57.7475 | lr=0.000289413 | grad_norm=2.3063\n",
            "interval_sec=18.81 | steps/sec=2.1261 | sec/step=0.4703\n",
            "examples_seen=573,440 | approx_epochs=0.3849\n",
            "batch_shapes input=(16, 27) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.944 | chrf=17.147 | comet=nan | text_eval_sec=4.22\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_004500\n",
            "========================================================================================\n",
            "step   4520/32000 | avg_loss=4.0488 | train_ppl=57.3297 | lr=0.000289203 | grad_norm=2.1242\n",
            "interval_sec=21.73 | steps/sec=1.8407 | sec/step=0.5433\n",
            "examples_seen=578,560 | approx_epochs=0.3883\n",
            "batch_shapes input=(16, 21) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.699 | chrf=16.435 | comet=nan | text_eval_sec=4.43\n",
            "========================================================================================\n",
            "step   4560/32000 | avg_loss=4.0299 | train_ppl=56.2576 | lr=0.000288991 | grad_norm=2.1882\n",
            "interval_sec=18.92 | steps/sec=2.1138 | sec/step=0.4731\n",
            "examples_seen=583,680 | approx_epochs=0.3918\n",
            "batch_shapes input=(16, 19) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.113 | chrf=18.105 | comet=nan | text_eval_sec=4.11\n",
            "========================================================================================\n",
            "step   4600/32000 | avg_loss=4.0331 | train_ppl=56.4346 | lr=0.000288777 | grad_norm=2.1378\n",
            "interval_sec=16.97 | steps/sec=2.3569 | sec/step=0.4243\n",
            "examples_seen=588,800 | approx_epochs=0.3952\n",
            "batch_shapes input=(16, 25) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.845 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.059 | chrf=15.566 | comet=nan | text_eval_sec=3.93\n",
            "========================================================================================\n",
            "step   4640/32000 | avg_loss=4.0295 | train_ppl=56.2342 | lr=0.000288561 | grad_norm=2.2945\n",
            "interval_sec=16.78 | steps/sec=2.3834 | sec/step=0.4196\n",
            "examples_seen=593,920 | approx_epochs=0.3986\n",
            "batch_shapes input=(16, 27) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.323 | chrf=19.108 | comet=nan | text_eval_sec=4.09\n",
            "========================================================================================\n",
            "step   4680/32000 | avg_loss=3.9743 | train_ppl=53.2146 | lr=0.000288343 | grad_norm=2.2351\n",
            "interval_sec=16.52 | steps/sec=2.4220 | sec/step=0.4129\n",
            "examples_seen=599,040 | approx_epochs=0.4021\n",
            "batch_shapes input=(16, 17) labels=(16, 14)\n",
            "cuda_mem_alloc_gb=0.824 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.070 | chrf=18.477 | comet=nan | text_eval_sec=4.07\n",
            "========================================================================================\n",
            "step   4720/32000 | avg_loss=3.9780 | train_ppl=53.4125 | lr=0.000288123 | grad_norm=2.2404\n",
            "interval_sec=18.74 | steps/sec=2.1350 | sec/step=0.4684\n",
            "examples_seen=604,160 | approx_epochs=0.4055\n",
            "batch_shapes input=(16, 53) labels=(16, 64)\n",
            "cuda_mem_alloc_gb=0.921 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.907 | chrf=15.530 | comet=nan | text_eval_sec=4.30\n",
            "========================================================================================\n",
            "step   4760/32000 | avg_loss=3.9821 | train_ppl=53.6291 | lr=0.000287902 | grad_norm=2.1928\n",
            "interval_sec=17.26 | steps/sec=2.3178 | sec/step=0.4314\n",
            "examples_seen=609,280 | approx_epochs=0.4090\n",
            "batch_shapes input=(16, 60) labels=(16, 67)\n",
            "cuda_mem_alloc_gb=0.927 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.840 | chrf=16.399 | comet=nan | text_eval_sec=4.36\n",
            "========================================================================================\n",
            "step   4800/32000 | avg_loss=4.0069 | train_ppl=54.9759 | lr=0.000287678 | grad_norm=2.1753\n",
            "interval_sec=16.75 | steps/sec=2.3880 | sec/step=0.4188\n",
            "examples_seen=614,400 | approx_epochs=0.4124\n",
            "batch_shapes input=(16, 19) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.304 | chrf=16.250 | comet=nan | text_eval_sec=4.17\n",
            "[eval] step=4800 val_loss=3.8277 | val_ppl=45.9576 | bleu=1.252 | chrf=17.852 | comet=nan | text_eval_sec=33.08\n",
            "[best] new best val_loss=3.8277 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step   4840/32000 | avg_loss=3.9607 | train_ppl=52.4945 | lr=0.000287452 | grad_norm=2.2794\n",
            "interval_sec=58.13 | steps/sec=0.6882 | sec/step=1.4531\n",
            "examples_seen=619,520 | approx_epochs=0.4158\n",
            "batch_shapes input=(16, 20) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.506 | chrf=18.743 | comet=nan | text_eval_sec=5.19\n",
            "========================================================================================\n",
            "step   4880/32000 | avg_loss=3.9694 | train_ppl=52.9540 | lr=0.000287225 | grad_norm=2.3130\n",
            "interval_sec=17.80 | steps/sec=2.2469 | sec/step=0.4450\n",
            "examples_seen=624,640 | approx_epochs=0.4193\n",
            "batch_shapes input=(16, 20) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.957 | chrf=16.204 | comet=nan | text_eval_sec=4.06\n",
            "========================================================================================\n",
            "step   4920/32000 | avg_loss=3.9151 | train_ppl=50.1539 | lr=0.000286995 | grad_norm=2.1647\n",
            "interval_sec=17.43 | steps/sec=2.2952 | sec/step=0.4357\n",
            "examples_seen=629,760 | approx_epochs=0.4227\n",
            "batch_shapes input=(16, 28) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.846 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.027 | chrf=16.771 | comet=nan | text_eval_sec=4.03\n",
            "========================================================================================\n",
            "step   4960/32000 | avg_loss=3.9555 | train_ppl=52.2210 | lr=0.000286764 | grad_norm=2.2269\n",
            "interval_sec=16.99 | steps/sec=2.3547 | sec/step=0.4247\n",
            "examples_seen=634,880 | approx_epochs=0.4261\n",
            "batch_shapes input=(16, 20) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.113 | chrf=16.226 | comet=nan | text_eval_sec=4.19\n",
            "========================================================================================\n",
            "step   5000/32000 | avg_loss=3.8420 | train_ppl=46.6206 | lr=0.000286531 | grad_norm=2.2356\n",
            "interval_sec=17.61 | steps/sec=2.2715 | sec/step=0.4402\n",
            "examples_seen=640,000 | approx_epochs=0.4296\n",
            "batch_shapes input=(16, 15) labels=(16, 14)\n",
            "cuda_mem_alloc_gb=0.824 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.521 | chrf=21.512 | comet=nan | text_eval_sec=4.22\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_005000\n",
            "========================================================================================\n",
            "step   5040/32000 | avg_loss=3.9112 | train_ppl=49.9593 | lr=0.000286295 | grad_norm=2.3695\n",
            "interval_sec=17.82 | steps/sec=2.2446 | sec/step=0.4455\n",
            "examples_seen=645,120 | approx_epochs=0.4330\n",
            "batch_shapes input=(16, 16) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.009 | chrf=17.446 | comet=nan | text_eval_sec=4.02\n",
            "========================================================================================\n",
            "step   5080/32000 | avg_loss=3.8840 | train_ppl=48.6202 | lr=0.000286058 | grad_norm=2.2387\n",
            "interval_sec=16.75 | steps/sec=2.3883 | sec/step=0.4187\n",
            "examples_seen=650,240 | approx_epochs=0.4364\n",
            "batch_shapes input=(16, 24) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.050 | chrf=16.738 | comet=nan | text_eval_sec=4.55\n",
            "========================================================================================\n",
            "step   5120/32000 | avg_loss=3.8724 | train_ppl=48.0578 | lr=0.000285819 | grad_norm=2.2268\n",
            "interval_sec=16.26 | steps/sec=2.4604 | sec/step=0.4064\n",
            "examples_seen=655,360 | approx_epochs=0.4399\n",
            "batch_shapes input=(16, 63) labels=(16, 58)\n",
            "cuda_mem_alloc_gb=0.910 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.198 | chrf=17.438 | comet=nan | text_eval_sec=4.28\n",
            "========================================================================================\n",
            "step   5160/32000 | avg_loss=3.9505 | train_ppl=51.9593 | lr=0.000285578 | grad_norm=2.3122\n",
            "interval_sec=16.22 | steps/sec=2.4668 | sec/step=0.4054\n",
            "examples_seen=660,480 | approx_epochs=0.4433\n",
            "batch_shapes input=(16, 55) labels=(16, 51)\n",
            "cuda_mem_alloc_gb=0.896 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.013 | chrf=16.822 | comet=nan | text_eval_sec=3.63\n",
            "========================================================================================\n",
            "step   5200/32000 | avg_loss=3.8923 | train_ppl=49.0219 | lr=0.000285335 | grad_norm=2.2597\n",
            "interval_sec=16.05 | steps/sec=2.4928 | sec/step=0.4012\n",
            "examples_seen=665,600 | approx_epochs=0.4468\n",
            "batch_shapes input=(16, 16) labels=(16, 14)\n",
            "cuda_mem_alloc_gb=0.824 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.246 | chrf=17.062 | comet=nan | text_eval_sec=3.70\n",
            "[eval] step=5200 val_loss=3.7570 | val_ppl=42.8193 | bleu=1.205 | chrf=17.874 | comet=nan | text_eval_sec=29.06\n",
            "[best] new best val_loss=3.7570 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step   5240/32000 | avg_loss=3.8575 | train_ppl=47.3450 | lr=0.00028509 | grad_norm=2.3631\n",
            "interval_sec=51.98 | steps/sec=0.7695 | sec/step=1.2996\n",
            "examples_seen=670,720 | approx_epochs=0.4502\n",
            "batch_shapes input=(16, 20) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.815 | chrf=17.667 | comet=nan | text_eval_sec=3.65\n",
            "========================================================================================\n",
            "step   5280/32000 | avg_loss=3.8781 | train_ppl=48.3340 | lr=0.000284843 | grad_norm=2.2736\n",
            "interval_sec=16.19 | steps/sec=2.4702 | sec/step=0.4048\n",
            "examples_seen=675,840 | approx_epochs=0.4536\n",
            "batch_shapes input=(16, 25) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.844 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.107 | chrf=16.685 | comet=nan | text_eval_sec=3.66\n",
            "========================================================================================\n",
            "step   5320/32000 | avg_loss=3.8542 | train_ppl=47.1887 | lr=0.000284594 | grad_norm=2.5680\n",
            "interval_sec=16.21 | steps/sec=2.4681 | sec/step=0.4052\n",
            "examples_seen=680,960 | approx_epochs=0.4571\n",
            "batch_shapes input=(16, 20) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.698 | chrf=17.036 | comet=nan | text_eval_sec=3.68\n",
            "========================================================================================\n",
            "step   5360/32000 | avg_loss=3.8434 | train_ppl=46.6819 | lr=0.000284344 | grad_norm=2.3504\n",
            "interval_sec=16.16 | steps/sec=2.4754 | sec/step=0.4040\n",
            "examples_seen=686,080 | approx_epochs=0.4605\n",
            "batch_shapes input=(16, 43) labels=(16, 49)\n",
            "cuda_mem_alloc_gb=0.892 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.896 | chrf=15.739 | comet=nan | text_eval_sec=3.70\n",
            "========================================================================================\n",
            "step   5400/32000 | avg_loss=3.8452 | train_ppl=46.7683 | lr=0.000284091 | grad_norm=2.2762\n",
            "interval_sec=16.23 | steps/sec=2.4640 | sec/step=0.4058\n",
            "examples_seen=691,200 | approx_epochs=0.4639\n",
            "batch_shapes input=(16, 27) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.846 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.108 | chrf=17.871 | comet=nan | text_eval_sec=3.61\n",
            "========================================================================================\n",
            "step   5440/32000 | avg_loss=3.8032 | train_ppl=44.8424 | lr=0.000283837 | grad_norm=2.3840\n",
            "interval_sec=16.27 | steps/sec=2.4582 | sec/step=0.4068\n",
            "examples_seen=696,320 | approx_epochs=0.4674\n",
            "batch_shapes input=(16, 22) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.728 | chrf=15.008 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step   5480/32000 | avg_loss=3.8557 | train_ppl=47.2604 | lr=0.00028358 | grad_norm=2.2202\n",
            "interval_sec=16.13 | steps/sec=2.4796 | sec/step=0.4033\n",
            "examples_seen=701,440 | approx_epochs=0.4708\n",
            "batch_shapes input=(16, 23) labels=(16, 27)\n",
            "cuda_mem_alloc_gb=0.849 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.625 | chrf=14.015 | comet=nan | text_eval_sec=3.61\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_005500\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_000500\n",
            "========================================================================================\n",
            "step   5520/32000 | avg_loss=3.8418 | train_ppl=46.6099 | lr=0.000283322 | grad_norm=2.2736\n",
            "interval_sec=18.08 | steps/sec=2.2129 | sec/step=0.4519\n",
            "examples_seen=706,560 | approx_epochs=0.4742\n",
            "batch_shapes input=(16, 20) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=0.939 | chrf=18.063 | comet=nan | text_eval_sec=3.78\n",
            "========================================================================================\n",
            "step   5560/32000 | avg_loss=3.7832 | train_ppl=43.9580 | lr=0.000283062 | grad_norm=2.2275\n",
            "interval_sec=16.24 | steps/sec=2.4626 | sec/step=0.4061\n",
            "examples_seen=711,680 | approx_epochs=0.4777\n",
            "batch_shapes input=(16, 25) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.200 | chrf=17.460 | comet=nan | text_eval_sec=3.77\n",
            "========================================================================================\n",
            "step   5600/32000 | avg_loss=3.7974 | train_ppl=44.5847 | lr=0.0002828 | grad_norm=2.3151\n",
            "interval_sec=16.10 | steps/sec=2.4847 | sec/step=0.4025\n",
            "examples_seen=716,800 | approx_epochs=0.4811\n",
            "batch_shapes input=(16, 22) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.207 | chrf=17.860 | comet=nan | text_eval_sec=3.75\n",
            "[eval] step=5600 val_loss=3.6762 | val_ppl=39.4973 | bleu=1.084 | chrf=16.749 | comet=nan | text_eval_sec=29.50\n",
            "[best] new best val_loss=3.6762 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step   5640/32000 | avg_loss=3.8228 | train_ppl=45.7316 | lr=0.000282536 | grad_norm=2.3130\n",
            "interval_sec=51.86 | steps/sec=0.7714 | sec/step=1.2964\n",
            "examples_seen=721,920 | approx_epochs=0.4846\n",
            "batch_shapes input=(16, 59) labels=(16, 53)\n",
            "cuda_mem_alloc_gb=0.900 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.178 | chrf=20.711 | comet=nan | text_eval_sec=4.41\n",
            "========================================================================================\n",
            "step   5680/32000 | avg_loss=3.7554 | train_ppl=42.7525 | lr=0.000282271 | grad_norm=2.2678\n",
            "interval_sec=15.84 | steps/sec=2.5259 | sec/step=0.3959\n",
            "examples_seen=727,040 | approx_epochs=0.4880\n",
            "batch_shapes input=(16, 28) labels=(16, 28)\n",
            "cuda_mem_alloc_gb=0.851 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.342 | chrf=18.946 | comet=nan | text_eval_sec=3.52\n",
            "========================================================================================\n",
            "step   5720/32000 | avg_loss=3.7834 | train_ppl=43.9632 | lr=0.000282003 | grad_norm=2.3573\n",
            "interval_sec=15.54 | steps/sec=2.5736 | sec/step=0.3886\n",
            "examples_seen=732,160 | approx_epochs=0.4914\n",
            "batch_shapes input=(16, 63) labels=(16, 54)\n",
            "cuda_mem_alloc_gb=0.902 | cuda_mem_peak_gb=2.747\n",
            "[text@log] samples=64 | bleu=1.472 | chrf=19.886 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step   5760/32000 | avg_loss=3.7679 | train_ppl=43.2876 | lr=0.000281734 | grad_norm=2.3419\n",
            "interval_sec=15.58 | steps/sec=2.5671 | sec/step=0.3895\n",
            "examples_seen=737,280 | approx_epochs=0.4949\n",
            "batch_shapes input=(16, 24) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.845 | cuda_mem_peak_gb=2.750\n",
            "[text@log] samples=64 | bleu=0.864 | chrf=17.305 | comet=nan | text_eval_sec=3.54\n",
            "========================================================================================\n",
            "step   5800/32000 | avg_loss=3.7626 | train_ppl=43.0586 | lr=0.000281462 | grad_norm=2.5790\n",
            "interval_sec=15.74 | steps/sec=2.5414 | sec/step=0.3935\n",
            "examples_seen=742,400 | approx_epochs=0.4983\n",
            "batch_shapes input=(16, 13) labels=(16, 11)\n",
            "cuda_mem_alloc_gb=0.818 | cuda_mem_peak_gb=2.750\n",
            "[text@log] samples=64 | bleu=1.043 | chrf=17.259 | comet=nan | text_eval_sec=3.49\n",
            "========================================================================================\n",
            "step   5840/32000 | avg_loss=3.7379 | train_ppl=42.0083 | lr=0.000281189 | grad_norm=2.4502\n",
            "interval_sec=15.60 | steps/sec=2.5641 | sec/step=0.3900\n",
            "examples_seen=747,520 | approx_epochs=0.5017\n",
            "batch_shapes input=(16, 71) labels=(16, 65)\n",
            "cuda_mem_alloc_gb=0.923 | cuda_mem_peak_gb=2.750\n",
            "[text@log] samples=64 | bleu=0.924 | chrf=17.233 | comet=nan | text_eval_sec=3.53\n",
            "========================================================================================\n",
            "step   5880/32000 | avg_loss=3.7543 | train_ppl=42.7056 | lr=0.000280914 | grad_norm=2.4796\n",
            "interval_sec=15.65 | steps/sec=2.5553 | sec/step=0.3914\n",
            "examples_seen=752,640 | approx_epochs=0.5052\n",
            "batch_shapes input=(16, 21) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.750\n",
            "[text@log] samples=64 | bleu=0.932 | chrf=16.948 | comet=nan | text_eval_sec=3.48\n",
            "========================================================================================\n",
            "step   5920/32000 | avg_loss=3.7472 | train_ppl=42.4038 | lr=0.000280637 | grad_norm=2.4592\n",
            "interval_sec=15.58 | steps/sec=2.5679 | sec/step=0.3894\n",
            "examples_seen=757,760 | approx_epochs=0.5086\n",
            "batch_shapes input=(16, 19) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.750\n",
            "[text@log] samples=64 | bleu=1.133 | chrf=17.152 | comet=nan | text_eval_sec=3.52\n",
            "========================================================================================\n",
            "step   5960/32000 | avg_loss=3.7431 | train_ppl=42.2270 | lr=0.000280359 | grad_norm=2.2994\n",
            "interval_sec=15.68 | steps/sec=2.5503 | sec/step=0.3921\n",
            "examples_seen=762,880 | approx_epochs=0.5121\n",
            "batch_shapes input=(16, 66) labels=(16, 53)\n",
            "cuda_mem_alloc_gb=0.900 | cuda_mem_peak_gb=2.750\n",
            "[text@log] samples=64 | bleu=1.143 | chrf=20.112 | comet=nan | text_eval_sec=3.51\n",
            "========================================================================================\n",
            "step   6000/32000 | avg_loss=3.7024 | train_ppl=40.5440 | lr=0.000280078 | grad_norm=2.4144\n",
            "interval_sec=15.79 | steps/sec=2.5335 | sec/step=0.3947\n",
            "examples_seen=768,000 | approx_epochs=0.5155\n",
            "batch_shapes input=(16, 18) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.750\n",
            "[text@log] samples=64 | bleu=0.682 | chrf=15.698 | comet=nan | text_eval_sec=3.55\n",
            "[eval] step=6000 val_loss=3.5521 | val_ppl=34.8878 | bleu=1.185 | chrf=17.247 | comet=nan | text_eval_sec=27.76\n",
            "[best] new best val_loss=3.5521 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_006000\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_001000\n",
            "========================================================================================\n",
            "step   6040/32000 | avg_loss=3.7003 | train_ppl=40.4574 | lr=0.000279796 | grad_norm=2.2555\n",
            "interval_sec=50.36 | steps/sec=0.7942 | sec/step=1.2591\n",
            "examples_seen=773,120 | approx_epochs=0.5189\n",
            "batch_shapes input=(16, 28) labels=(16, 27)\n",
            "cuda_mem_alloc_gb=0.849 | cuda_mem_peak_gb=2.750\n",
            "[text@log] samples=64 | bleu=0.912 | chrf=14.350 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step   6080/32000 | avg_loss=3.6992 | train_ppl=40.4138 | lr=0.000279512 | grad_norm=2.3638\n",
            "interval_sec=15.63 | steps/sec=2.5592 | sec/step=0.3907\n",
            "examples_seen=778,240 | approx_epochs=0.5224\n",
            "batch_shapes input=(16, 22) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.750\n",
            "[text@log] samples=64 | bleu=1.361 | chrf=19.051 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step   6120/32000 | avg_loss=3.6917 | train_ppl=40.1145 | lr=0.000279226 | grad_norm=2.4537\n",
            "interval_sec=15.69 | steps/sec=2.5497 | sec/step=0.3922\n",
            "examples_seen=783,360 | approx_epochs=0.5258\n",
            "batch_shapes input=(16, 23) labels=(16, 28)\n",
            "cuda_mem_alloc_gb=0.851 | cuda_mem_peak_gb=2.750\n",
            "[text@log] samples=64 | bleu=0.931 | chrf=14.910 | comet=nan | text_eval_sec=3.49\n",
            "========================================================================================\n",
            "step   6160/32000 | avg_loss=3.6933 | train_ppl=40.1777 | lr=0.000278938 | grad_norm=2.3464\n",
            "interval_sec=15.69 | steps/sec=2.5488 | sec/step=0.3923\n",
            "examples_seen=788,480 | approx_epochs=0.5292\n",
            "batch_shapes input=(16, 17) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.750\n",
            "[text@log] samples=64 | bleu=0.827 | chrf=16.469 | comet=nan | text_eval_sec=3.72\n",
            "========================================================================================\n",
            "step   6200/32000 | avg_loss=3.7588 | train_ppl=42.8986 | lr=0.000278649 | grad_norm=2.3033\n",
            "interval_sec=15.91 | steps/sec=2.5138 | sec/step=0.3978\n",
            "examples_seen=793,600 | approx_epochs=0.5327\n",
            "batch_shapes input=(16, 23) labels=(16, 26)\n",
            "cuda_mem_alloc_gb=0.847 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.206 | chrf=18.772 | comet=nan | text_eval_sec=3.50\n",
            "========================================================================================\n",
            "step   6240/32000 | avg_loss=3.6745 | train_ppl=39.4282 | lr=0.000278357 | grad_norm=2.4340\n",
            "interval_sec=15.71 | steps/sec=2.5465 | sec/step=0.3927\n",
            "examples_seen=798,720 | approx_epochs=0.5361\n",
            "batch_shapes input=(16, 16) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.066 | chrf=16.725 | comet=nan | text_eval_sec=3.49\n",
            "========================================================================================\n",
            "step   6280/32000 | avg_loss=3.7000 | train_ppl=40.4467 | lr=0.000278064 | grad_norm=2.5287\n",
            "interval_sec=15.78 | steps/sec=2.5350 | sec/step=0.3945\n",
            "examples_seen=803,840 | approx_epochs=0.5395\n",
            "batch_shapes input=(16, 18) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.889 | chrf=17.076 | comet=nan | text_eval_sec=4.11\n",
            "========================================================================================\n",
            "step   6320/32000 | avg_loss=3.6610 | train_ppl=38.8984 | lr=0.000277769 | grad_norm=2.3422\n",
            "interval_sec=16.26 | steps/sec=2.4598 | sec/step=0.4065\n",
            "examples_seen=808,960 | approx_epochs=0.5430\n",
            "batch_shapes input=(16, 21) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.155 | chrf=18.161 | comet=nan | text_eval_sec=3.78\n",
            "========================================================================================\n",
            "step   6360/32000 | avg_loss=3.6826 | train_ppl=39.7484 | lr=0.000277472 | grad_norm=2.4713\n",
            "interval_sec=16.19 | steps/sec=2.4707 | sec/step=0.4047\n",
            "examples_seen=814,080 | approx_epochs=0.5464\n",
            "batch_shapes input=(16, 27) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.844 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.001 | chrf=15.508 | comet=nan | text_eval_sec=3.76\n",
            "========================================================================================\n",
            "step   6400/32000 | avg_loss=3.6129 | train_ppl=37.0720 | lr=0.000277174 | grad_norm=2.2856\n",
            "interval_sec=16.33 | steps/sec=2.4499 | sec/step=0.4082\n",
            "examples_seen=819,200 | approx_epochs=0.5499\n",
            "batch_shapes input=(16, 20) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.960 | chrf=18.812 | comet=nan | text_eval_sec=3.89\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=6400 val_loss=3.4819 | val_ppl=32.5215 | bleu=0.997 | chrf=19.538 | comet=nan | text_eval_sec=30.38\n",
            "[best] new best val_loss=3.4819 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step   6440/32000 | avg_loss=3.6105 | train_ppl=36.9849 | lr=0.000276873 | grad_norm=2.3452\n",
            "interval_sec=55.28 | steps/sec=0.7236 | sec/step=1.3819\n",
            "examples_seen=824,320 | approx_epochs=0.5533\n",
            "batch_shapes input=(16, 26) labels=(16, 28)\n",
            "cuda_mem_alloc_gb=0.851 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.079 | chrf=19.264 | comet=nan | text_eval_sec=4.65\n",
            "========================================================================================\n",
            "step   6480/32000 | avg_loss=3.6400 | train_ppl=38.0924 | lr=0.000276571 | grad_norm=2.7213\n",
            "interval_sec=17.00 | steps/sec=2.3525 | sec/step=0.4251\n",
            "examples_seen=829,440 | approx_epochs=0.5567\n",
            "batch_shapes input=(16, 16) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.007 | chrf=17.205 | comet=nan | text_eval_sec=3.89\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_006500\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_001500\n",
            "========================================================================================\n",
            "step   6520/32000 | avg_loss=3.6857 | train_ppl=39.8746 | lr=0.000276267 | grad_norm=2.4326\n",
            "interval_sec=19.34 | steps/sec=2.0680 | sec/step=0.4836\n",
            "examples_seen=834,560 | approx_epochs=0.5602\n",
            "batch_shapes input=(16, 18) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.988 | chrf=20.859 | comet=nan | text_eval_sec=3.91\n",
            "========================================================================================\n",
            "step   6560/32000 | avg_loss=3.6668 | train_ppl=39.1252 | lr=0.000275962 | grad_norm=2.4517\n",
            "interval_sec=16.89 | steps/sec=2.3685 | sec/step=0.4222\n",
            "examples_seen=839,680 | approx_epochs=0.5636\n",
            "batch_shapes input=(16, 20) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.111 | chrf=20.998 | comet=nan | text_eval_sec=3.92\n",
            "========================================================================================\n",
            "step   6600/32000 | avg_loss=3.7481 | train_ppl=42.4412 | lr=0.000275654 | grad_norm=2.5336\n",
            "interval_sec=16.82 | steps/sec=2.3787 | sec/step=0.4204\n",
            "examples_seen=844,800 | approx_epochs=0.5670\n",
            "batch_shapes input=(16, 17) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.790 | chrf=16.943 | comet=nan | text_eval_sec=4.00\n",
            "========================================================================================\n",
            "step   6640/32000 | avg_loss=3.6159 | train_ppl=37.1848 | lr=0.000275345 | grad_norm=2.4561\n",
            "interval_sec=16.80 | steps/sec=2.3816 | sec/step=0.4199\n",
            "examples_seen=849,920 | approx_epochs=0.5705\n",
            "batch_shapes input=(16, 21) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.061 | chrf=17.392 | comet=nan | text_eval_sec=3.81\n",
            "========================================================================================\n",
            "step   6680/32000 | avg_loss=3.5845 | train_ppl=36.0353 | lr=0.000275034 | grad_norm=2.2920\n",
            "interval_sec=16.85 | steps/sec=2.3744 | sec/step=0.4212\n",
            "examples_seen=855,040 | approx_epochs=0.5739\n",
            "batch_shapes input=(16, 24) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.442 | chrf=19.356 | comet=nan | text_eval_sec=3.82\n",
            "========================================================================================\n",
            "step   6720/32000 | avg_loss=3.5433 | train_ppl=34.5812 | lr=0.000274721 | grad_norm=2.4802\n",
            "interval_sec=16.69 | steps/sec=2.3961 | sec/step=0.4173\n",
            "examples_seen=860,160 | approx_epochs=0.5773\n",
            "batch_shapes input=(16, 23) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.555 | chrf=17.500 | comet=nan | text_eval_sec=3.98\n",
            "========================================================================================\n",
            "step   6760/32000 | avg_loss=3.5997 | train_ppl=36.5860 | lr=0.000274407 | grad_norm=2.4611\n",
            "interval_sec=16.85 | steps/sec=2.3742 | sec/step=0.4212\n",
            "examples_seen=865,280 | approx_epochs=0.5808\n",
            "batch_shapes input=(16, 14) labels=(16, 13)\n",
            "cuda_mem_alloc_gb=0.822 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.063 | chrf=20.982 | comet=nan | text_eval_sec=3.88\n",
            "========================================================================================\n",
            "step   6800/32000 | avg_loss=3.6160 | train_ppl=37.1896 | lr=0.000274091 | grad_norm=2.2650\n",
            "interval_sec=16.67 | steps/sec=2.3991 | sec/step=0.4168\n",
            "examples_seen=870,400 | approx_epochs=0.5842\n",
            "batch_shapes input=(16, 88) labels=(16, 88)\n",
            "cuda_mem_alloc_gb=0.968 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.864 | chrf=21.184 | comet=nan | text_eval_sec=3.97\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=6800 val_loss=3.4598 | val_ppl=31.8092 | bleu=1.080 | chrf=20.494 | comet=nan | text_eval_sec=30.65\n",
            "[best] new best val_loss=3.4598 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step   6840/32000 | avg_loss=3.5918 | train_ppl=36.2981 | lr=0.000273773 | grad_norm=2.5576\n",
            "interval_sec=54.49 | steps/sec=0.7340 | sec/step=1.3623\n",
            "examples_seen=875,520 | approx_epochs=0.5877\n",
            "batch_shapes input=(16, 23) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.178 | chrf=16.173 | comet=nan | text_eval_sec=3.98\n",
            "========================================================================================\n",
            "step   6880/32000 | avg_loss=3.5788 | train_ppl=35.8313 | lr=0.000273453 | grad_norm=2.4082\n",
            "interval_sec=16.91 | steps/sec=2.3652 | sec/step=0.4228\n",
            "examples_seen=880,640 | approx_epochs=0.5911\n",
            "batch_shapes input=(16, 22) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.621 | chrf=18.419 | comet=nan | text_eval_sec=3.90\n",
            "========================================================================================\n",
            "step   6920/32000 | avg_loss=3.5321 | train_ppl=34.1950 | lr=0.000273132 | grad_norm=2.6712\n",
            "interval_sec=15.65 | steps/sec=2.5563 | sec/step=0.3912\n",
            "examples_seen=885,760 | approx_epochs=0.5945\n",
            "batch_shapes input=(16, 30) labels=(16, 27)\n",
            "cuda_mem_alloc_gb=0.849 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.681 | chrf=18.268 | comet=nan | text_eval_sec=3.59\n",
            "========================================================================================\n",
            "step   6960/32000 | avg_loss=3.5454 | train_ppl=34.6535 | lr=0.000272809 | grad_norm=2.4246\n",
            "interval_sec=15.60 | steps/sec=2.5645 | sec/step=0.3899\n",
            "examples_seen=890,880 | approx_epochs=0.5980\n",
            "batch_shapes input=(16, 22) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.845 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.496 | chrf=18.527 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step   7000/32000 | avg_loss=3.4868 | train_ppl=32.6822 | lr=0.000272484 | grad_norm=2.6113\n",
            "interval_sec=15.55 | steps/sec=2.5716 | sec/step=0.3889\n",
            "examples_seen=896,000 | approx_epochs=0.6014\n",
            "batch_shapes input=(16, 23) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.066 | chrf=18.679 | comet=nan | text_eval_sec=3.58\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_007000\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_002000\n",
            "========================================================================================\n",
            "step   7040/32000 | avg_loss=3.6052 | train_ppl=36.7896 | lr=0.000272157 | grad_norm=2.3607\n",
            "interval_sec=16.60 | steps/sec=2.4091 | sec/step=0.4151\n",
            "examples_seen=901,120 | approx_epochs=0.6048\n",
            "batch_shapes input=(16, 17) labels=(16, 14)\n",
            "cuda_mem_alloc_gb=0.824 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.259 | chrf=17.359 | comet=nan | text_eval_sec=5.09\n",
            "========================================================================================\n",
            "step   7080/32000 | avg_loss=3.5287 | train_ppl=34.0797 | lr=0.000271829 | grad_norm=2.4242\n",
            "interval_sec=15.51 | steps/sec=2.5796 | sec/step=0.3877\n",
            "examples_seen=906,240 | approx_epochs=0.6083\n",
            "batch_shapes input=(16, 21) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.396 | chrf=21.612 | comet=nan | text_eval_sec=3.54\n",
            "========================================================================================\n",
            "step   7120/32000 | avg_loss=3.5455 | train_ppl=34.6567 | lr=0.000271499 | grad_norm=2.4945\n",
            "interval_sec=15.44 | steps/sec=2.5914 | sec/step=0.3859\n",
            "examples_seen=911,360 | approx_epochs=0.6117\n",
            "batch_shapes input=(16, 22) labels=(16, 30)\n",
            "cuda_mem_alloc_gb=0.855 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.331 | chrf=20.707 | comet=nan | text_eval_sec=3.53\n",
            "========================================================================================\n",
            "step   7160/32000 | avg_loss=3.5356 | train_ppl=34.3170 | lr=0.000271168 | grad_norm=2.7355\n",
            "interval_sec=15.46 | steps/sec=2.5870 | sec/step=0.3866\n",
            "examples_seen=916,480 | approx_epochs=0.6151\n",
            "batch_shapes input=(16, 20) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.256 | chrf=15.695 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step   7200/32000 | avg_loss=3.5680 | train_ppl=35.4460 | lr=0.000270834 | grad_norm=2.4186\n",
            "interval_sec=15.74 | steps/sec=2.5415 | sec/step=0.3935\n",
            "examples_seen=921,600 | approx_epochs=0.6186\n",
            "batch_shapes input=(16, 23) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.069 | chrf=18.185 | comet=nan | text_eval_sec=3.45\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=7200 val_loss=3.3746 | val_ppl=29.2113 | bleu=1.264 | chrf=17.932 | comet=nan | text_eval_sec=28.05\n",
            "[best] new best val_loss=3.3746 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step   7240/32000 | avg_loss=3.5119 | train_ppl=33.5129 | lr=0.000270499 | grad_norm=2.4592\n",
            "interval_sec=51.16 | steps/sec=0.7819 | sec/step=1.2790\n",
            "examples_seen=926,720 | approx_epochs=0.6220\n",
            "batch_shapes input=(16, 19) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.920 | chrf=15.724 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step   7280/32000 | avg_loss=3.5790 | train_ppl=35.8360 | lr=0.000270163 | grad_norm=2.3967\n",
            "interval_sec=15.81 | steps/sec=2.5299 | sec/step=0.3953\n",
            "examples_seen=931,840 | approx_epochs=0.6255\n",
            "batch_shapes input=(16, 17) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.113 | chrf=19.758 | comet=nan | text_eval_sec=3.47\n",
            "========================================================================================\n",
            "step   7320/32000 | avg_loss=3.5289 | train_ppl=34.0866 | lr=0.000269824 | grad_norm=2.5306\n",
            "interval_sec=15.73 | steps/sec=2.5432 | sec/step=0.3932\n",
            "examples_seen=936,960 | approx_epochs=0.6289\n",
            "batch_shapes input=(16, 27) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.844 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.062 | chrf=19.241 | comet=nan | text_eval_sec=3.55\n",
            "========================================================================================\n",
            "step   7360/32000 | avg_loss=3.5581 | train_ppl=35.0981 | lr=0.000269484 | grad_norm=2.6579\n",
            "interval_sec=15.49 | steps/sec=2.5826 | sec/step=0.3872\n",
            "examples_seen=942,080 | approx_epochs=0.6323\n",
            "batch_shapes input=(16, 27) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.827 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.170 | chrf=19.135 | comet=nan | text_eval_sec=3.52\n",
            "========================================================================================\n",
            "step   7400/32000 | avg_loss=3.4805 | train_ppl=32.4766 | lr=0.000269143 | grad_norm=2.5891\n",
            "interval_sec=15.56 | steps/sec=2.5702 | sec/step=0.3891\n",
            "examples_seen=947,200 | approx_epochs=0.6358\n",
            "batch_shapes input=(16, 19) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.101 | chrf=17.814 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step   7440/32000 | avg_loss=3.5278 | train_ppl=34.0488 | lr=0.000268799 | grad_norm=2.6024\n",
            "interval_sec=15.63 | steps/sec=2.5585 | sec/step=0.3908\n",
            "examples_seen=952,320 | approx_epochs=0.6392\n",
            "batch_shapes input=(16, 50) labels=(16, 58)\n",
            "cuda_mem_alloc_gb=0.909 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.524 | chrf=21.385 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step   7480/32000 | avg_loss=3.4937 | train_ppl=32.9074 | lr=0.000268454 | grad_norm=2.5831\n",
            "interval_sec=15.64 | steps/sec=2.5579 | sec/step=0.3909\n",
            "examples_seen=957,440 | approx_epochs=0.6426\n",
            "batch_shapes input=(16, 22) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.981 | chrf=17.385 | comet=nan | text_eval_sec=3.56\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_007500\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_002500\n",
            "========================================================================================\n",
            "step   7520/32000 | avg_loss=3.5089 | train_ppl=33.4116 | lr=0.000268108 | grad_norm=2.8250\n",
            "interval_sec=16.33 | steps/sec=2.4490 | sec/step=0.4083\n",
            "examples_seen=962,560 | approx_epochs=0.6461\n",
            "batch_shapes input=(16, 21) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.129 | chrf=18.598 | comet=nan | text_eval_sec=4.61\n",
            "========================================================================================\n",
            "step   7560/32000 | avg_loss=3.5321 | train_ppl=34.1961 | lr=0.00026776 | grad_norm=2.6446\n",
            "interval_sec=15.67 | steps/sec=2.5530 | sec/step=0.3917\n",
            "examples_seen=967,680 | approx_epochs=0.6495\n",
            "batch_shapes input=(16, 22) labels=(16, 27)\n",
            "cuda_mem_alloc_gb=0.849 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.283 | chrf=19.926 | comet=nan | text_eval_sec=3.55\n",
            "========================================================================================\n",
            "step   7600/32000 | avg_loss=3.4983 | train_ppl=33.0584 | lr=0.00026741 | grad_norm=2.5637\n",
            "interval_sec=15.56 | steps/sec=2.5705 | sec/step=0.3890\n",
            "examples_seen=972,800 | approx_epochs=0.6530\n",
            "batch_shapes input=(16, 20) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.221 | chrf=16.283 | comet=nan | text_eval_sec=3.64\n",
            "[eval] step=7600 val_loss=3.3279 | val_ppl=27.8805 | bleu=1.143 | chrf=18.330 | comet=nan | text_eval_sec=28.07\n",
            "[best] new best val_loss=3.3279 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step   7640/32000 | avg_loss=3.4945 | train_ppl=32.9340 | lr=0.000267058 | grad_norm=2.6034\n",
            "interval_sec=50.01 | steps/sec=0.7998 | sec/step=1.2504\n",
            "examples_seen=977,920 | approx_epochs=0.6564\n",
            "batch_shapes input=(16, 23) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.178 | chrf=19.586 | comet=nan | text_eval_sec=3.52\n",
            "========================================================================================\n",
            "step   7680/32000 | avg_loss=3.4866 | train_ppl=32.6745 | lr=0.000266705 | grad_norm=2.6826\n",
            "interval_sec=15.55 | steps/sec=2.5723 | sec/step=0.3888\n",
            "examples_seen=983,040 | approx_epochs=0.6598\n",
            "batch_shapes input=(16, 15) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.018 | chrf=15.933 | comet=nan | text_eval_sec=3.48\n",
            "========================================================================================\n",
            "step   7720/32000 | avg_loss=3.4736 | train_ppl=32.2530 | lr=0.00026635 | grad_norm=2.5916\n",
            "interval_sec=15.64 | steps/sec=2.5571 | sec/step=0.3911\n",
            "examples_seen=988,160 | approx_epochs=0.6633\n",
            "batch_shapes input=(16, 24) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.126 | chrf=17.438 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step   7760/32000 | avg_loss=3.4618 | train_ppl=31.8748 | lr=0.000265994 | grad_norm=2.3876\n",
            "interval_sec=15.73 | steps/sec=2.5436 | sec/step=0.3931\n",
            "examples_seen=993,280 | approx_epochs=0.6667\n",
            "batch_shapes input=(16, 23) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.929 | chrf=15.635 | comet=nan | text_eval_sec=3.54\n",
            "========================================================================================\n",
            "step   7800/32000 | avg_loss=3.4420 | train_ppl=31.2481 | lr=0.000265636 | grad_norm=2.4109\n",
            "interval_sec=15.69 | steps/sec=2.5487 | sec/step=0.3924\n",
            "examples_seen=998,400 | approx_epochs=0.6701\n",
            "batch_shapes input=(16, 22) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.536 | chrf=19.459 | comet=nan | text_eval_sec=3.64\n",
            "========================================================================================\n",
            "step   7840/32000 | avg_loss=3.4918 | train_ppl=32.8458 | lr=0.000265277 | grad_norm=2.4372\n",
            "interval_sec=15.87 | steps/sec=2.5210 | sec/step=0.3967\n",
            "examples_seen=1,003,520 | approx_epochs=0.6736\n",
            "batch_shapes input=(16, 21) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.163 | chrf=18.612 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step   7880/32000 | avg_loss=3.4433 | train_ppl=31.2905 | lr=0.000264915 | grad_norm=2.5735\n",
            "interval_sec=15.70 | steps/sec=2.5482 | sec/step=0.3924\n",
            "examples_seen=1,008,640 | approx_epochs=0.6770\n",
            "batch_shapes input=(16, 22) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.772 | chrf=21.691 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step   7920/32000 | avg_loss=3.4236 | train_ppl=30.6811 | lr=0.000264553 | grad_norm=2.7395\n",
            "interval_sec=15.60 | steps/sec=2.5643 | sec/step=0.3900\n",
            "examples_seen=1,013,760 | approx_epochs=0.6804\n",
            "batch_shapes input=(16, 22) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.527 | chrf=13.390 | comet=nan | text_eval_sec=3.54\n",
            "========================================================================================\n",
            "step   7960/32000 | avg_loss=3.4260 | train_ppl=30.7547 | lr=0.000264188 | grad_norm=2.7381\n",
            "interval_sec=15.71 | steps/sec=2.5469 | sec/step=0.3926\n",
            "examples_seen=1,018,880 | approx_epochs=0.6839\n",
            "batch_shapes input=(16, 20) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.218 | chrf=17.883 | comet=nan | text_eval_sec=3.55\n",
            "========================================================================================\n",
            "step   8000/32000 | avg_loss=3.4248 | train_ppl=30.7150 | lr=0.000263822 | grad_norm=2.7582\n",
            "interval_sec=15.63 | steps/sec=2.5596 | sec/step=0.3907\n",
            "examples_seen=1,024,000 | approx_epochs=0.6873\n",
            "batch_shapes input=(16, 23) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.845 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.211 | chrf=20.690 | comet=nan | text_eval_sec=3.58\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=8000 val_loss=3.2669 | val_ppl=26.2297 | bleu=1.210 | chrf=18.587 | comet=nan | text_eval_sec=28.24\n",
            "[best] new best val_loss=3.2669 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_008000\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_003000\n",
            "========================================================================================\n",
            "step   8040/32000 | avg_loss=3.4251 | train_ppl=30.7263 | lr=0.000263455 | grad_norm=2.7830\n",
            "interval_sec=51.03 | steps/sec=0.7839 | sec/step=1.2757\n",
            "examples_seen=1,029,120 | approx_epochs=0.6908\n",
            "batch_shapes input=(16, 16) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.184 | chrf=18.773 | comet=nan | text_eval_sec=4.06\n",
            "========================================================================================\n",
            "step   8080/32000 | avg_loss=3.4027 | train_ppl=30.0443 | lr=0.000263086 | grad_norm=2.5622\n",
            "interval_sec=15.64 | steps/sec=2.5571 | sec/step=0.3911\n",
            "examples_seen=1,034,240 | approx_epochs=0.6942\n",
            "batch_shapes input=(16, 20) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.141 | chrf=18.810 | comet=nan | text_eval_sec=3.50\n",
            "========================================================================================\n",
            "step   8120/32000 | avg_loss=3.4057 | train_ppl=30.1348 | lr=0.000262715 | grad_norm=2.5911\n",
            "interval_sec=15.67 | steps/sec=2.5523 | sec/step=0.3918\n",
            "examples_seen=1,039,360 | approx_epochs=0.6976\n",
            "batch_shapes input=(16, 22) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.562 | chrf=19.159 | comet=nan | text_eval_sec=5.20\n",
            "========================================================================================\n",
            "step   8160/32000 | avg_loss=3.4340 | train_ppl=30.9997 | lr=0.000262343 | grad_norm=2.8540\n",
            "interval_sec=15.58 | steps/sec=2.5682 | sec/step=0.3894\n",
            "examples_seen=1,044,480 | approx_epochs=0.7011\n",
            "batch_shapes input=(16, 15) labels=(16, 13)\n",
            "cuda_mem_alloc_gb=0.822 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.983 | chrf=20.045 | comet=nan | text_eval_sec=3.79\n",
            "========================================================================================\n",
            "step   8200/32000 | avg_loss=3.4306 | train_ppl=30.8964 | lr=0.00026197 | grad_norm=2.6268\n",
            "interval_sec=15.78 | steps/sec=2.5348 | sec/step=0.3945\n",
            "examples_seen=1,049,600 | approx_epochs=0.7045\n",
            "batch_shapes input=(16, 25) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.845 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.381 | chrf=16.745 | comet=nan | text_eval_sec=3.54\n",
            "========================================================================================\n",
            "step   8240/32000 | avg_loss=3.3434 | train_ppl=28.3164 | lr=0.000261594 | grad_norm=2.5675\n",
            "interval_sec=15.52 | steps/sec=2.5781 | sec/step=0.3879\n",
            "examples_seen=1,054,720 | approx_epochs=0.7079\n",
            "batch_shapes input=(16, 21) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.194 | chrf=21.395 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step   8280/32000 | avg_loss=3.3752 | train_ppl=29.2304 | lr=0.000261218 | grad_norm=3.1415\n",
            "interval_sec=15.61 | steps/sec=2.5623 | sec/step=0.3903\n",
            "examples_seen=1,059,840 | approx_epochs=0.7114\n",
            "batch_shapes input=(16, 24) labels=(16, 28)\n",
            "cuda_mem_alloc_gb=0.851 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.069 | chrf=15.721 | comet=nan | text_eval_sec=3.65\n",
            "========================================================================================\n",
            "step   8320/32000 | avg_loss=3.3917 | train_ppl=29.7160 | lr=0.000260839 | grad_norm=2.6146\n",
            "interval_sec=15.51 | steps/sec=2.5790 | sec/step=0.3878\n",
            "examples_seen=1,064,960 | approx_epochs=0.7148\n",
            "batch_shapes input=(16, 16) labels=(16, 14)\n",
            "cuda_mem_alloc_gb=0.824 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.967 | chrf=22.530 | comet=nan | text_eval_sec=3.52\n",
            "========================================================================================\n",
            "step   8360/32000 | avg_loss=3.4311 | train_ppl=30.9107 | lr=0.000260459 | grad_norm=2.5576\n",
            "interval_sec=15.50 | steps/sec=2.5798 | sec/step=0.3876\n",
            "examples_seen=1,070,080 | approx_epochs=0.7182\n",
            "batch_shapes input=(16, 18) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.705 | chrf=20.408 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step   8400/32000 | avg_loss=3.3429 | train_ppl=28.3006 | lr=0.000260078 | grad_norm=2.5530\n",
            "interval_sec=15.80 | steps/sec=2.5324 | sec/step=0.3949\n",
            "examples_seen=1,075,200 | approx_epochs=0.7217\n",
            "batch_shapes input=(16, 19) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.092 | chrf=18.639 | comet=nan | text_eval_sec=3.54\n",
            "[eval] step=8400 val_loss=3.2441 | val_ppl=25.6388 | bleu=1.192 | chrf=19.167 | comet=nan | text_eval_sec=28.01\n",
            "[best] new best val_loss=3.2441 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step   8440/32000 | avg_loss=3.4201 | train_ppl=30.5730 | lr=0.000259695 | grad_norm=2.7215\n",
            "interval_sec=49.73 | steps/sec=0.8043 | sec/step=1.2433\n",
            "examples_seen=1,080,320 | approx_epochs=0.7251\n",
            "batch_shapes input=(16, 21) labels=(16, 26)\n",
            "cuda_mem_alloc_gb=0.847 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.442 | chrf=17.120 | comet=nan | text_eval_sec=5.50\n",
            "========================================================================================\n",
            "step   8480/32000 | avg_loss=3.4232 | train_ppl=30.6676 | lr=0.000259311 | grad_norm=2.8384\n",
            "interval_sec=15.71 | steps/sec=2.5466 | sec/step=0.3927\n",
            "examples_seen=1,085,440 | approx_epochs=0.7286\n",
            "batch_shapes input=(16, 21) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.355 | chrf=19.513 | comet=nan | text_eval_sec=3.69\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_008500\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_003500\n",
            "========================================================================================\n",
            "step   8520/32000 | avg_loss=3.3884 | train_ppl=29.6200 | lr=0.000258925 | grad_norm=2.4623\n",
            "interval_sec=17.80 | steps/sec=2.2477 | sec/step=0.4449\n",
            "examples_seen=1,090,560 | approx_epochs=0.7320\n",
            "batch_shapes input=(16, 21) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.144 | chrf=15.689 | comet=nan | text_eval_sec=5.49\n",
            "========================================================================================\n",
            "step   8560/32000 | avg_loss=3.3655 | train_ppl=28.9492 | lr=0.000258538 | grad_norm=2.5576\n",
            "interval_sec=15.65 | steps/sec=2.5553 | sec/step=0.3913\n",
            "examples_seen=1,095,680 | approx_epochs=0.7354\n",
            "batch_shapes input=(16, 19) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.034 | chrf=16.164 | comet=nan | text_eval_sec=3.61\n",
            "========================================================================================\n",
            "step   8600/32000 | avg_loss=3.3750 | train_ppl=29.2233 | lr=0.000258149 | grad_norm=2.7159\n",
            "interval_sec=15.49 | steps/sec=2.5830 | sec/step=0.3871\n",
            "examples_seen=1,100,800 | approx_epochs=0.7389\n",
            "batch_shapes input=(16, 19) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.457 | chrf=20.356 | comet=nan | text_eval_sec=3.65\n",
            "========================================================================================\n",
            "step   8640/32000 | avg_loss=3.3603 | train_ppl=28.7977 | lr=0.000257759 | grad_norm=2.4897\n",
            "interval_sec=15.94 | steps/sec=2.5102 | sec/step=0.3984\n",
            "examples_seen=1,105,920 | approx_epochs=0.7423\n",
            "batch_shapes input=(16, 24) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.739 | chrf=23.680 | comet=nan | text_eval_sec=3.61\n",
            "========================================================================================\n",
            "step   8680/32000 | avg_loss=3.3597 | train_ppl=28.7811 | lr=0.000257367 | grad_norm=2.7743\n",
            "interval_sec=15.74 | steps/sec=2.5409 | sec/step=0.3936\n",
            "examples_seen=1,111,040 | approx_epochs=0.7457\n",
            "batch_shapes input=(16, 22) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.865 | chrf=15.871 | comet=nan | text_eval_sec=3.64\n",
            "========================================================================================\n",
            "step   8720/32000 | avg_loss=3.2923 | train_ppl=26.9058 | lr=0.000256974 | grad_norm=3.0343\n",
            "interval_sec=15.75 | steps/sec=2.5392 | sec/step=0.3938\n",
            "examples_seen=1,116,160 | approx_epochs=0.7492\n",
            "batch_shapes input=(16, 28) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.220 | chrf=18.906 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step   8760/32000 | avg_loss=3.2871 | train_ppl=26.7648 | lr=0.000256579 | grad_norm=2.7325\n",
            "interval_sec=16.24 | steps/sec=2.4638 | sec/step=0.4059\n",
            "examples_seen=1,121,280 | approx_epochs=0.7526\n",
            "batch_shapes input=(16, 25) labels=(16, 30)\n",
            "cuda_mem_alloc_gb=0.855 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.589 | chrf=19.153 | comet=nan | text_eval_sec=3.61\n",
            "========================================================================================\n",
            "step   8800/32000 | avg_loss=3.2916 | train_ppl=26.8856 | lr=0.000256183 | grad_norm=2.6515\n",
            "interval_sec=15.92 | steps/sec=2.5122 | sec/step=0.3981\n",
            "examples_seen=1,126,400 | approx_epochs=0.7560\n",
            "batch_shapes input=(16, 18) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.918 | chrf=19.379 | comet=nan | text_eval_sec=3.71\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=8800 val_loss=3.1866 | val_ppl=24.2063 | bleu=1.109 | chrf=18.103 | comet=nan | text_eval_sec=31.20\n",
            "[best] new best val_loss=3.1866 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step   8840/32000 | avg_loss=3.3139 | train_ppl=27.4913 | lr=0.000255785 | grad_norm=2.8252\n",
            "interval_sec=53.45 | steps/sec=0.7483 | sec/step=1.3363\n",
            "examples_seen=1,131,520 | approx_epochs=0.7595\n",
            "batch_shapes input=(16, 16) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.692 | chrf=19.900 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step   8880/32000 | avg_loss=3.2911 | train_ppl=26.8719 | lr=0.000255386 | grad_norm=2.8848\n",
            "interval_sec=17.46 | steps/sec=2.2904 | sec/step=0.4366\n",
            "examples_seen=1,136,640 | approx_epochs=0.7629\n",
            "batch_shapes input=(16, 24) labels=(16, 27)\n",
            "cuda_mem_alloc_gb=0.849 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.242 | chrf=19.860 | comet=nan | text_eval_sec=3.53\n",
            "========================================================================================\n",
            "step   8920/32000 | avg_loss=3.3332 | train_ppl=28.0269 | lr=0.000254985 | grad_norm=2.6187\n",
            "interval_sec=15.65 | steps/sec=2.5562 | sec/step=0.3912\n",
            "examples_seen=1,141,760 | approx_epochs=0.7664\n",
            "batch_shapes input=(16, 69) labels=(16, 71)\n",
            "cuda_mem_alloc_gb=0.935 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.370 | chrf=21.454 | comet=nan | text_eval_sec=3.59\n",
            "========================================================================================\n",
            "step   8960/32000 | avg_loss=3.3033 | train_ppl=27.2024 | lr=0.000254584 | grad_norm=2.5288\n",
            "interval_sec=15.65 | steps/sec=2.5563 | sec/step=0.3912\n",
            "examples_seen=1,146,880 | approx_epochs=0.7698\n",
            "batch_shapes input=(16, 21) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.740 | chrf=21.790 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step   9000/32000 | avg_loss=3.3075 | train_ppl=27.3171 | lr=0.00025418 | grad_norm=3.6438\n",
            "interval_sec=15.72 | steps/sec=2.5450 | sec/step=0.3929\n",
            "examples_seen=1,152,000 | approx_epochs=0.7732\n",
            "batch_shapes input=(16, 21) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.022 | chrf=19.841 | comet=nan | text_eval_sec=3.54\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_009000\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_004000\n",
            "========================================================================================\n",
            "step   9040/32000 | avg_loss=3.3068 | train_ppl=27.2972 | lr=0.000253775 | grad_norm=2.5733\n",
            "interval_sec=16.63 | steps/sec=2.4054 | sec/step=0.4157\n",
            "examples_seen=1,157,120 | approx_epochs=0.7767\n",
            "batch_shapes input=(16, 23) labels=(16, 26)\n",
            "cuda_mem_alloc_gb=0.847 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.549 | chrf=18.078 | comet=nan | text_eval_sec=4.94\n",
            "========================================================================================\n",
            "step   9080/32000 | avg_loss=3.3175 | train_ppl=27.5907 | lr=0.000253369 | grad_norm=2.3484\n",
            "interval_sec=15.75 | steps/sec=2.5394 | sec/step=0.3938\n",
            "examples_seen=1,162,240 | approx_epochs=0.7801\n",
            "batch_shapes input=(16, 63) labels=(16, 55)\n",
            "cuda_mem_alloc_gb=0.904 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.211 | chrf=20.026 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step   9120/32000 | avg_loss=3.3097 | train_ppl=27.3766 | lr=0.000252962 | grad_norm=2.7514\n",
            "interval_sec=15.72 | steps/sec=2.5440 | sec/step=0.3931\n",
            "examples_seen=1,167,360 | approx_epochs=0.7835\n",
            "batch_shapes input=(16, 17) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.050 | chrf=18.129 | comet=nan | text_eval_sec=3.56\n",
            "========================================================================================\n",
            "step   9160/32000 | avg_loss=3.2869 | train_ppl=26.7591 | lr=0.000252553 | grad_norm=2.7518\n",
            "interval_sec=15.57 | steps/sec=2.5687 | sec/step=0.3893\n",
            "examples_seen=1,172,480 | approx_epochs=0.7870\n",
            "batch_shapes input=(16, 15) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.125 | chrf=18.791 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step   9200/32000 | avg_loss=3.3022 | train_ppl=27.1715 | lr=0.000252142 | grad_norm=2.5894\n",
            "interval_sec=15.67 | steps/sec=2.5521 | sec/step=0.3918\n",
            "examples_seen=1,177,600 | approx_epochs=0.7904\n",
            "batch_shapes input=(16, 28) labels=(16, 26)\n",
            "cuda_mem_alloc_gb=0.847 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.302 | chrf=20.382 | comet=nan | text_eval_sec=3.78\n",
            "[eval] step=9200 val_loss=3.1531 | val_ppl=23.4091 | bleu=1.519 | chrf=19.720 | comet=nan | text_eval_sec=28.90\n",
            "[best] new best val_loss=3.1531 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step   9240/32000 | avg_loss=3.2856 | train_ppl=26.7246 | lr=0.000251731 | grad_norm=2.8100\n",
            "interval_sec=51.08 | steps/sec=0.7831 | sec/step=1.2770\n",
            "examples_seen=1,182,720 | approx_epochs=0.7939\n",
            "batch_shapes input=(16, 34) labels=(16, 26)\n",
            "cuda_mem_alloc_gb=0.848 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.907 | chrf=15.640 | comet=nan | text_eval_sec=3.69\n",
            "========================================================================================\n",
            "step   9280/32000 | avg_loss=3.2832 | train_ppl=26.6612 | lr=0.000251317 | grad_norm=2.8391\n",
            "interval_sec=15.67 | steps/sec=2.5532 | sec/step=0.3917\n",
            "examples_seen=1,187,840 | approx_epochs=0.7973\n",
            "batch_shapes input=(16, 21) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.408 | chrf=15.670 | comet=nan | text_eval_sec=3.67\n",
            "========================================================================================\n",
            "step   9320/32000 | avg_loss=3.2842 | train_ppl=26.6887 | lr=0.000250903 | grad_norm=2.7730\n",
            "interval_sec=16.04 | steps/sec=2.4940 | sec/step=0.4010\n",
            "examples_seen=1,192,960 | approx_epochs=0.8007\n",
            "batch_shapes input=(16, 18) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.951 | chrf=20.126 | comet=nan | text_eval_sec=3.78\n",
            "========================================================================================\n",
            "step   9360/32000 | avg_loss=3.3048 | train_ppl=27.2427 | lr=0.000250487 | grad_norm=2.4510\n",
            "interval_sec=15.58 | steps/sec=2.5666 | sec/step=0.3896\n",
            "examples_seen=1,198,080 | approx_epochs=0.8042\n",
            "batch_shapes input=(16, 69) labels=(16, 56)\n",
            "cuda_mem_alloc_gb=0.906 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.255 | chrf=21.147 | comet=nan | text_eval_sec=3.56\n",
            "========================================================================================\n",
            "step   9400/32000 | avg_loss=3.2327 | train_ppl=25.3468 | lr=0.00025007 | grad_norm=2.6554\n",
            "interval_sec=15.94 | steps/sec=2.5088 | sec/step=0.3986\n",
            "examples_seen=1,203,200 | approx_epochs=0.8076\n",
            "batch_shapes input=(16, 23) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.132 | chrf=16.183 | comet=nan | text_eval_sec=3.64\n",
            "========================================================================================\n",
            "step   9440/32000 | avg_loss=3.3205 | train_ppl=27.6739 | lr=0.000249651 | grad_norm=2.7195\n",
            "interval_sec=15.65 | steps/sec=2.5563 | sec/step=0.3912\n",
            "examples_seen=1,208,320 | approx_epochs=0.8110\n",
            "batch_shapes input=(16, 20) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.845 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.994 | chrf=15.573 | comet=nan | text_eval_sec=3.52\n",
            "========================================================================================\n",
            "step   9480/32000 | avg_loss=3.2964 | train_ppl=27.0163 | lr=0.000249231 | grad_norm=2.7697\n",
            "interval_sec=15.65 | steps/sec=2.5561 | sec/step=0.3912\n",
            "examples_seen=1,213,440 | approx_epochs=0.8145\n",
            "batch_shapes input=(16, 27) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.715 | chrf=20.188 | comet=nan | text_eval_sec=3.59\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_009500\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_004500\n",
            "========================================================================================\n",
            "step   9520/32000 | avg_loss=3.2469 | train_ppl=25.7101 | lr=0.00024881 | grad_norm=2.7265\n",
            "interval_sec=16.37 | steps/sec=2.4436 | sec/step=0.4092\n",
            "examples_seen=1,218,560 | approx_epochs=0.8179\n",
            "batch_shapes input=(16, 19) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.452 | chrf=21.828 | comet=nan | text_eval_sec=3.66\n",
            "========================================================================================\n",
            "step   9560/32000 | avg_loss=3.2463 | train_ppl=25.6958 | lr=0.000248387 | grad_norm=2.8379\n",
            "interval_sec=15.72 | steps/sec=2.5443 | sec/step=0.3930\n",
            "examples_seen=1,223,680 | approx_epochs=0.8213\n",
            "batch_shapes input=(16, 16) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.669 | chrf=22.341 | comet=nan | text_eval_sec=3.53\n",
            "========================================================================================\n",
            "step   9600/32000 | avg_loss=3.3080 | train_ppl=27.3306 | lr=0.000247963 | grad_norm=2.9163\n",
            "interval_sec=15.64 | steps/sec=2.5573 | sec/step=0.3910\n",
            "examples_seen=1,228,800 | approx_epochs=0.8248\n",
            "batch_shapes input=(16, 24) labels=(16, 26)\n",
            "cuda_mem_alloc_gb=0.847 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.290 | chrf=19.830 | comet=nan | text_eval_sec=3.66\n",
            "[eval] step=9600 val_loss=3.1279 | val_ppl=22.8260 | bleu=1.641 | chrf=20.740 | comet=nan | text_eval_sec=29.47\n",
            "[best] new best val_loss=3.1279 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step   9640/32000 | avg_loss=3.2807 | train_ppl=26.5932 | lr=0.000247538 | grad_norm=2.8994\n",
            "interval_sec=51.08 | steps/sec=0.7831 | sec/step=1.2770\n",
            "examples_seen=1,233,920 | approx_epochs=0.8282\n",
            "batch_shapes input=(16, 24) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.124 | chrf=20.162 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step   9680/32000 | avg_loss=3.2296 | train_ppl=25.2701 | lr=0.000247112 | grad_norm=2.7937\n",
            "interval_sec=15.77 | steps/sec=2.5360 | sec/step=0.3943\n",
            "examples_seen=1,239,040 | approx_epochs=0.8317\n",
            "batch_shapes input=(16, 22) labels=(16, 53)\n",
            "cuda_mem_alloc_gb=0.899 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.537 | chrf=20.299 | comet=nan | text_eval_sec=3.54\n",
            "========================================================================================\n",
            "step   9720/32000 | avg_loss=3.2250 | train_ppl=25.1534 | lr=0.000246684 | grad_norm=2.8548\n",
            "interval_sec=15.61 | steps/sec=2.5622 | sec/step=0.3903\n",
            "examples_seen=1,244,160 | approx_epochs=0.8351\n",
            "batch_shapes input=(16, 18) labels=(16, 13)\n",
            "cuda_mem_alloc_gb=0.822 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.275 | chrf=19.048 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step   9760/32000 | avg_loss=3.2520 | train_ppl=25.8418 | lr=0.000246255 | grad_norm=2.6042\n",
            "interval_sec=15.96 | steps/sec=2.5064 | sec/step=0.3990\n",
            "examples_seen=1,249,280 | approx_epochs=0.8385\n",
            "batch_shapes input=(16, 23) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.475 | chrf=20.401 | comet=nan | text_eval_sec=3.71\n",
            "========================================================================================\n",
            "step   9800/32000 | avg_loss=3.3181 | train_ppl=27.6088 | lr=0.000245824 | grad_norm=2.8386\n",
            "interval_sec=16.19 | steps/sec=2.4703 | sec/step=0.4048\n",
            "examples_seen=1,254,400 | approx_epochs=0.8420\n",
            "batch_shapes input=(16, 43) labels=(16, 43)\n",
            "cuda_mem_alloc_gb=0.880 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.698 | chrf=18.327 | comet=nan | text_eval_sec=3.70\n",
            "========================================================================================\n",
            "step   9840/32000 | avg_loss=3.2157 | train_ppl=24.9202 | lr=0.000245392 | grad_norm=2.6999\n",
            "interval_sec=15.74 | steps/sec=2.5414 | sec/step=0.3935\n",
            "examples_seen=1,259,520 | approx_epochs=0.8454\n",
            "batch_shapes input=(16, 18) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.495 | chrf=21.238 | comet=nan | text_eval_sec=3.50\n",
            "========================================================================================\n",
            "step   9880/32000 | avg_loss=3.2102 | train_ppl=24.7829 | lr=0.000244959 | grad_norm=2.7036\n",
            "interval_sec=15.66 | steps/sec=2.5545 | sec/step=0.3915\n",
            "examples_seen=1,264,640 | approx_epochs=0.8488\n",
            "batch_shapes input=(16, 25) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.844 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.833 | chrf=17.024 | comet=nan | text_eval_sec=3.59\n",
            "========================================================================================\n",
            "step   9920/32000 | avg_loss=3.2763 | train_ppl=26.4777 | lr=0.000244525 | grad_norm=2.8646\n",
            "interval_sec=15.58 | steps/sec=2.5670 | sec/step=0.3896\n",
            "examples_seen=1,269,760 | approx_epochs=0.8523\n",
            "batch_shapes input=(16, 27) labels=(16, 26)\n",
            "cuda_mem_alloc_gb=0.847 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.299 | chrf=22.520 | comet=nan | text_eval_sec=3.54\n",
            "========================================================================================\n",
            "step   9960/32000 | avg_loss=3.1980 | train_ppl=24.4839 | lr=0.00024409 | grad_norm=2.8942\n",
            "interval_sec=16.10 | steps/sec=2.4844 | sec/step=0.4025\n",
            "examples_seen=1,274,880 | approx_epochs=0.8557\n",
            "batch_shapes input=(16, 17) labels=(16, 14)\n",
            "cuda_mem_alloc_gb=0.824 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.512 | chrf=21.533 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step  10000/32000 | avg_loss=3.2645 | train_ppl=26.1673 | lr=0.000243653 | grad_norm=2.7356\n",
            "interval_sec=16.08 | steps/sec=2.4878 | sec/step=0.4020\n",
            "examples_seen=1,280,000 | approx_epochs=0.8591\n",
            "batch_shapes input=(16, 23) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.883 | chrf=20.061 | comet=nan | text_eval_sec=3.64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=10000 val_loss=3.1009 | val_ppl=22.2178 | bleu=1.191 | chrf=21.008 | comet=nan | text_eval_sec=28.27\n",
            "[best] new best val_loss=3.1009 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_010000\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_005000\n",
            "========================================================================================\n",
            "step  10040/32000 | avg_loss=3.2215 | train_ppl=25.0649 | lr=0.000243215 | grad_norm=2.7979\n",
            "interval_sec=50.73 | steps/sec=0.7886 | sec/step=1.2681\n",
            "examples_seen=1,285,120 | approx_epochs=0.8626\n",
            "batch_shapes input=(16, 19) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.962 | chrf=18.231 | comet=nan | text_eval_sec=4.36\n",
            "========================================================================================\n",
            "step  10080/32000 | avg_loss=3.2327 | train_ppl=25.3493 | lr=0.000242775 | grad_norm=2.7579\n",
            "interval_sec=15.59 | steps/sec=2.5661 | sec/step=0.3897\n",
            "examples_seen=1,290,240 | approx_epochs=0.8660\n",
            "batch_shapes input=(16, 23) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.736 | chrf=22.107 | comet=nan | text_eval_sec=3.53\n",
            "========================================================================================\n",
            "step  10120/32000 | avg_loss=3.2110 | train_ppl=24.8030 | lr=0.000242335 | grad_norm=2.6489\n",
            "interval_sec=15.73 | steps/sec=2.5426 | sec/step=0.3933\n",
            "examples_seen=1,295,360 | approx_epochs=0.8695\n",
            "batch_shapes input=(16, 101) labels=(16, 80)\n",
            "cuda_mem_alloc_gb=0.953 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.244 | chrf=21.075 | comet=nan | text_eval_sec=3.51\n",
            "========================================================================================\n",
            "step  10160/32000 | avg_loss=3.2015 | train_ppl=24.5686 | lr=0.000241893 | grad_norm=2.5683\n",
            "interval_sec=15.62 | steps/sec=2.5603 | sec/step=0.3906\n",
            "examples_seen=1,300,480 | approx_epochs=0.8729\n",
            "batch_shapes input=(16, 25) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.844 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.753 | chrf=22.281 | comet=nan | text_eval_sec=3.67\n",
            "========================================================================================\n",
            "step  10200/32000 | avg_loss=3.2055 | train_ppl=24.6669 | lr=0.00024145 | grad_norm=2.7950\n",
            "interval_sec=15.76 | steps/sec=2.5377 | sec/step=0.3941\n",
            "examples_seen=1,305,600 | approx_epochs=0.8763\n",
            "batch_shapes input=(16, 21) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.272 | chrf=20.163 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step  10240/32000 | avg_loss=3.2017 | train_ppl=24.5743 | lr=0.000241006 | grad_norm=2.5516\n",
            "interval_sec=15.81 | steps/sec=2.5301 | sec/step=0.3952\n",
            "examples_seen=1,310,720 | approx_epochs=0.8798\n",
            "batch_shapes input=(16, 15) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.175 | chrf=18.989 | comet=nan | text_eval_sec=3.55\n",
            "========================================================================================\n",
            "step  10280/32000 | avg_loss=3.2189 | train_ppl=25.0016 | lr=0.00024056 | grad_norm=2.6206\n",
            "interval_sec=15.71 | steps/sec=2.5459 | sec/step=0.3928\n",
            "examples_seen=1,315,840 | approx_epochs=0.8832\n",
            "batch_shapes input=(16, 24) labels=(16, 29)\n",
            "cuda_mem_alloc_gb=0.853 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.672 | chrf=21.329 | comet=nan | text_eval_sec=3.83\n",
            "========================================================================================\n",
            "step  10320/32000 | avg_loss=3.2281 | train_ppl=25.2315 | lr=0.000240114 | grad_norm=2.7942\n",
            "interval_sec=16.22 | steps/sec=2.4664 | sec/step=0.4054\n",
            "examples_seen=1,320,960 | approx_epochs=0.8866\n",
            "batch_shapes input=(16, 20) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.843 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.850 | chrf=17.339 | comet=nan | text_eval_sec=3.66\n",
            "========================================================================================\n",
            "step  10360/32000 | avg_loss=3.2036 | train_ppl=24.6215 | lr=0.000239666 | grad_norm=2.7451\n",
            "interval_sec=15.99 | steps/sec=2.5012 | sec/step=0.3998\n",
            "examples_seen=1,326,080 | approx_epochs=0.8901\n",
            "batch_shapes input=(16, 20) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.843 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.089 | chrf=19.060 | comet=nan | text_eval_sec=3.63\n",
            "========================================================================================\n",
            "step  10400/32000 | avg_loss=3.1788 | train_ppl=24.0175 | lr=0.000239217 | grad_norm=2.7246\n",
            "interval_sec=15.68 | steps/sec=2.5510 | sec/step=0.3920\n",
            "examples_seen=1,331,200 | approx_epochs=0.8935\n",
            "batch_shapes input=(16, 18) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.259 | chrf=17.197 | comet=nan | text_eval_sec=3.74\n",
            "[eval] step=10400 val_loss=3.0621 | val_ppl=21.3723 | bleu=1.252 | chrf=18.631 | comet=nan | text_eval_sec=28.52\n",
            "[best] new best val_loss=3.0621 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step  10440/32000 | avg_loss=3.1984 | train_ppl=24.4935 | lr=0.000238767 | grad_norm=3.0136\n",
            "interval_sec=50.67 | steps/sec=0.7894 | sec/step=1.2667\n",
            "examples_seen=1,336,320 | approx_epochs=0.8969\n",
            "batch_shapes input=(16, 17) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.893 | chrf=16.024 | comet=nan | text_eval_sec=4.41\n",
            "========================================================================================\n",
            "step  10480/32000 | avg_loss=3.1491 | train_ppl=23.3146 | lr=0.000238315 | grad_norm=3.0285\n",
            "interval_sec=15.81 | steps/sec=2.5306 | sec/step=0.3952\n",
            "examples_seen=1,341,440 | approx_epochs=0.9004\n",
            "batch_shapes input=(16, 19) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.341 | chrf=17.772 | comet=nan | text_eval_sec=3.70\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_010500\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_005500\n",
            "========================================================================================\n",
            "step  10520/32000 | avg_loss=3.2085 | train_ppl=24.7419 | lr=0.000237863 | grad_norm=2.8077\n",
            "interval_sec=17.87 | steps/sec=2.2389 | sec/step=0.4466\n",
            "examples_seen=1,346,560 | approx_epochs=0.9038\n",
            "batch_shapes input=(16, 34) labels=(16, 29)\n",
            "cuda_mem_alloc_gb=0.854 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.480 | chrf=20.480 | comet=nan | text_eval_sec=3.61\n",
            "========================================================================================\n",
            "step  10560/32000 | avg_loss=3.1437 | train_ppl=23.1904 | lr=0.000237409 | grad_norm=2.7770\n",
            "interval_sec=15.56 | steps/sec=2.5703 | sec/step=0.3891\n",
            "examples_seen=1,351,680 | approx_epochs=0.9073\n",
            "batch_shapes input=(16, 56) labels=(16, 61)\n",
            "cuda_mem_alloc_gb=0.915 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.241 | chrf=19.303 | comet=nan | text_eval_sec=3.56\n",
            "========================================================================================\n",
            "step  10600/32000 | avg_loss=3.1277 | train_ppl=22.8225 | lr=0.000236954 | grad_norm=2.8758\n",
            "interval_sec=15.70 | steps/sec=2.5477 | sec/step=0.3925\n",
            "examples_seen=1,356,800 | approx_epochs=0.9107\n",
            "batch_shapes input=(16, 19) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.208 | chrf=23.808 | comet=nan | text_eval_sec=3.55\n",
            "========================================================================================\n",
            "step  10640/32000 | avg_loss=3.1616 | train_ppl=23.6074 | lr=0.000236499 | grad_norm=2.7368\n",
            "interval_sec=15.76 | steps/sec=2.5373 | sec/step=0.3941\n",
            "examples_seen=1,361,920 | approx_epochs=0.9141\n",
            "batch_shapes input=(16, 22) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.343 | chrf=18.295 | comet=nan | text_eval_sec=3.63\n",
            "========================================================================================\n",
            "step  10680/32000 | avg_loss=3.1803 | train_ppl=24.0544 | lr=0.000236041 | grad_norm=2.7492\n",
            "interval_sec=15.69 | steps/sec=2.5486 | sec/step=0.3924\n",
            "examples_seen=1,367,040 | approx_epochs=0.9176\n",
            "batch_shapes input=(16, 16) labels=(16, 14)\n",
            "cuda_mem_alloc_gb=0.824 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.195 | chrf=19.266 | comet=nan | text_eval_sec=3.59\n",
            "========================================================================================\n",
            "step  10720/32000 | avg_loss=3.1703 | train_ppl=23.8149 | lr=0.000235583 | grad_norm=2.7852\n",
            "interval_sec=15.68 | steps/sec=2.5513 | sec/step=0.3920\n",
            "examples_seen=1,372,160 | approx_epochs=0.9210\n",
            "batch_shapes input=(16, 20) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.110 | chrf=17.057 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step  10760/32000 | avg_loss=3.1580 | train_ppl=23.5239 | lr=0.000235124 | grad_norm=2.8195\n",
            "interval_sec=15.74 | steps/sec=2.5416 | sec/step=0.3935\n",
            "examples_seen=1,377,280 | approx_epochs=0.9244\n",
            "batch_shapes input=(16, 47) labels=(16, 52)\n",
            "cuda_mem_alloc_gb=0.898 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.324 | chrf=20.466 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step  10800/32000 | avg_loss=3.1293 | train_ppl=22.8574 | lr=0.000234663 | grad_norm=2.6744\n",
            "interval_sec=15.73 | steps/sec=2.5427 | sec/step=0.3933\n",
            "examples_seen=1,382,400 | approx_epochs=0.9279\n",
            "batch_shapes input=(16, 28) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.846 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.273 | chrf=19.301 | comet=nan | text_eval_sec=3.66\n",
            "[eval] step=10800 val_loss=2.9683 | val_ppl=19.4583 | bleu=1.262 | chrf=18.764 | comet=nan | text_eval_sec=28.37\n",
            "[best] new best val_loss=2.9683 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step  10840/32000 | avg_loss=3.1422 | train_ppl=23.1546 | lr=0.000234202 | grad_norm=2.7385\n",
            "interval_sec=51.72 | steps/sec=0.7734 | sec/step=1.2930\n",
            "examples_seen=1,387,520 | approx_epochs=0.9313\n",
            "batch_shapes input=(16, 24) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.948 | chrf=16.588 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step  10880/32000 | avg_loss=3.1549 | train_ppl=23.4517 | lr=0.000233739 | grad_norm=2.8437\n",
            "interval_sec=15.50 | steps/sec=2.5800 | sec/step=0.3876\n",
            "examples_seen=1,392,640 | approx_epochs=0.9348\n",
            "batch_shapes input=(16, 21) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.173 | chrf=15.924 | comet=nan | text_eval_sec=3.66\n",
            "========================================================================================\n",
            "step  10920/32000 | avg_loss=3.1240 | train_ppl=22.7364 | lr=0.000233275 | grad_norm=2.7338\n",
            "interval_sec=15.56 | steps/sec=2.5715 | sec/step=0.3889\n",
            "examples_seen=1,397,760 | approx_epochs=0.9382\n",
            "batch_shapes input=(16, 27) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.844 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.014 | chrf=20.835 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step  10960/32000 | avg_loss=3.1361 | train_ppl=23.0128 | lr=0.000232811 | grad_norm=2.7288\n",
            "interval_sec=15.72 | steps/sec=2.5453 | sec/step=0.3929\n",
            "examples_seen=1,402,880 | approx_epochs=0.9416\n",
            "batch_shapes input=(16, 22) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.521 | chrf=19.032 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step  11000/32000 | avg_loss=3.1537 | train_ppl=23.4223 | lr=0.000232345 | grad_norm=2.8453\n",
            "interval_sec=15.76 | steps/sec=2.5377 | sec/step=0.3941\n",
            "examples_seen=1,408,000 | approx_epochs=0.9451\n",
            "batch_shapes input=(16, 32) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.432 | chrf=18.656 | comet=nan | text_eval_sec=3.56\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_011000\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_006000\n",
            "========================================================================================\n",
            "step  11040/32000 | avg_loss=3.1094 | train_ppl=22.4083 | lr=0.000231878 | grad_norm=2.8024\n",
            "interval_sec=16.40 | steps/sec=2.4396 | sec/step=0.4099\n",
            "examples_seen=1,413,120 | approx_epochs=0.9485\n",
            "batch_shapes input=(16, 18) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.366 | chrf=17.963 | comet=nan | text_eval_sec=3.61\n",
            "========================================================================================\n",
            "step  11080/32000 | avg_loss=3.1284 | train_ppl=22.8384 | lr=0.00023141 | grad_norm=2.9638\n",
            "interval_sec=15.52 | steps/sec=2.5778 | sec/step=0.3879\n",
            "examples_seen=1,418,240 | approx_epochs=0.9519\n",
            "batch_shapes input=(16, 19) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.177 | chrf=15.622 | comet=nan | text_eval_sec=3.54\n",
            "========================================================================================\n",
            "step  11120/32000 | avg_loss=3.1426 | train_ppl=23.1634 | lr=0.00023094 | grad_norm=2.7882\n",
            "interval_sec=15.71 | steps/sec=2.5468 | sec/step=0.3926\n",
            "examples_seen=1,423,360 | approx_epochs=0.9554\n",
            "batch_shapes input=(16, 23) labels=(16, 33)\n",
            "cuda_mem_alloc_gb=0.861 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.636 | chrf=18.248 | comet=nan | text_eval_sec=4.01\n",
            "========================================================================================\n",
            "step  11160/32000 | avg_loss=3.1715 | train_ppl=23.8429 | lr=0.00023047 | grad_norm=2.7713\n",
            "interval_sec=15.40 | steps/sec=2.5982 | sec/step=0.3849\n",
            "examples_seen=1,428,480 | approx_epochs=0.9588\n",
            "batch_shapes input=(16, 17) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.461 | chrf=19.890 | comet=nan | text_eval_sec=3.48\n",
            "========================================================================================\n",
            "step  11200/32000 | avg_loss=3.0910 | train_ppl=21.9996 | lr=0.000229999 | grad_norm=2.6973\n",
            "interval_sec=15.60 | steps/sec=2.5637 | sec/step=0.3901\n",
            "examples_seen=1,433,600 | approx_epochs=0.9622\n",
            "batch_shapes input=(16, 56) labels=(16, 67)\n",
            "cuda_mem_alloc_gb=0.927 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.235 | chrf=19.203 | comet=nan | text_eval_sec=3.55\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=11200 val_loss=2.9453 | val_ppl=19.0161 | bleu=1.356 | chrf=19.155 | comet=nan | text_eval_sec=28.41\n",
            "[best] new best val_loss=2.9453 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step  11240/32000 | avg_loss=3.1374 | train_ppl=23.0445 | lr=0.000229527 | grad_norm=2.8500\n",
            "interval_sec=51.05 | steps/sec=0.7836 | sec/step=1.2762\n",
            "examples_seen=1,438,720 | approx_epochs=0.9657\n",
            "batch_shapes input=(16, 20) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.404 | chrf=21.068 | comet=nan | text_eval_sec=6.67\n",
            "========================================================================================\n",
            "step  11280/32000 | avg_loss=3.1062 | train_ppl=22.3349 | lr=0.000229054 | grad_norm=2.6984\n",
            "interval_sec=15.81 | steps/sec=2.5300 | sec/step=0.3953\n",
            "examples_seen=1,443,840 | approx_epochs=0.9691\n",
            "batch_shapes input=(16, 25) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.525 | chrf=15.679 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step  11320/32000 | avg_loss=3.1787 | train_ppl=24.0158 | lr=0.000228579 | grad_norm=2.7774\n",
            "interval_sec=15.57 | steps/sec=2.5683 | sec/step=0.3894\n",
            "examples_seen=1,448,960 | approx_epochs=0.9726\n",
            "batch_shapes input=(16, 17) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.370 | chrf=19.389 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step  11360/32000 | avg_loss=3.1243 | train_ppl=22.7434 | lr=0.000228104 | grad_norm=2.6797\n",
            "interval_sec=15.79 | steps/sec=2.5335 | sec/step=0.3947\n",
            "examples_seen=1,454,080 | approx_epochs=0.9760\n",
            "batch_shapes input=(16, 15) labels=(16, 14)\n",
            "cuda_mem_alloc_gb=0.824 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.707 | chrf=18.855 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step  11400/32000 | avg_loss=3.1397 | train_ppl=23.0981 | lr=0.000227628 | grad_norm=3.0697\n",
            "interval_sec=15.47 | steps/sec=2.5850 | sec/step=0.3868\n",
            "examples_seen=1,459,200 | approx_epochs=0.9794\n",
            "batch_shapes input=(16, 22) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.880 | chrf=15.707 | comet=nan | text_eval_sec=3.56\n",
            "========================================================================================\n",
            "step  11440/32000 | avg_loss=3.0953 | train_ppl=22.0941 | lr=0.00022715 | grad_norm=2.9943\n",
            "interval_sec=15.58 | steps/sec=2.5675 | sec/step=0.3895\n",
            "examples_seen=1,464,320 | approx_epochs=0.9829\n",
            "batch_shapes input=(16, 20) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.723 | chrf=17.766 | comet=nan | text_eval_sec=3.83\n",
            "========================================================================================\n",
            "step  11480/32000 | avg_loss=3.0865 | train_ppl=21.9000 | lr=0.000226672 | grad_norm=2.5986\n",
            "interval_sec=16.23 | steps/sec=2.4639 | sec/step=0.4059\n",
            "examples_seen=1,469,440 | approx_epochs=0.9863\n",
            "batch_shapes input=(16, 21) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.460 | chrf=15.384 | comet=nan | text_eval_sec=3.68\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_011500\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_006500\n",
            "========================================================================================\n",
            "step  11520/32000 | avg_loss=3.1003 | train_ppl=22.2052 | lr=0.000226193 | grad_norm=2.7599\n",
            "interval_sec=16.45 | steps/sec=2.4309 | sec/step=0.4114\n",
            "examples_seen=1,474,560 | approx_epochs=0.9897\n",
            "batch_shapes input=(16, 17) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.392 | chrf=16.800 | comet=nan | text_eval_sec=5.18\n",
            "========================================================================================\n",
            "step  11560/32000 | avg_loss=3.1169 | train_ppl=22.5757 | lr=0.000225713 | grad_norm=2.7456\n",
            "interval_sec=15.81 | steps/sec=2.5296 | sec/step=0.3953\n",
            "examples_seen=1,479,680 | approx_epochs=0.9932\n",
            "batch_shapes input=(16, 18) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.775 | chrf=21.670 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step  11600/32000 | avg_loss=3.1270 | train_ppl=22.8057 | lr=0.000225231 | grad_norm=2.7328\n",
            "interval_sec=15.51 | steps/sec=2.5793 | sec/step=0.3877\n",
            "examples_seen=1,484,800 | approx_epochs=0.9966\n",
            "batch_shapes input=(16, 26) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.857 | chrf=22.794 | comet=nan | text_eval_sec=3.56\n",
            "[eval] step=11600 val_loss=2.9422 | val_ppl=18.9569 | bleu=1.594 | chrf=20.433 | comet=nan | text_eval_sec=29.06\n",
            "[best] new best val_loss=2.9422 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step  11640/32000 | avg_loss=3.1245 | train_ppl=22.7484 | lr=0.000224749 | grad_norm=2.8148\n",
            "interval_sec=51.04 | steps/sec=0.7838 | sec/step=1.2759\n",
            "examples_seen=1,489,920 | approx_epochs=1.0000\n",
            "batch_shapes input=(16, 17) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=2.067 | chrf=20.136 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step  11680/32000 | avg_loss=2.9181 | train_ppl=18.5067 | lr=0.000224266 | grad_norm=2.7769\n",
            "interval_sec=15.96 | steps/sec=2.5056 | sec/step=0.3991\n",
            "examples_seen=1,495,040 | approx_epochs=1.0035\n",
            "batch_shapes input=(16, 17) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.504 | chrf=16.705 | comet=nan | text_eval_sec=3.53\n",
            "========================================================================================\n",
            "step  11720/32000 | avg_loss=2.9619 | train_ppl=19.3343 | lr=0.000223782 | grad_norm=2.8553\n",
            "interval_sec=15.57 | steps/sec=2.5688 | sec/step=0.3893\n",
            "examples_seen=1,500,160 | approx_epochs=1.0069\n",
            "batch_shapes input=(16, 20) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.137 | chrf=16.686 | comet=nan | text_eval_sec=3.52\n",
            "========================================================================================\n",
            "step  11760/32000 | avg_loss=2.9337 | train_ppl=18.7974 | lr=0.000223297 | grad_norm=2.6436\n",
            "interval_sec=15.80 | steps/sec=2.5316 | sec/step=0.3950\n",
            "examples_seen=1,505,280 | approx_epochs=1.0104\n",
            "batch_shapes input=(16, 24) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.085 | chrf=16.891 | comet=nan | text_eval_sec=3.56\n",
            "========================================================================================\n",
            "step  11800/32000 | avg_loss=2.9091 | train_ppl=18.3409 | lr=0.000222811 | grad_norm=2.8960\n",
            "interval_sec=15.64 | steps/sec=2.5574 | sec/step=0.3910\n",
            "examples_seen=1,510,400 | approx_epochs=1.0138\n",
            "batch_shapes input=(16, 16) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.953 | chrf=17.559 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step  11840/32000 | avg_loss=2.9156 | train_ppl=18.4602 | lr=0.000222324 | grad_norm=2.8104\n",
            "interval_sec=15.50 | steps/sec=2.5805 | sec/step=0.3875\n",
            "examples_seen=1,515,520 | approx_epochs=1.0172\n",
            "batch_shapes input=(16, 24) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.371 | chrf=16.513 | comet=nan | text_eval_sec=3.53\n",
            "========================================================================================\n",
            "step  11880/32000 | avg_loss=2.8690 | train_ppl=17.6192 | lr=0.000221837 | grad_norm=2.6880\n",
            "interval_sec=15.64 | steps/sec=2.5579 | sec/step=0.3909\n",
            "examples_seen=1,520,640 | approx_epochs=1.0207\n",
            "batch_shapes input=(16, 8) labels=(16, 11)\n",
            "cuda_mem_alloc_gb=0.818 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.701 | chrf=17.785 | comet=nan | text_eval_sec=3.68\n",
            "========================================================================================\n",
            "step  11920/32000 | avg_loss=2.9745 | train_ppl=19.5803 | lr=0.000221348 | grad_norm=2.9136\n",
            "interval_sec=15.73 | steps/sec=2.5430 | sec/step=0.3932\n",
            "examples_seen=1,525,760 | approx_epochs=1.0241\n",
            "batch_shapes input=(16, 22) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.859 | chrf=23.918 | comet=nan | text_eval_sec=3.53\n",
            "========================================================================================\n",
            "step  11960/32000 | avg_loss=2.9868 | train_ppl=19.8228 | lr=0.000220858 | grad_norm=2.7483\n",
            "interval_sec=15.71 | steps/sec=2.5461 | sec/step=0.3928\n",
            "examples_seen=1,530,880 | approx_epochs=1.0275\n",
            "batch_shapes input=(16, 34) labels=(16, 31)\n",
            "cuda_mem_alloc_gb=0.857 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.142 | chrf=19.685 | comet=nan | text_eval_sec=3.61\n",
            "========================================================================================\n",
            "step  12000/32000 | avg_loss=2.9017 | train_ppl=18.2046 | lr=0.000220368 | grad_norm=2.8947\n",
            "interval_sec=15.67 | steps/sec=2.5533 | sec/step=0.3917\n",
            "examples_seen=1,536,000 | approx_epochs=1.0310\n",
            "batch_shapes input=(16, 19) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.774 | chrf=20.101 | comet=nan | text_eval_sec=3.56\n",
            "[eval] step=12000 val_loss=2.9089 | val_ppl=18.3363 | bleu=1.421 | chrf=19.114 | comet=nan | text_eval_sec=27.70\n",
            "[best] new best val_loss=2.9089 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_012000\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_007000\n",
            "========================================================================================\n",
            "step  12040/32000 | avg_loss=2.9395 | train_ppl=18.9064 | lr=0.000219877 | grad_norm=3.0168\n",
            "interval_sec=52.84 | steps/sec=0.7570 | sec/step=1.3210\n",
            "examples_seen=1,541,120 | approx_epochs=1.0344\n",
            "batch_shapes input=(16, 21) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.881 | chrf=22.158 | comet=nan | text_eval_sec=5.23\n",
            "========================================================================================\n",
            "step  12080/32000 | avg_loss=2.8762 | train_ppl=17.7467 | lr=0.000219384 | grad_norm=3.0490\n",
            "interval_sec=15.68 | steps/sec=2.5514 | sec/step=0.3919\n",
            "examples_seen=1,546,240 | approx_epochs=1.0378\n",
            "batch_shapes input=(16, 19) labels=(16, 12)\n",
            "cuda_mem_alloc_gb=0.820 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.875 | chrf=20.738 | comet=nan | text_eval_sec=4.69\n",
            "========================================================================================\n",
            "step  12120/32000 | avg_loss=2.9137 | train_ppl=18.4247 | lr=0.000218891 | grad_norm=2.7864\n",
            "interval_sec=15.68 | steps/sec=2.5504 | sec/step=0.3921\n",
            "examples_seen=1,551,360 | approx_epochs=1.0413\n",
            "batch_shapes input=(16, 16) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.358 | chrf=19.573 | comet=nan | text_eval_sec=3.65\n",
            "========================================================================================\n",
            "step  12160/32000 | avg_loss=2.9563 | train_ppl=19.2269 | lr=0.000218398 | grad_norm=2.6418\n",
            "interval_sec=15.53 | steps/sec=2.5751 | sec/step=0.3883\n",
            "examples_seen=1,556,480 | approx_epochs=1.0447\n",
            "batch_shapes input=(16, 17) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.833 | chrf=21.428 | comet=nan | text_eval_sec=3.56\n",
            "========================================================================================\n",
            "step  12200/32000 | avg_loss=2.9757 | train_ppl=19.6040 | lr=0.000217903 | grad_norm=2.9272\n",
            "interval_sec=15.74 | steps/sec=2.5416 | sec/step=0.3935\n",
            "examples_seen=1,561,600 | approx_epochs=1.0482\n",
            "batch_shapes input=(16, 16) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.965 | chrf=21.379 | comet=nan | text_eval_sec=3.70\n",
            "========================================================================================\n",
            "step  12240/32000 | avg_loss=2.9549 | train_ppl=19.2007 | lr=0.000217407 | grad_norm=2.7506\n",
            "interval_sec=15.61 | steps/sec=2.5633 | sec/step=0.3901\n",
            "examples_seen=1,566,720 | approx_epochs=1.0516\n",
            "batch_shapes input=(16, 16) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.858 | chrf=20.019 | comet=nan | text_eval_sec=3.64\n",
            "========================================================================================\n",
            "step  12280/32000 | avg_loss=2.9713 | train_ppl=19.5170 | lr=0.000216911 | grad_norm=2.6067\n",
            "interval_sec=16.01 | steps/sec=2.4982 | sec/step=0.4003\n",
            "examples_seen=1,571,840 | approx_epochs=1.0550\n",
            "batch_shapes input=(16, 23) labels=(16, 28)\n",
            "cuda_mem_alloc_gb=0.851 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.618 | chrf=23.232 | comet=nan | text_eval_sec=3.79\n",
            "========================================================================================\n",
            "step  12320/32000 | avg_loss=2.9417 | train_ppl=18.9489 | lr=0.000216414 | grad_norm=3.0365\n",
            "interval_sec=15.94 | steps/sec=2.5094 | sec/step=0.3985\n",
            "examples_seen=1,576,960 | approx_epochs=1.0585\n",
            "batch_shapes input=(16, 14) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.726 | chrf=20.507 | comet=nan | text_eval_sec=3.72\n",
            "========================================================================================\n",
            "step  12360/32000 | avg_loss=2.9620 | train_ppl=19.3364 | lr=0.000215915 | grad_norm=2.5882\n",
            "interval_sec=15.71 | steps/sec=2.5466 | sec/step=0.3927\n",
            "examples_seen=1,582,080 | approx_epochs=1.0619\n",
            "batch_shapes input=(16, 19) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.290 | chrf=19.261 | comet=nan | text_eval_sec=3.54\n",
            "========================================================================================\n",
            "step  12400/32000 | avg_loss=2.9862 | train_ppl=19.8108 | lr=0.000215417 | grad_norm=2.6433\n",
            "interval_sec=15.58 | steps/sec=2.5668 | sec/step=0.3896\n",
            "examples_seen=1,587,200 | approx_epochs=1.0653\n",
            "batch_shapes input=(16, 18) labels=(16, 13)\n",
            "cuda_mem_alloc_gb=0.822 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.997 | chrf=19.764 | comet=nan | text_eval_sec=3.57\n",
            "[eval] step=12400 val_loss=2.9397 | val_ppl=18.9094 | bleu=1.416 | chrf=17.834 | comet=nan | text_eval_sec=28.06\n",
            "========================================================================================\n",
            "step  12440/32000 | avg_loss=2.9271 | train_ppl=18.6730 | lr=0.000214917 | grad_norm=2.7410\n",
            "interval_sec=49.20 | steps/sec=0.8131 | sec/step=1.2299\n",
            "examples_seen=1,592,320 | approx_epochs=1.0688\n",
            "batch_shapes input=(16, 22) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.384 | chrf=23.356 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  12480/32000 | avg_loss=2.9236 | train_ppl=18.6073 | lr=0.000214416 | grad_norm=2.6348\n",
            "interval_sec=15.46 | steps/sec=2.5865 | sec/step=0.3866\n",
            "examples_seen=1,597,440 | approx_epochs=1.0722\n",
            "batch_shapes input=(16, 25) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.386 | chrf=18.906 | comet=nan | text_eval_sec=3.73\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_012500\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_007500\n",
            "========================================================================================\n",
            "step  12520/32000 | avg_loss=2.9298 | train_ppl=18.7237 | lr=0.000213915 | grad_norm=2.7237\n",
            "interval_sec=16.42 | steps/sec=2.4357 | sec/step=0.4106\n",
            "examples_seen=1,602,560 | approx_epochs=1.0757\n",
            "batch_shapes input=(16, 25) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.164 | chrf=19.745 | comet=nan | text_eval_sec=5.13\n",
            "========================================================================================\n",
            "step  12560/32000 | avg_loss=2.9272 | train_ppl=18.6750 | lr=0.000213413 | grad_norm=3.1169\n",
            "interval_sec=15.45 | steps/sec=2.5894 | sec/step=0.3862\n",
            "examples_seen=1,607,680 | approx_epochs=1.0791\n",
            "batch_shapes input=(16, 21) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.521 | chrf=22.699 | comet=nan | text_eval_sec=3.50\n",
            "========================================================================================\n",
            "step  12600/32000 | avg_loss=2.9390 | train_ppl=18.8961 | lr=0.00021291 | grad_norm=2.9732\n",
            "interval_sec=15.73 | steps/sec=2.5431 | sec/step=0.3932\n",
            "examples_seen=1,612,800 | approx_epochs=1.0825\n",
            "batch_shapes input=(16, 39) labels=(16, 28)\n",
            "cuda_mem_alloc_gb=0.852 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.002 | chrf=19.162 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  12640/32000 | avg_loss=2.9080 | train_ppl=18.3196 | lr=0.000212407 | grad_norm=2.7853\n",
            "interval_sec=15.63 | steps/sec=2.5596 | sec/step=0.3907\n",
            "examples_seen=1,617,920 | approx_epochs=1.0860\n",
            "batch_shapes input=(16, 20) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.878 | chrf=19.427 | comet=nan | text_eval_sec=3.55\n",
            "========================================================================================\n",
            "step  12680/32000 | avg_loss=2.8454 | train_ppl=17.2083 | lr=0.000211902 | grad_norm=2.9938\n",
            "interval_sec=15.61 | steps/sec=2.5621 | sec/step=0.3903\n",
            "examples_seen=1,623,040 | approx_epochs=1.0894\n",
            "batch_shapes input=(16, 25) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.245 | chrf=16.113 | comet=nan | text_eval_sec=3.50\n",
            "========================================================================================\n",
            "step  12720/32000 | avg_loss=2.9296 | train_ppl=18.7207 | lr=0.000211397 | grad_norm=2.7320\n",
            "interval_sec=15.71 | steps/sec=2.5460 | sec/step=0.3928\n",
            "examples_seen=1,628,160 | approx_epochs=1.0928\n",
            "batch_shapes input=(16, 22) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.388 | chrf=21.105 | comet=nan | text_eval_sec=3.59\n",
            "========================================================================================\n",
            "step  12760/32000 | avg_loss=2.9682 | train_ppl=19.4564 | lr=0.000210892 | grad_norm=2.7720\n",
            "interval_sec=15.67 | steps/sec=2.5522 | sec/step=0.3918\n",
            "examples_seen=1,633,280 | approx_epochs=1.0963\n",
            "batch_shapes input=(16, 25) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.306 | chrf=17.967 | comet=nan | text_eval_sec=3.54\n",
            "========================================================================================\n",
            "step  12800/32000 | avg_loss=2.9221 | train_ppl=18.5805 | lr=0.000210385 | grad_norm=2.7945\n",
            "interval_sec=15.73 | steps/sec=2.5436 | sec/step=0.3931\n",
            "examples_seen=1,638,400 | approx_epochs=1.0997\n",
            "batch_shapes input=(16, 26) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=2.200 | chrf=21.796 | comet=nan | text_eval_sec=3.56\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=12800 val_loss=2.8298 | val_ppl=16.9420 | bleu=1.493 | chrf=20.668 | comet=nan | text_eval_sec=28.22\n",
            "[best] new best val_loss=2.8298 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step  12840/32000 | avg_loss=2.9427 | train_ppl=18.9671 | lr=0.000209878 | grad_norm=3.0015\n",
            "interval_sec=50.15 | steps/sec=0.7977 | sec/step=1.2536\n",
            "examples_seen=1,643,520 | approx_epochs=1.1031\n",
            "batch_shapes input=(16, 17) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.505 | chrf=19.086 | comet=nan | text_eval_sec=3.63\n",
            "========================================================================================\n",
            "step  12880/32000 | avg_loss=2.9261 | train_ppl=18.6539 | lr=0.00020937 | grad_norm=2.7052\n",
            "interval_sec=15.47 | steps/sec=2.5852 | sec/step=0.3868\n",
            "examples_seen=1,648,640 | approx_epochs=1.1066\n",
            "batch_shapes input=(16, 57) labels=(16, 48)\n",
            "cuda_mem_alloc_gb=0.890 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.414 | chrf=20.288 | comet=nan | text_eval_sec=3.55\n",
            "========================================================================================\n",
            "step  12920/32000 | avg_loss=2.8522 | train_ppl=17.3264 | lr=0.000208861 | grad_norm=2.8316\n",
            "interval_sec=15.61 | steps/sec=2.5626 | sec/step=0.3902\n",
            "examples_seen=1,653,760 | approx_epochs=1.1100\n",
            "batch_shapes input=(16, 37) labels=(16, 28)\n",
            "cuda_mem_alloc_gb=0.852 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.152 | chrf=19.063 | comet=nan | text_eval_sec=3.53\n",
            "========================================================================================\n",
            "step  12960/32000 | avg_loss=2.9565 | train_ppl=19.2299 | lr=0.000208352 | grad_norm=2.7940\n",
            "interval_sec=15.63 | steps/sec=2.5599 | sec/step=0.3906\n",
            "examples_seen=1,658,880 | approx_epochs=1.1135\n",
            "batch_shapes input=(16, 16) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.369 | chrf=22.138 | comet=nan | text_eval_sec=3.55\n",
            "========================================================================================\n",
            "step  13000/32000 | avg_loss=2.8947 | train_ppl=18.0784 | lr=0.000207842 | grad_norm=2.8788\n",
            "interval_sec=15.57 | steps/sec=2.5687 | sec/step=0.3893\n",
            "examples_seen=1,664,000 | approx_epochs=1.1169\n",
            "batch_shapes input=(16, 60) labels=(16, 54)\n",
            "cuda_mem_alloc_gb=0.902 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.253 | chrf=17.773 | comet=nan | text_eval_sec=3.51\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_013000\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_008000\n",
            "========================================================================================\n",
            "step  13040/32000 | avg_loss=2.9051 | train_ppl=18.2675 | lr=0.000207331 | grad_norm=2.8781\n",
            "interval_sec=16.41 | steps/sec=2.4376 | sec/step=0.4102\n",
            "examples_seen=1,669,120 | approx_epochs=1.1203\n",
            "batch_shapes input=(16, 21) labels=(16, 26)\n",
            "cuda_mem_alloc_gb=0.847 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.668 | chrf=22.598 | comet=nan | text_eval_sec=3.63\n",
            "========================================================================================\n",
            "step  13080/32000 | avg_loss=2.8801 | train_ppl=17.8163 | lr=0.00020682 | grad_norm=3.1203\n",
            "interval_sec=15.61 | steps/sec=2.5633 | sec/step=0.3901\n",
            "examples_seen=1,674,240 | approx_epochs=1.1238\n",
            "batch_shapes input=(16, 28) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.846 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.241 | chrf=20.112 | comet=nan | text_eval_sec=3.52\n",
            "========================================================================================\n",
            "step  13120/32000 | avg_loss=2.8837 | train_ppl=17.8795 | lr=0.000206308 | grad_norm=2.9050\n",
            "interval_sec=15.57 | steps/sec=2.5694 | sec/step=0.3892\n",
            "examples_seen=1,679,360 | approx_epochs=1.1272\n",
            "batch_shapes input=(16, 30) labels=(16, 35)\n",
            "cuda_mem_alloc_gb=0.865 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.796 | chrf=16.417 | comet=nan | text_eval_sec=3.65\n",
            "========================================================================================\n",
            "step  13160/32000 | avg_loss=2.9214 | train_ppl=18.5670 | lr=0.000205795 | grad_norm=2.9034\n",
            "interval_sec=15.71 | steps/sec=2.5455 | sec/step=0.3929\n",
            "examples_seen=1,684,480 | approx_epochs=1.1306\n",
            "batch_shapes input=(16, 19) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.497 | chrf=18.443 | comet=nan | text_eval_sec=3.59\n",
            "========================================================================================\n",
            "step  13200/32000 | avg_loss=2.9602 | train_ppl=19.3016 | lr=0.000205281 | grad_norm=2.9166\n",
            "interval_sec=15.62 | steps/sec=2.5607 | sec/step=0.3905\n",
            "examples_seen=1,689,600 | approx_epochs=1.1341\n",
            "batch_shapes input=(16, 25) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.285 | chrf=18.557 | comet=nan | text_eval_sec=3.62\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=13200 val_loss=2.8093 | val_ppl=16.5984 | bleu=1.358 | chrf=20.363 | comet=nan | text_eval_sec=27.87\n",
            "[best] new best val_loss=2.8093 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step  13240/32000 | avg_loss=2.9142 | train_ppl=18.4332 | lr=0.000204767 | grad_norm=2.8485\n",
            "interval_sec=49.82 | steps/sec=0.8028 | sec/step=1.2456\n",
            "examples_seen=1,694,720 | approx_epochs=1.1375\n",
            "batch_shapes input=(16, 17) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.253 | chrf=20.326 | comet=nan | text_eval_sec=3.48\n",
            "========================================================================================\n",
            "step  13280/32000 | avg_loss=2.8971 | train_ppl=18.1207 | lr=0.000204253 | grad_norm=2.9654\n",
            "interval_sec=15.53 | steps/sec=2.5758 | sec/step=0.3882\n",
            "examples_seen=1,699,840 | approx_epochs=1.1409\n",
            "batch_shapes input=(16, 32) labels=(16, 31)\n",
            "cuda_mem_alloc_gb=0.857 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.703 | chrf=20.182 | comet=nan | text_eval_sec=3.49\n",
            "========================================================================================\n",
            "step  13320/32000 | avg_loss=2.8912 | train_ppl=18.0153 | lr=0.000203738 | grad_norm=2.7390\n",
            "interval_sec=15.81 | steps/sec=2.5298 | sec/step=0.3953\n",
            "examples_seen=1,704,960 | approx_epochs=1.1444\n",
            "batch_shapes input=(16, 36) labels=(16, 41)\n",
            "cuda_mem_alloc_gb=0.876 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.861 | chrf=16.692 | comet=nan | text_eval_sec=3.53\n",
            "========================================================================================\n",
            "step  13360/32000 | avg_loss=2.8640 | train_ppl=17.5320 | lr=0.000203222 | grad_norm=2.8706\n",
            "interval_sec=15.68 | steps/sec=2.5511 | sec/step=0.3920\n",
            "examples_seen=1,710,080 | approx_epochs=1.1478\n",
            "batch_shapes input=(16, 18) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.506 | chrf=19.884 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step  13400/32000 | avg_loss=2.9157 | train_ppl=18.4617 | lr=0.000202705 | grad_norm=3.0477\n",
            "interval_sec=15.64 | steps/sec=2.5574 | sec/step=0.3910\n",
            "examples_seen=1,715,200 | approx_epochs=1.1513\n",
            "batch_shapes input=(16, 24) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.383 | chrf=21.509 | comet=nan | text_eval_sec=3.71\n",
            "========================================================================================\n",
            "step  13440/32000 | avg_loss=2.9185 | train_ppl=18.5139 | lr=0.000202188 | grad_norm=2.8674\n",
            "interval_sec=16.14 | steps/sec=2.4783 | sec/step=0.4035\n",
            "examples_seen=1,720,320 | approx_epochs=1.1547\n",
            "batch_shapes input=(16, 90) labels=(16, 90)\n",
            "cuda_mem_alloc_gb=0.971 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.167 | chrf=19.643 | comet=nan | text_eval_sec=3.61\n",
            "========================================================================================\n",
            "step  13480/32000 | avg_loss=2.9128 | train_ppl=18.4075 | lr=0.00020167 | grad_norm=2.7509\n",
            "interval_sec=15.76 | steps/sec=2.5384 | sec/step=0.3939\n",
            "examples_seen=1,725,440 | approx_epochs=1.1581\n",
            "batch_shapes input=(16, 23) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.227 | chrf=20.827 | comet=nan | text_eval_sec=3.68\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_013500\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_008500\n",
            "========================================================================================\n",
            "step  13520/32000 | avg_loss=2.8683 | train_ppl=17.6076 | lr=0.000201152 | grad_norm=3.0459\n",
            "interval_sec=16.56 | steps/sec=2.4148 | sec/step=0.4141\n",
            "examples_seen=1,730,560 | approx_epochs=1.1616\n",
            "batch_shapes input=(16, 20) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.780 | chrf=22.234 | comet=nan | text_eval_sec=3.56\n",
            "========================================================================================\n",
            "step  13560/32000 | avg_loss=2.8629 | train_ppl=17.5116 | lr=0.000200633 | grad_norm=2.8546\n",
            "interval_sec=15.60 | steps/sec=2.5640 | sec/step=0.3900\n",
            "examples_seen=1,735,680 | approx_epochs=1.1650\n",
            "batch_shapes input=(16, 19) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.293 | chrf=20.123 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step  13600/32000 | avg_loss=2.9473 | train_ppl=19.0548 | lr=0.000200114 | grad_norm=2.8680\n",
            "interval_sec=15.80 | steps/sec=2.5310 | sec/step=0.3951\n",
            "examples_seen=1,740,800 | approx_epochs=1.1684\n",
            "batch_shapes input=(16, 27) labels=(16, 29)\n",
            "cuda_mem_alloc_gb=0.853 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.868 | chrf=22.711 | comet=nan | text_eval_sec=3.65\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=13600 val_loss=2.7868 | val_ppl=16.2294 | bleu=1.454 | chrf=20.065 | comet=nan | text_eval_sec=27.91\n",
            "[best] new best val_loss=2.7868 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step  13640/32000 | avg_loss=2.8530 | train_ppl=17.3394 | lr=0.000199594 | grad_norm=2.6866\n",
            "interval_sec=50.36 | steps/sec=0.7943 | sec/step=1.2589\n",
            "examples_seen=1,745,920 | approx_epochs=1.1719\n",
            "batch_shapes input=(16, 21) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.845 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.566 | chrf=20.031 | comet=nan | text_eval_sec=3.52\n",
            "========================================================================================\n",
            "step  13680/32000 | avg_loss=2.8772 | train_ppl=17.7650 | lr=0.000199073 | grad_norm=3.0965\n",
            "interval_sec=15.73 | steps/sec=2.5424 | sec/step=0.3933\n",
            "examples_seen=1,751,040 | approx_epochs=1.1753\n",
            "batch_shapes input=(16, 29) labels=(16, 26)\n",
            "cuda_mem_alloc_gb=0.848 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.408 | chrf=22.160 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  13720/32000 | avg_loss=2.8560 | train_ppl=17.3921 | lr=0.000198552 | grad_norm=2.8595\n",
            "interval_sec=15.62 | steps/sec=2.5610 | sec/step=0.3905\n",
            "examples_seen=1,756,160 | approx_epochs=1.1787\n",
            "batch_shapes input=(16, 51) labels=(16, 63)\n",
            "cuda_mem_alloc_gb=0.919 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.715 | chrf=23.147 | comet=nan | text_eval_sec=3.51\n",
            "========================================================================================\n",
            "step  13760/32000 | avg_loss=2.9196 | train_ppl=18.5336 | lr=0.000198031 | grad_norm=2.8242\n",
            "interval_sec=15.60 | steps/sec=2.5645 | sec/step=0.3899\n",
            "examples_seen=1,761,280 | approx_epochs=1.1822\n",
            "batch_shapes input=(16, 21) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.125 | chrf=18.511 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  13800/32000 | avg_loss=2.8194 | train_ppl=16.7660 | lr=0.000197509 | grad_norm=2.7824\n",
            "interval_sec=15.57 | steps/sec=2.5698 | sec/step=0.3891\n",
            "examples_seen=1,766,400 | approx_epochs=1.1856\n",
            "batch_shapes input=(16, 20) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.386 | chrf=21.102 | comet=nan | text_eval_sec=3.63\n",
            "========================================================================================\n",
            "step  13840/32000 | avg_loss=2.8890 | train_ppl=17.9758 | lr=0.000196986 | grad_norm=3.1498\n",
            "interval_sec=15.68 | steps/sec=2.5514 | sec/step=0.3919\n",
            "examples_seen=1,771,520 | approx_epochs=1.1891\n",
            "batch_shapes input=(16, 22) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.843 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.235 | chrf=21.046 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step  13880/32000 | avg_loss=2.8419 | train_ppl=17.1483 | lr=0.000196463 | grad_norm=2.9016\n",
            "interval_sec=15.53 | steps/sec=2.5757 | sec/step=0.3883\n",
            "examples_seen=1,776,640 | approx_epochs=1.1925\n",
            "batch_shapes input=(16, 23) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.733 | chrf=18.973 | comet=nan | text_eval_sec=3.43\n",
            "========================================================================================\n",
            "step  13920/32000 | avg_loss=2.8484 | train_ppl=17.2603 | lr=0.000195939 | grad_norm=3.0747\n",
            "interval_sec=15.70 | steps/sec=2.5472 | sec/step=0.3926\n",
            "examples_seen=1,781,760 | approx_epochs=1.1959\n",
            "batch_shapes input=(16, 22) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.843 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.666 | chrf=22.373 | comet=nan | text_eval_sec=3.54\n",
            "========================================================================================\n",
            "step  13960/32000 | avg_loss=2.8509 | train_ppl=17.3037 | lr=0.000195415 | grad_norm=2.9014\n",
            "interval_sec=15.88 | steps/sec=2.5185 | sec/step=0.3971\n",
            "examples_seen=1,786,880 | approx_epochs=1.1994\n",
            "batch_shapes input=(16, 14) labels=(16, 13)\n",
            "cuda_mem_alloc_gb=0.822 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.309 | chrf=21.349 | comet=nan | text_eval_sec=3.50\n",
            "========================================================================================\n",
            "step  14000/32000 | avg_loss=2.9121 | train_ppl=18.3961 | lr=0.00019489 | grad_norm=2.9302\n",
            "interval_sec=15.77 | steps/sec=2.5360 | sec/step=0.3943\n",
            "examples_seen=1,792,000 | approx_epochs=1.2028\n",
            "batch_shapes input=(16, 23) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.183 | chrf=20.450 | comet=nan | text_eval_sec=3.59\n",
            "[eval] step=14000 val_loss=2.8114 | val_ppl=16.6327 | bleu=1.381 | chrf=22.226 | comet=nan | text_eval_sec=28.17\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_014000\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_009000\n",
            "========================================================================================\n",
            "step  14040/32000 | avg_loss=2.8475 | train_ppl=17.2452 | lr=0.000194365 | grad_norm=3.1228\n",
            "interval_sec=50.12 | steps/sec=0.7982 | sec/step=1.2529\n",
            "examples_seen=1,797,120 | approx_epochs=1.2062\n",
            "batch_shapes input=(16, 26) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.069 | chrf=19.083 | comet=nan | text_eval_sec=5.18\n",
            "========================================================================================\n",
            "step  14080/32000 | avg_loss=2.8405 | train_ppl=17.1250 | lr=0.00019384 | grad_norm=2.9101\n",
            "interval_sec=15.59 | steps/sec=2.5651 | sec/step=0.3899\n",
            "examples_seen=1,802,240 | approx_epochs=1.2097\n",
            "batch_shapes input=(16, 21) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.843 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.628 | chrf=21.236 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step  14120/32000 | avg_loss=2.8520 | train_ppl=17.3221 | lr=0.000193314 | grad_norm=3.0820\n",
            "interval_sec=15.90 | steps/sec=2.5158 | sec/step=0.3975\n",
            "examples_seen=1,807,360 | approx_epochs=1.2131\n",
            "batch_shapes input=(16, 27) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.613 | chrf=25.152 | comet=nan | text_eval_sec=3.75\n",
            "========================================================================================\n",
            "step  14160/32000 | avg_loss=2.8480 | train_ppl=17.2526 | lr=0.000192787 | grad_norm=2.8677\n",
            "interval_sec=15.93 | steps/sec=2.5112 | sec/step=0.3982\n",
            "examples_seen=1,812,480 | approx_epochs=1.2166\n",
            "batch_shapes input=(16, 20) labels=(16, 14)\n",
            "cuda_mem_alloc_gb=0.824 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.445 | chrf=21.482 | comet=nan | text_eval_sec=3.67\n",
            "========================================================================================\n",
            "step  14200/32000 | avg_loss=2.8452 | train_ppl=17.2048 | lr=0.00019226 | grad_norm=2.8149\n",
            "interval_sec=15.97 | steps/sec=2.5049 | sec/step=0.3992\n",
            "examples_seen=1,817,600 | approx_epochs=1.2200\n",
            "batch_shapes input=(16, 55) labels=(16, 44)\n",
            "cuda_mem_alloc_gb=0.883 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.523 | chrf=20.663 | comet=nan | text_eval_sec=3.64\n",
            "========================================================================================\n",
            "step  14240/32000 | avg_loss=2.8870 | train_ppl=17.9402 | lr=0.000191733 | grad_norm=2.7512\n",
            "interval_sec=15.83 | steps/sec=2.5265 | sec/step=0.3958\n",
            "examples_seen=1,822,720 | approx_epochs=1.2234\n",
            "batch_shapes input=(16, 20) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.807 | chrf=25.050 | comet=nan | text_eval_sec=3.59\n",
            "========================================================================================\n",
            "step  14280/32000 | avg_loss=2.8596 | train_ppl=17.4539 | lr=0.000191205 | grad_norm=2.8446\n",
            "interval_sec=15.67 | steps/sec=2.5520 | sec/step=0.3919\n",
            "examples_seen=1,827,840 | approx_epochs=1.2269\n",
            "batch_shapes input=(16, 29) labels=(16, 29)\n",
            "cuda_mem_alloc_gb=0.853 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.675 | chrf=23.650 | comet=nan | text_eval_sec=3.48\n",
            "========================================================================================\n",
            "step  14320/32000 | avg_loss=2.8560 | train_ppl=17.3916 | lr=0.000190677 | grad_norm=3.1586\n",
            "interval_sec=15.62 | steps/sec=2.5613 | sec/step=0.3904\n",
            "examples_seen=1,832,960 | approx_epochs=1.2303\n",
            "batch_shapes input=(16, 23) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.308 | chrf=20.983 | comet=nan | text_eval_sec=3.63\n",
            "========================================================================================\n",
            "step  14360/32000 | avg_loss=2.8325 | train_ppl=16.9872 | lr=0.000190148 | grad_norm=2.9636\n",
            "interval_sec=15.74 | steps/sec=2.5409 | sec/step=0.3936\n",
            "examples_seen=1,838,080 | approx_epochs=1.2337\n",
            "batch_shapes input=(16, 19) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.390 | chrf=19.138 | comet=nan | text_eval_sec=3.56\n",
            "========================================================================================\n",
            "step  14400/32000 | avg_loss=2.7955 | train_ppl=16.3706 | lr=0.000189619 | grad_norm=2.7786\n",
            "interval_sec=15.64 | steps/sec=2.5572 | sec/step=0.3911\n",
            "examples_seen=1,843,200 | approx_epochs=1.2372\n",
            "batch_shapes input=(16, 23) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.813 | chrf=22.896 | comet=nan | text_eval_sec=3.62\n",
            "[eval] step=14400 val_loss=2.7669 | val_ppl=15.9096 | bleu=1.615 | chrf=21.865 | comet=nan | text_eval_sec=28.06\n",
            "[best] new best val_loss=2.7669 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step  14440/32000 | avg_loss=2.8842 | train_ppl=17.8894 | lr=0.00018909 | grad_norm=2.9247\n",
            "interval_sec=50.02 | steps/sec=0.7997 | sec/step=1.2505\n",
            "examples_seen=1,848,320 | approx_epochs=1.2406\n",
            "batch_shapes input=(16, 49) labels=(16, 41)\n",
            "cuda_mem_alloc_gb=0.877 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.455 | chrf=20.252 | comet=nan | text_eval_sec=3.50\n",
            "========================================================================================\n",
            "step  14480/32000 | avg_loss=2.8308 | train_ppl=16.9591 | lr=0.00018856 | grad_norm=2.9498\n",
            "interval_sec=15.66 | steps/sec=2.5535 | sec/step=0.3916\n",
            "examples_seen=1,853,440 | approx_epochs=1.2440\n",
            "batch_shapes input=(16, 22) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.782 | chrf=19.742 | comet=nan | text_eval_sec=3.54\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_014500\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_009500\n",
            "========================================================================================\n",
            "step  14520/32000 | avg_loss=2.8873 | train_ppl=17.9443 | lr=0.00018803 | grad_norm=2.9981\n",
            "interval_sec=16.40 | steps/sec=2.4395 | sec/step=0.4099\n",
            "examples_seen=1,858,560 | approx_epochs=1.2475\n",
            "batch_shapes input=(16, 24) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.845 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.395 | chrf=19.739 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step  14560/32000 | avg_loss=2.8289 | train_ppl=16.9275 | lr=0.0001875 | grad_norm=2.9939\n",
            "interval_sec=15.63 | steps/sec=2.5586 | sec/step=0.3908\n",
            "examples_seen=1,863,680 | approx_epochs=1.2509\n",
            "batch_shapes input=(16, 54) labels=(16, 47)\n",
            "cuda_mem_alloc_gb=0.888 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.810 | chrf=21.515 | comet=nan | text_eval_sec=3.65\n",
            "========================================================================================\n",
            "step  14600/32000 | avg_loss=2.8792 | train_ppl=17.8002 | lr=0.000186969 | grad_norm=2.9071\n",
            "interval_sec=16.13 | steps/sec=2.4804 | sec/step=0.4032\n",
            "examples_seen=1,868,800 | approx_epochs=1.2544\n",
            "batch_shapes input=(16, 20) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.845 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.956 | chrf=24.779 | comet=nan | text_eval_sec=3.89\n",
            "========================================================================================\n",
            "step  14640/32000 | avg_loss=2.8644 | train_ppl=17.5377 | lr=0.000186438 | grad_norm=2.7670\n",
            "interval_sec=16.21 | steps/sec=2.4680 | sec/step=0.4052\n",
            "examples_seen=1,873,920 | approx_epochs=1.2578\n",
            "batch_shapes input=(16, 17) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.238 | chrf=19.762 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step  14680/32000 | avg_loss=2.8590 | train_ppl=17.4434 | lr=0.000185906 | grad_norm=3.1323\n",
            "interval_sec=15.91 | steps/sec=2.5137 | sec/step=0.3978\n",
            "examples_seen=1,879,040 | approx_epochs=1.2612\n",
            "batch_shapes input=(16, 23) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.420 | chrf=20.101 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step  14720/32000 | avg_loss=2.8484 | train_ppl=17.2593 | lr=0.000185374 | grad_norm=2.9145\n",
            "interval_sec=15.69 | steps/sec=2.5500 | sec/step=0.3922\n",
            "examples_seen=1,884,160 | approx_epochs=1.2647\n",
            "batch_shapes input=(16, 19) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.263 | chrf=24.398 | comet=nan | text_eval_sec=3.56\n",
            "========================================================================================\n",
            "step  14760/32000 | avg_loss=2.8009 | train_ppl=16.4593 | lr=0.000184842 | grad_norm=3.0532\n",
            "interval_sec=15.78 | steps/sec=2.5356 | sec/step=0.3944\n",
            "examples_seen=1,889,280 | approx_epochs=1.2681\n",
            "batch_shapes input=(16, 24) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=2.046 | chrf=23.798 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step  14800/32000 | avg_loss=2.8731 | train_ppl=17.6909 | lr=0.000184309 | grad_norm=2.7884\n",
            "interval_sec=15.70 | steps/sec=2.5472 | sec/step=0.3926\n",
            "examples_seen=1,894,400 | approx_epochs=1.2715\n",
            "batch_shapes input=(16, 15) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.485 | chrf=20.571 | comet=nan | text_eval_sec=3.50\n",
            "[eval] step=14800 val_loss=2.7556 | val_ppl=15.7311 | bleu=1.404 | chrf=19.698 | comet=nan | text_eval_sec=27.96\n",
            "[best] new best val_loss=2.7556 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step  14840/32000 | avg_loss=2.8391 | train_ppl=17.1005 | lr=0.000183777 | grad_norm=2.9028\n",
            "interval_sec=49.86 | steps/sec=0.8023 | sec/step=1.2464\n",
            "examples_seen=1,899,520 | approx_epochs=1.2750\n",
            "batch_shapes input=(16, 22) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.730 | chrf=20.197 | comet=nan | text_eval_sec=3.55\n",
            "========================================================================================\n",
            "step  14880/32000 | avg_loss=2.8228 | train_ppl=16.8233 | lr=0.000183243 | grad_norm=2.8364\n",
            "interval_sec=15.68 | steps/sec=2.5507 | sec/step=0.3921\n",
            "examples_seen=1,904,640 | approx_epochs=1.2784\n",
            "batch_shapes input=(16, 18) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.253 | chrf=22.107 | comet=nan | text_eval_sec=3.56\n",
            "========================================================================================\n",
            "step  14920/32000 | avg_loss=2.8297 | train_ppl=16.9403 | lr=0.00018271 | grad_norm=2.5865\n",
            "interval_sec=15.49 | steps/sec=2.5820 | sec/step=0.3873\n",
            "examples_seen=1,909,760 | approx_epochs=1.2818\n",
            "batch_shapes input=(16, 18) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.751 | chrf=20.959 | comet=nan | text_eval_sec=3.63\n",
            "========================================================================================\n",
            "step  14960/32000 | avg_loss=2.7829 | train_ppl=16.1665 | lr=0.000182176 | grad_norm=2.9520\n",
            "interval_sec=15.62 | steps/sec=2.5602 | sec/step=0.3906\n",
            "examples_seen=1,914,880 | approx_epochs=1.2853\n",
            "batch_shapes input=(16, 26) labels=(16, 26)\n",
            "cuda_mem_alloc_gb=0.847 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.303 | chrf=19.775 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  15000/32000 | avg_loss=2.8415 | train_ppl=17.1412 | lr=0.000181642 | grad_norm=3.1780\n",
            "interval_sec=15.75 | steps/sec=2.5403 | sec/step=0.3937\n",
            "examples_seen=1,920,000 | approx_epochs=1.2887\n",
            "batch_shapes input=(16, 20) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.452 | chrf=19.293 | comet=nan | text_eval_sec=3.56\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_015000\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_010000\n",
            "========================================================================================\n",
            "step  15040/32000 | avg_loss=2.7780 | train_ppl=16.0864 | lr=0.000181108 | grad_norm=2.9723\n",
            "interval_sec=16.37 | steps/sec=2.4436 | sec/step=0.4092\n",
            "examples_seen=1,925,120 | approx_epochs=1.2922\n",
            "batch_shapes input=(16, 15) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.524 | chrf=20.341 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  15080/32000 | avg_loss=2.8244 | train_ppl=16.8502 | lr=0.000180574 | grad_norm=2.8990\n",
            "interval_sec=15.75 | steps/sec=2.5389 | sec/step=0.3939\n",
            "examples_seen=1,930,240 | approx_epochs=1.2956\n",
            "batch_shapes input=(16, 30) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.592 | chrf=18.267 | comet=nan | text_eval_sec=3.59\n",
            "========================================================================================\n",
            "step  15120/32000 | avg_loss=2.7683 | train_ppl=15.9314 | lr=0.000180039 | grad_norm=2.9408\n",
            "interval_sec=15.61 | steps/sec=2.5627 | sec/step=0.3902\n",
            "examples_seen=1,935,360 | approx_epochs=1.2990\n",
            "batch_shapes input=(16, 24) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.308 | chrf=17.767 | comet=nan | text_eval_sec=3.53\n",
            "========================================================================================\n",
            "step  15160/32000 | avg_loss=2.8209 | train_ppl=16.7926 | lr=0.000179504 | grad_norm=2.9020\n",
            "interval_sec=16.17 | steps/sec=2.4730 | sec/step=0.4044\n",
            "examples_seen=1,940,480 | approx_epochs=1.3025\n",
            "batch_shapes input=(16, 18) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.391 | chrf=21.843 | comet=nan | text_eval_sec=3.63\n",
            "========================================================================================\n",
            "step  15200/32000 | avg_loss=2.7603 | train_ppl=15.8048 | lr=0.000178969 | grad_norm=2.7427\n",
            "interval_sec=16.59 | steps/sec=2.4118 | sec/step=0.4146\n",
            "examples_seen=1,945,600 | approx_epochs=1.3059\n",
            "batch_shapes input=(16, 28) labels=(16, 29)\n",
            "cuda_mem_alloc_gb=0.853 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.164 | chrf=18.375 | comet=nan | text_eval_sec=3.60\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=15200 val_loss=2.7643 | val_ppl=15.8673 | bleu=1.374 | chrf=20.816 | comet=nan | text_eval_sec=27.96\n",
            "========================================================================================\n",
            "step  15240/32000 | avg_loss=2.7391 | train_ppl=15.4728 | lr=0.000178433 | grad_norm=2.8433\n",
            "interval_sec=49.12 | steps/sec=0.8144 | sec/step=1.2280\n",
            "examples_seen=1,950,720 | approx_epochs=1.3093\n",
            "batch_shapes input=(16, 24) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.520 | chrf=20.798 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step  15280/32000 | avg_loss=2.7902 | train_ppl=16.2845 | lr=0.000177898 | grad_norm=3.2090\n",
            "interval_sec=15.91 | steps/sec=2.5136 | sec/step=0.3978\n",
            "examples_seen=1,955,840 | approx_epochs=1.3128\n",
            "batch_shapes input=(16, 47) labels=(16, 62)\n",
            "cuda_mem_alloc_gb=0.917 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.573 | chrf=20.361 | comet=nan | text_eval_sec=3.63\n",
            "========================================================================================\n",
            "step  15320/32000 | avg_loss=2.8321 | train_ppl=16.9805 | lr=0.000177362 | grad_norm=2.8500\n",
            "interval_sec=15.77 | steps/sec=2.5370 | sec/step=0.3942\n",
            "examples_seen=1,960,960 | approx_epochs=1.3162\n",
            "batch_shapes input=(16, 20) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.845 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.433 | chrf=18.993 | comet=nan | text_eval_sec=3.56\n",
            "========================================================================================\n",
            "step  15360/32000 | avg_loss=2.8258 | train_ppl=16.8743 | lr=0.000176826 | grad_norm=2.9872\n",
            "interval_sec=15.78 | steps/sec=2.5352 | sec/step=0.3944\n",
            "examples_seen=1,966,080 | approx_epochs=1.3196\n",
            "batch_shapes input=(16, 22) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.362 | chrf=19.064 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step  15400/32000 | avg_loss=2.8026 | train_ppl=16.4870 | lr=0.000176289 | grad_norm=3.0084\n",
            "interval_sec=15.70 | steps/sec=2.5480 | sec/step=0.3925\n",
            "examples_seen=1,971,200 | approx_epochs=1.3231\n",
            "batch_shapes input=(16, 23) labels=(16, 30)\n",
            "cuda_mem_alloc_gb=0.855 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.866 | chrf=20.844 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step  15440/32000 | avg_loss=2.7682 | train_ppl=15.9302 | lr=0.000175753 | grad_norm=2.8182\n",
            "interval_sec=15.65 | steps/sec=2.5562 | sec/step=0.3912\n",
            "examples_seen=1,976,320 | approx_epochs=1.3265\n",
            "batch_shapes input=(16, 24) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.844 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.535 | chrf=20.970 | comet=nan | text_eval_sec=3.59\n",
            "========================================================================================\n",
            "step  15480/32000 | avg_loss=2.7843 | train_ppl=16.1883 | lr=0.000175216 | grad_norm=2.9022\n",
            "interval_sec=15.58 | steps/sec=2.5680 | sec/step=0.3894\n",
            "examples_seen=1,981,440 | approx_epochs=1.3300\n",
            "batch_shapes input=(16, 23) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.550 | chrf=18.933 | comet=nan | text_eval_sec=3.55\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_015500\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_010500\n",
            "========================================================================================\n",
            "step  15520/32000 | avg_loss=2.7853 | train_ppl=16.2041 | lr=0.00017468 | grad_norm=2.9027\n",
            "interval_sec=16.59 | steps/sec=2.4115 | sec/step=0.4147\n",
            "examples_seen=1,986,560 | approx_epochs=1.3334\n",
            "batch_shapes input=(16, 22) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.057 | chrf=18.741 | comet=nan | text_eval_sec=5.50\n",
            "========================================================================================\n",
            "step  15560/32000 | avg_loss=2.8016 | train_ppl=16.4710 | lr=0.000174143 | grad_norm=3.0478\n",
            "interval_sec=15.56 | steps/sec=2.5700 | sec/step=0.3891\n",
            "examples_seen=1,991,680 | approx_epochs=1.3368\n",
            "batch_shapes input=(16, 20) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.838 | chrf=23.824 | comet=nan | text_eval_sec=3.55\n",
            "========================================================================================\n",
            "step  15600/32000 | avg_loss=2.8210 | train_ppl=16.7942 | lr=0.000173606 | grad_norm=2.8597\n",
            "interval_sec=15.69 | steps/sec=2.5492 | sec/step=0.3923\n",
            "examples_seen=1,996,800 | approx_epochs=1.3403\n",
            "batch_shapes input=(16, 19) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.823 | chrf=19.050 | comet=nan | text_eval_sec=3.52\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=15600 val_loss=2.7484 | val_ppl=15.6182 | bleu=1.523 | chrf=20.869 | comet=nan | text_eval_sec=28.93\n",
            "[best] new best val_loss=2.7484 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step  15640/32000 | avg_loss=2.7884 | train_ppl=16.2547 | lr=0.000173068 | grad_norm=2.8745\n",
            "interval_sec=51.18 | steps/sec=0.7815 | sec/step=1.2796\n",
            "examples_seen=2,001,920 | approx_epochs=1.3437\n",
            "batch_shapes input=(16, 24) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.844 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.558 | chrf=22.900 | comet=nan | text_eval_sec=4.19\n",
            "========================================================================================\n",
            "step  15680/32000 | avg_loss=2.8087 | train_ppl=16.5879 | lr=0.000172531 | grad_norm=3.2225\n",
            "interval_sec=15.64 | steps/sec=2.5580 | sec/step=0.3909\n",
            "examples_seen=2,007,040 | approx_epochs=1.3471\n",
            "batch_shapes input=(16, 25) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.720 | chrf=19.558 | comet=nan | text_eval_sec=3.51\n",
            "========================================================================================\n",
            "step  15720/32000 | avg_loss=2.7976 | train_ppl=16.4048 | lr=0.000171994 | grad_norm=2.9150\n",
            "interval_sec=15.59 | steps/sec=2.5652 | sec/step=0.3898\n",
            "examples_seen=2,012,160 | approx_epochs=1.3506\n",
            "batch_shapes input=(16, 25) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.337 | chrf=20.967 | comet=nan | text_eval_sec=3.49\n",
            "========================================================================================\n",
            "step  15760/32000 | avg_loss=2.8130 | train_ppl=16.6598 | lr=0.000171456 | grad_norm=3.0660\n",
            "interval_sec=15.60 | steps/sec=2.5645 | sec/step=0.3899\n",
            "examples_seen=2,017,280 | approx_epochs=1.3540\n",
            "batch_shapes input=(16, 21) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.224 | chrf=16.319 | comet=nan | text_eval_sec=3.50\n",
            "========================================================================================\n",
            "step  15800/32000 | avg_loss=2.7717 | train_ppl=15.9860 | lr=0.000170918 | grad_norm=3.1250\n",
            "interval_sec=15.77 | steps/sec=2.5370 | sec/step=0.3942\n",
            "examples_seen=2,022,400 | approx_epochs=1.3575\n",
            "batch_shapes input=(16, 24) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.695 | chrf=21.407 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  15840/32000 | avg_loss=2.8381 | train_ppl=17.0830 | lr=0.000170381 | grad_norm=3.0324\n",
            "interval_sec=15.63 | steps/sec=2.5594 | sec/step=0.3907\n",
            "examples_seen=2,027,520 | approx_epochs=1.3609\n",
            "batch_shapes input=(16, 21) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.013 | chrf=18.066 | comet=nan | text_eval_sec=3.64\n",
            "========================================================================================\n",
            "step  15880/32000 | avg_loss=2.7476 | train_ppl=15.6047 | lr=0.000169843 | grad_norm=2.9157\n",
            "interval_sec=15.58 | steps/sec=2.5678 | sec/step=0.3894\n",
            "examples_seen=2,032,640 | approx_epochs=1.3643\n",
            "batch_shapes input=(16, 20) labels=(16, 26)\n",
            "cuda_mem_alloc_gb=0.847 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.404 | chrf=21.311 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step  15920/32000 | avg_loss=2.7822 | train_ppl=16.1549 | lr=0.000169305 | grad_norm=3.0311\n",
            "interval_sec=16.13 | steps/sec=2.4795 | sec/step=0.4033\n",
            "examples_seen=2,037,760 | approx_epochs=1.3678\n",
            "batch_shapes input=(16, 16) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.221 | chrf=20.763 | comet=nan | text_eval_sec=3.70\n",
            "========================================================================================\n",
            "step  15960/32000 | avg_loss=2.7620 | train_ppl=15.8318 | lr=0.000168767 | grad_norm=2.8509\n",
            "interval_sec=16.10 | steps/sec=2.4849 | sec/step=0.4024\n",
            "examples_seen=2,042,880 | approx_epochs=1.3712\n",
            "batch_shapes input=(16, 21) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.849 | chrf=17.761 | comet=nan | text_eval_sec=3.65\n",
            "========================================================================================\n",
            "step  16000/32000 | avg_loss=2.7735 | train_ppl=16.0150 | lr=0.000168229 | grad_norm=2.7958\n",
            "interval_sec=16.05 | steps/sec=2.4929 | sec/step=0.4011\n",
            "examples_seen=2,048,000 | approx_epochs=1.3746\n",
            "batch_shapes input=(16, 20) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.486 | chrf=20.784 | comet=nan | text_eval_sec=3.67\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=16000 val_loss=2.7153 | val_ppl=15.1099 | bleu=1.435 | chrf=20.195 | comet=nan | text_eval_sec=27.84\n",
            "[best] new best val_loss=2.7153 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_016000\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_011000\n",
            "========================================================================================\n",
            "step  16040/32000 | avg_loss=2.8047 | train_ppl=16.5225 | lr=0.000167691 | grad_norm=2.8629\n",
            "interval_sec=55.34 | steps/sec=0.7228 | sec/step=1.3835\n",
            "examples_seen=2,053,120 | approx_epochs=1.3781\n",
            "batch_shapes input=(16, 16) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.242 | chrf=19.195 | comet=nan | text_eval_sec=4.01\n",
            "========================================================================================\n",
            "step  16080/32000 | avg_loss=2.8181 | train_ppl=16.7444 | lr=0.000167153 | grad_norm=2.8221\n",
            "interval_sec=15.62 | steps/sec=2.5607 | sec/step=0.3905\n",
            "examples_seen=2,058,240 | approx_epochs=1.3815\n",
            "batch_shapes input=(16, 23) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.845 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.340 | chrf=20.178 | comet=nan | text_eval_sec=3.59\n",
            "========================================================================================\n",
            "step  16120/32000 | avg_loss=2.7452 | train_ppl=15.5680 | lr=0.000166615 | grad_norm=2.8456\n",
            "interval_sec=15.53 | steps/sec=2.5763 | sec/step=0.3882\n",
            "examples_seen=2,063,360 | approx_epochs=1.3849\n",
            "batch_shapes input=(16, 40) labels=(16, 40)\n",
            "cuda_mem_alloc_gb=0.875 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.503 | chrf=21.433 | comet=nan | text_eval_sec=4.41\n",
            "========================================================================================\n",
            "step  16160/32000 | avg_loss=2.7765 | train_ppl=16.0630 | lr=0.000166076 | grad_norm=3.2166\n",
            "interval_sec=16.09 | steps/sec=2.4853 | sec/step=0.4024\n",
            "examples_seen=2,068,480 | approx_epochs=1.3884\n",
            "batch_shapes input=(16, 24) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.413 | chrf=20.949 | comet=nan | text_eval_sec=3.81\n",
            "========================================================================================\n",
            "step  16200/32000 | avg_loss=2.7774 | train_ppl=16.0777 | lr=0.000165538 | grad_norm=3.0399\n",
            "interval_sec=15.67 | steps/sec=2.5522 | sec/step=0.3918\n",
            "examples_seen=2,073,600 | approx_epochs=1.3918\n",
            "batch_shapes input=(16, 54) labels=(16, 50)\n",
            "cuda_mem_alloc_gb=0.894 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.887 | chrf=18.670 | comet=nan | text_eval_sec=3.54\n",
            "========================================================================================\n",
            "step  16240/32000 | avg_loss=2.7761 | train_ppl=16.0570 | lr=0.000165 | grad_norm=2.9509\n",
            "interval_sec=15.98 | steps/sec=2.5038 | sec/step=0.3994\n",
            "examples_seen=2,078,720 | approx_epochs=1.3953\n",
            "batch_shapes input=(16, 42) labels=(16, 46)\n",
            "cuda_mem_alloc_gb=0.886 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.576 | chrf=21.120 | comet=nan | text_eval_sec=3.50\n",
            "========================================================================================\n",
            "step  16280/32000 | avg_loss=2.7585 | train_ppl=15.7762 | lr=0.000164462 | grad_norm=3.0043\n",
            "interval_sec=15.64 | steps/sec=2.5574 | sec/step=0.3910\n",
            "examples_seen=2,083,840 | approx_epochs=1.3987\n",
            "batch_shapes input=(16, 20) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.845 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.461 | chrf=19.928 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  16320/32000 | avg_loss=2.8150 | train_ppl=16.6929 | lr=0.000163924 | grad_norm=2.9862\n",
            "interval_sec=15.81 | steps/sec=2.5303 | sec/step=0.3952\n",
            "examples_seen=2,088,960 | approx_epochs=1.4021\n",
            "batch_shapes input=(16, 73) labels=(16, 67)\n",
            "cuda_mem_alloc_gb=0.927 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.400 | chrf=22.224 | comet=nan | text_eval_sec=3.56\n",
            "========================================================================================\n",
            "step  16360/32000 | avg_loss=2.7963 | train_ppl=16.3837 | lr=0.000163385 | grad_norm=2.9674\n",
            "interval_sec=16.66 | steps/sec=2.4007 | sec/step=0.4165\n",
            "examples_seen=2,094,080 | approx_epochs=1.4056\n",
            "batch_shapes input=(16, 16) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.445 | chrf=19.175 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  16400/32000 | avg_loss=2.7570 | train_ppl=15.7530 | lr=0.000162847 | grad_norm=3.0891\n",
            "interval_sec=15.82 | steps/sec=2.5282 | sec/step=0.3955\n",
            "examples_seen=2,099,200 | approx_epochs=1.4090\n",
            "batch_shapes input=(16, 21) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.943 | chrf=24.429 | comet=nan | text_eval_sec=3.58\n",
            "[eval] step=16400 val_loss=2.6811 | val_ppl=14.6010 | bleu=1.345 | chrf=21.345 | comet=nan | text_eval_sec=27.98\n",
            "[best] new best val_loss=2.6811 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step  16440/32000 | avg_loss=2.7790 | train_ppl=16.1027 | lr=0.000162309 | grad_norm=2.9013\n",
            "interval_sec=49.83 | steps/sec=0.8028 | sec/step=1.2457\n",
            "examples_seen=2,104,320 | approx_epochs=1.4124\n",
            "batch_shapes input=(16, 20) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.671 | chrf=24.067 | comet=nan | text_eval_sec=4.08\n",
            "========================================================================================\n",
            "step  16480/32000 | avg_loss=2.7663 | train_ppl=15.8991 | lr=0.000161771 | grad_norm=3.0234\n",
            "interval_sec=15.63 | steps/sec=2.5593 | sec/step=0.3907\n",
            "examples_seen=2,109,440 | approx_epochs=1.4159\n",
            "batch_shapes input=(16, 27) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.844 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.745 | chrf=23.059 | comet=nan | text_eval_sec=3.56\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_016500\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_011500\n",
            "========================================================================================\n",
            "step  16520/32000 | avg_loss=2.7739 | train_ppl=16.0216 | lr=0.000161233 | grad_norm=2.8262\n",
            "interval_sec=16.33 | steps/sec=2.4495 | sec/step=0.4083\n",
            "examples_seen=2,114,560 | approx_epochs=1.4193\n",
            "batch_shapes input=(16, 24) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.961 | chrf=20.607 | comet=nan | text_eval_sec=4.50\n",
            "========================================================================================\n",
            "step  16560/32000 | avg_loss=2.7389 | train_ppl=15.4702 | lr=0.000160695 | grad_norm=2.8206\n",
            "interval_sec=15.63 | steps/sec=2.5586 | sec/step=0.3908\n",
            "examples_seen=2,119,680 | approx_epochs=1.4227\n",
            "batch_shapes input=(16, 15) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.927 | chrf=22.409 | comet=nan | text_eval_sec=3.55\n",
            "========================================================================================\n",
            "step  16600/32000 | avg_loss=2.8393 | train_ppl=17.1039 | lr=0.000160157 | grad_norm=3.0313\n",
            "interval_sec=15.82 | steps/sec=2.5291 | sec/step=0.3954\n",
            "examples_seen=2,124,800 | approx_epochs=1.4262\n",
            "batch_shapes input=(16, 30) labels=(16, 31)\n",
            "cuda_mem_alloc_gb=0.857 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.125 | chrf=16.333 | comet=nan | text_eval_sec=3.53\n",
            "========================================================================================\n",
            "step  16640/32000 | avg_loss=2.7355 | train_ppl=15.4182 | lr=0.000159619 | grad_norm=2.6625\n",
            "interval_sec=15.63 | steps/sec=2.5589 | sec/step=0.3908\n",
            "examples_seen=2,129,920 | approx_epochs=1.4296\n",
            "batch_shapes input=(16, 17) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.360 | chrf=19.658 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step  16680/32000 | avg_loss=2.7586 | train_ppl=15.7774 | lr=0.000159082 | grad_norm=2.9519\n",
            "interval_sec=15.58 | steps/sec=2.5671 | sec/step=0.3895\n",
            "examples_seen=2,135,040 | approx_epochs=1.4331\n",
            "batch_shapes input=(16, 15) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.148 | chrf=20.449 | comet=nan | text_eval_sec=3.51\n",
            "========================================================================================\n",
            "step  16720/32000 | avg_loss=2.7219 | train_ppl=15.2095 | lr=0.000158544 | grad_norm=3.0033\n",
            "interval_sec=15.72 | steps/sec=2.5448 | sec/step=0.3930\n",
            "examples_seen=2,140,160 | approx_epochs=1.4365\n",
            "batch_shapes input=(16, 44) labels=(16, 49)\n",
            "cuda_mem_alloc_gb=0.892 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.242 | chrf=21.119 | comet=nan | text_eval_sec=3.63\n",
            "========================================================================================\n",
            "step  16760/32000 | avg_loss=2.7760 | train_ppl=16.0544 | lr=0.000158006 | grad_norm=2.8485\n",
            "interval_sec=15.63 | steps/sec=2.5595 | sec/step=0.3907\n",
            "examples_seen=2,145,280 | approx_epochs=1.4399\n",
            "batch_shapes input=(16, 15) labels=(16, 12)\n",
            "cuda_mem_alloc_gb=0.820 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.613 | chrf=21.865 | comet=nan | text_eval_sec=3.63\n",
            "========================================================================================\n",
            "step  16800/32000 | avg_loss=2.7568 | train_ppl=15.7498 | lr=0.000157469 | grad_norm=2.9414\n",
            "interval_sec=15.65 | steps/sec=2.5551 | sec/step=0.3914\n",
            "examples_seen=2,150,400 | approx_epochs=1.4434\n",
            "batch_shapes input=(16, 25) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.769 | chrf=22.868 | comet=nan | text_eval_sec=3.66\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=16800 val_loss=2.6017 | val_ppl=13.4866 | bleu=1.360 | chrf=21.295 | comet=nan | text_eval_sec=28.04\n",
            "[best] new best val_loss=2.6017 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step  16840/32000 | avg_loss=2.7406 | train_ppl=15.4956 | lr=0.000156932 | grad_norm=2.7290\n",
            "interval_sec=50.08 | steps/sec=0.7987 | sec/step=1.2521\n",
            "examples_seen=2,155,520 | approx_epochs=1.4468\n",
            "batch_shapes input=(16, 48) labels=(16, 55)\n",
            "cuda_mem_alloc_gb=0.903 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.209 | chrf=19.907 | comet=nan | text_eval_sec=3.55\n",
            "========================================================================================\n",
            "step  16880/32000 | avg_loss=2.7684 | train_ppl=15.9335 | lr=0.000156394 | grad_norm=2.8264\n",
            "interval_sec=15.67 | steps/sec=2.5524 | sec/step=0.3918\n",
            "examples_seen=2,160,640 | approx_epochs=1.4502\n",
            "batch_shapes input=(16, 15) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.401 | chrf=19.340 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step  16920/32000 | avg_loss=2.7329 | train_ppl=15.3772 | lr=0.000155857 | grad_norm=3.0137\n",
            "interval_sec=15.57 | steps/sec=2.5686 | sec/step=0.3893\n",
            "examples_seen=2,165,760 | approx_epochs=1.4537\n",
            "batch_shapes input=(16, 27) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.624 | chrf=22.547 | comet=nan | text_eval_sec=3.59\n",
            "========================================================================================\n",
            "step  16960/32000 | avg_loss=2.7362 | train_ppl=15.4290 | lr=0.00015532 | grad_norm=3.0330\n",
            "interval_sec=15.64 | steps/sec=2.5578 | sec/step=0.3910\n",
            "examples_seen=2,170,880 | approx_epochs=1.4571\n",
            "batch_shapes input=(16, 16) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.827 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.573 | chrf=19.606 | comet=nan | text_eval_sec=3.63\n",
            "========================================================================================\n",
            "step  17000/32000 | avg_loss=2.7275 | train_ppl=15.2948 | lr=0.000154784 | grad_norm=3.2514\n",
            "interval_sec=15.66 | steps/sec=2.5540 | sec/step=0.3915\n",
            "examples_seen=2,176,000 | approx_epochs=1.4605\n",
            "batch_shapes input=(16, 26) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.279 | chrf=22.466 | comet=nan | text_eval_sec=3.79\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_017000\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_012000\n",
            "========================================================================================\n",
            "step  17040/32000 | avg_loss=2.6891 | train_ppl=14.7191 | lr=0.000154247 | grad_norm=2.8170\n",
            "interval_sec=16.70 | steps/sec=2.3952 | sec/step=0.4175\n",
            "examples_seen=2,181,120 | approx_epochs=1.4640\n",
            "batch_shapes input=(16, 15) labels=(16, 14)\n",
            "cuda_mem_alloc_gb=0.824 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.359 | chrf=20.881 | comet=nan | text_eval_sec=3.64\n",
            "========================================================================================\n",
            "step  17080/32000 | avg_loss=2.7810 | train_ppl=16.1356 | lr=0.000153711 | grad_norm=2.7364\n",
            "interval_sec=15.67 | steps/sec=2.5528 | sec/step=0.3917\n",
            "examples_seen=2,186,240 | approx_epochs=1.4674\n",
            "batch_shapes input=(16, 20) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.325 | chrf=22.533 | comet=nan | text_eval_sec=3.59\n",
            "========================================================================================\n",
            "step  17120/32000 | avg_loss=2.7505 | train_ppl=15.6510 | lr=0.000153174 | grad_norm=3.0342\n",
            "interval_sec=15.81 | steps/sec=2.5301 | sec/step=0.3952\n",
            "examples_seen=2,191,360 | approx_epochs=1.4709\n",
            "batch_shapes input=(16, 21) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.374 | chrf=23.266 | comet=nan | text_eval_sec=3.64\n",
            "========================================================================================\n",
            "step  17160/32000 | avg_loss=2.7688 | train_ppl=15.9399 | lr=0.000152638 | grad_norm=3.0077\n",
            "interval_sec=15.63 | steps/sec=2.5599 | sec/step=0.3906\n",
            "examples_seen=2,196,480 | approx_epochs=1.4743\n",
            "batch_shapes input=(16, 23) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.132 | chrf=19.314 | comet=nan | text_eval_sec=3.51\n",
            "========================================================================================\n",
            "step  17200/32000 | avg_loss=2.7758 | train_ppl=16.0510 | lr=0.000152102 | grad_norm=3.2356\n",
            "interval_sec=15.84 | steps/sec=2.5253 | sec/step=0.3960\n",
            "examples_seen=2,201,600 | approx_epochs=1.4777\n",
            "batch_shapes input=(16, 17) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.164 | chrf=18.305 | comet=nan | text_eval_sec=3.57\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=17200 val_loss=2.6245 | val_ppl=13.7978 | bleu=1.463 | chrf=21.974 | comet=nan | text_eval_sec=28.58\n",
            "========================================================================================\n",
            "step  17240/32000 | avg_loss=2.7256 | train_ppl=15.2659 | lr=0.000151567 | grad_norm=3.1551\n",
            "interval_sec=49.93 | steps/sec=0.8012 | sec/step=1.2481\n",
            "examples_seen=2,206,720 | approx_epochs=1.4812\n",
            "batch_shapes input=(16, 17) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.623 | chrf=21.758 | comet=nan | text_eval_sec=5.49\n",
            "========================================================================================\n",
            "step  17280/32000 | avg_loss=2.7458 | train_ppl=15.5765 | lr=0.000151031 | grad_norm=2.8913\n",
            "interval_sec=16.13 | steps/sec=2.4794 | sec/step=0.4033\n",
            "examples_seen=2,211,840 | approx_epochs=1.4846\n",
            "batch_shapes input=(16, 37) labels=(16, 30)\n",
            "cuda_mem_alloc_gb=0.855 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.733 | chrf=23.541 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step  17320/32000 | avg_loss=2.6956 | train_ppl=14.8151 | lr=0.000150496 | grad_norm=2.9335\n",
            "interval_sec=15.60 | steps/sec=2.5643 | sec/step=0.3900\n",
            "examples_seen=2,216,960 | approx_epochs=1.4880\n",
            "batch_shapes input=(16, 19) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.315 | chrf=21.316 | comet=nan | text_eval_sec=3.77\n",
            "========================================================================================\n",
            "step  17360/32000 | avg_loss=2.7260 | train_ppl=15.2712 | lr=0.000149961 | grad_norm=2.8612\n",
            "interval_sec=15.81 | steps/sec=2.5300 | sec/step=0.3953\n",
            "examples_seen=2,222,080 | approx_epochs=1.4915\n",
            "batch_shapes input=(16, 21) labels=(16, 14)\n",
            "cuda_mem_alloc_gb=0.824 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.727 | chrf=22.299 | comet=nan | text_eval_sec=3.49\n",
            "========================================================================================\n",
            "step  17400/32000 | avg_loss=2.7446 | train_ppl=15.5581 | lr=0.000149426 | grad_norm=3.0443\n",
            "interval_sec=15.84 | steps/sec=2.5254 | sec/step=0.3960\n",
            "examples_seen=2,227,200 | approx_epochs=1.4949\n",
            "batch_shapes input=(16, 21) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.522 | chrf=22.575 | comet=nan | text_eval_sec=3.65\n",
            "========================================================================================\n",
            "step  17440/32000 | avg_loss=2.7065 | train_ppl=14.9763 | lr=0.000148892 | grad_norm=2.7599\n",
            "interval_sec=15.83 | steps/sec=2.5267 | sec/step=0.3958\n",
            "examples_seen=2,232,320 | approx_epochs=1.4984\n",
            "batch_shapes input=(16, 23) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.845 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.339 | chrf=21.837 | comet=nan | text_eval_sec=3.61\n",
            "========================================================================================\n",
            "step  17480/32000 | avg_loss=2.7212 | train_ppl=15.1983 | lr=0.000148358 | grad_norm=3.0287\n",
            "interval_sec=15.62 | steps/sec=2.5605 | sec/step=0.3905\n",
            "examples_seen=2,237,440 | approx_epochs=1.5018\n",
            "batch_shapes input=(16, 16) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.839 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.365 | chrf=20.256 | comet=nan | text_eval_sec=3.64\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_017500\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_012500\n",
            "========================================================================================\n",
            "step  17520/32000 | avg_loss=2.7504 | train_ppl=15.6495 | lr=0.000147824 | grad_norm=2.8305\n",
            "interval_sec=16.48 | steps/sec=2.4272 | sec/step=0.4120\n",
            "examples_seen=2,242,560 | approx_epochs=1.5052\n",
            "batch_shapes input=(16, 17) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.339 | chrf=21.932 | comet=nan | text_eval_sec=4.05\n",
            "========================================================================================\n",
            "step  17560/32000 | avg_loss=2.7068 | train_ppl=14.9819 | lr=0.00014729 | grad_norm=3.0317\n",
            "interval_sec=15.66 | steps/sec=2.5550 | sec/step=0.3914\n",
            "examples_seen=2,247,680 | approx_epochs=1.5087\n",
            "batch_shapes input=(16, 19) labels=(16, 13)\n",
            "cuda_mem_alloc_gb=0.822 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.943 | chrf=23.556 | comet=nan | text_eval_sec=3.61\n",
            "========================================================================================\n",
            "step  17600/32000 | avg_loss=2.6765 | train_ppl=14.5334 | lr=0.000146757 | grad_norm=3.1838\n",
            "interval_sec=15.54 | steps/sec=2.5734 | sec/step=0.3886\n",
            "examples_seen=2,252,800 | approx_epochs=1.5121\n",
            "batch_shapes input=(16, 22) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.739 | chrf=24.121 | comet=nan | text_eval_sec=3.60\n",
            "[eval] step=17600 val_loss=2.6661 | val_ppl=14.3842 | bleu=1.589 | chrf=22.695 | comet=nan | text_eval_sec=28.53\n",
            "========================================================================================\n",
            "step  17640/32000 | avg_loss=2.7054 | train_ppl=14.9596 | lr=0.000146223 | grad_norm=3.1597\n",
            "interval_sec=50.35 | steps/sec=0.7944 | sec/step=1.2589\n",
            "examples_seen=2,257,920 | approx_epochs=1.5155\n",
            "batch_shapes input=(16, 28) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.846 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.702 | chrf=20.828 | comet=nan | text_eval_sec=3.71\n",
            "========================================================================================\n",
            "step  17680/32000 | avg_loss=2.6837 | train_ppl=14.6395 | lr=0.000145691 | grad_norm=2.9253\n",
            "interval_sec=15.93 | steps/sec=2.5116 | sec/step=0.3982\n",
            "examples_seen=2,263,040 | approx_epochs=1.5190\n",
            "batch_shapes input=(16, 22) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.400 | chrf=19.921 | comet=nan | text_eval_sec=3.59\n",
            "========================================================================================\n",
            "step  17720/32000 | avg_loss=2.7352 | train_ppl=15.4122 | lr=0.000145158 | grad_norm=2.9578\n",
            "interval_sec=15.87 | steps/sec=2.5209 | sec/step=0.3967\n",
            "examples_seen=2,268,160 | approx_epochs=1.5224\n",
            "batch_shapes input=(16, 17) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.290 | chrf=23.088 | comet=nan | text_eval_sec=3.63\n",
            "========================================================================================\n",
            "step  17760/32000 | avg_loss=2.7368 | train_ppl=15.4375 | lr=0.000144626 | grad_norm=3.0771\n",
            "interval_sec=15.70 | steps/sec=2.5472 | sec/step=0.3926\n",
            "examples_seen=2,273,280 | approx_epochs=1.5258\n",
            "batch_shapes input=(16, 22) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.691 | chrf=21.823 | comet=nan | text_eval_sec=3.55\n",
            "========================================================================================\n",
            "step  17800/32000 | avg_loss=2.7103 | train_ppl=15.0333 | lr=0.000144094 | grad_norm=3.0724\n",
            "interval_sec=15.73 | steps/sec=2.5428 | sec/step=0.3933\n",
            "examples_seen=2,278,400 | approx_epochs=1.5293\n",
            "batch_shapes input=(16, 14) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.433 | chrf=21.070 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  17840/32000 | avg_loss=2.6924 | train_ppl=14.7670 | lr=0.000143562 | grad_norm=2.9773\n",
            "interval_sec=15.49 | steps/sec=2.5831 | sec/step=0.3871\n",
            "examples_seen=2,283,520 | approx_epochs=1.5327\n",
            "batch_shapes input=(16, 20) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.203 | chrf=20.524 | comet=nan | text_eval_sec=3.56\n",
            "========================================================================================\n",
            "step  17880/32000 | avg_loss=2.7589 | train_ppl=15.7828 | lr=0.000143031 | grad_norm=3.1049\n",
            "interval_sec=15.71 | steps/sec=2.5456 | sec/step=0.3928\n",
            "examples_seen=2,288,640 | approx_epochs=1.5362\n",
            "batch_shapes input=(16, 20) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.499 | chrf=22.703 | comet=nan | text_eval_sec=3.48\n",
            "========================================================================================\n",
            "step  17920/32000 | avg_loss=2.7004 | train_ppl=14.8857 | lr=0.0001425 | grad_norm=2.9419\n",
            "interval_sec=15.55 | steps/sec=2.5728 | sec/step=0.3887\n",
            "examples_seen=2,293,760 | approx_epochs=1.5396\n",
            "batch_shapes input=(16, 20) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.843 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.857 | chrf=25.883 | comet=nan | text_eval_sec=3.73\n",
            "========================================================================================\n",
            "step  17960/32000 | avg_loss=2.7247 | train_ppl=15.2517 | lr=0.00014197 | grad_norm=2.7175\n",
            "interval_sec=15.67 | steps/sec=2.5534 | sec/step=0.3916\n",
            "examples_seen=2,298,880 | approx_epochs=1.5430\n",
            "batch_shapes input=(16, 22) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.604 | chrf=20.699 | comet=nan | text_eval_sec=3.67\n",
            "========================================================================================\n",
            "step  18000/32000 | avg_loss=2.7097 | train_ppl=15.0244 | lr=0.00014144 | grad_norm=3.1692\n",
            "interval_sec=16.00 | steps/sec=2.4996 | sec/step=0.4001\n",
            "examples_seen=2,304,000 | approx_epochs=1.5465\n",
            "batch_shapes input=(16, 23) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.718 | chrf=16.924 | comet=nan | text_eval_sec=3.64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=18000 val_loss=2.6144 | val_ppl=13.6591 | bleu=1.528 | chrf=22.191 | comet=nan | text_eval_sec=28.21\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_018000\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_013000\n",
            "========================================================================================\n",
            "step  18040/32000 | avg_loss=2.7171 | train_ppl=15.1369 | lr=0.00014091 | grad_norm=3.1198\n",
            "interval_sec=50.37 | steps/sec=0.7942 | sec/step=1.2592\n",
            "examples_seen=2,309,120 | approx_epochs=1.5499\n",
            "batch_shapes input=(16, 18) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=2.099 | chrf=22.731 | comet=nan | text_eval_sec=3.61\n",
            "========================================================================================\n",
            "step  18080/32000 | avg_loss=2.7175 | train_ppl=15.1427 | lr=0.000140381 | grad_norm=2.9469\n",
            "interval_sec=15.67 | steps/sec=2.5522 | sec/step=0.3918\n",
            "examples_seen=2,314,240 | approx_epochs=1.5533\n",
            "batch_shapes input=(16, 23) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.285 | chrf=19.827 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step  18120/32000 | avg_loss=2.6392 | train_ppl=14.0022 | lr=0.000139852 | grad_norm=3.0051\n",
            "interval_sec=16.15 | steps/sec=2.4768 | sec/step=0.4037\n",
            "examples_seen=2,319,360 | approx_epochs=1.5568\n",
            "batch_shapes input=(16, 23) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.845 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.784 | chrf=21.183 | comet=nan | text_eval_sec=3.51\n",
            "========================================================================================\n",
            "step  18160/32000 | avg_loss=2.6867 | train_ppl=14.6832 | lr=0.000139323 | grad_norm=2.9326\n",
            "interval_sec=16.44 | steps/sec=2.4333 | sec/step=0.4110\n",
            "examples_seen=2,324,480 | approx_epochs=1.5602\n",
            "batch_shapes input=(16, 23) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.273 | chrf=22.020 | comet=nan | text_eval_sec=3.65\n",
            "========================================================================================\n",
            "step  18200/32000 | avg_loss=2.6842 | train_ppl=14.6461 | lr=0.000138795 | grad_norm=2.7478\n",
            "interval_sec=15.67 | steps/sec=2.5533 | sec/step=0.3916\n",
            "examples_seen=2,329,600 | approx_epochs=1.5636\n",
            "batch_shapes input=(16, 17) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.771 | chrf=21.096 | comet=nan | text_eval_sec=3.55\n",
            "========================================================================================\n",
            "step  18240/32000 | avg_loss=2.7168 | train_ppl=15.1312 | lr=0.000138267 | grad_norm=2.9754\n",
            "interval_sec=15.52 | steps/sec=2.5775 | sec/step=0.3880\n",
            "examples_seen=2,334,720 | approx_epochs=1.5671\n",
            "batch_shapes input=(16, 16) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.279 | chrf=19.052 | comet=nan | text_eval_sec=3.59\n",
            "========================================================================================\n",
            "step  18280/32000 | avg_loss=2.6971 | train_ppl=14.8362 | lr=0.00013774 | grad_norm=2.8548\n",
            "interval_sec=15.67 | steps/sec=2.5524 | sec/step=0.3918\n",
            "examples_seen=2,339,840 | approx_epochs=1.5705\n",
            "batch_shapes input=(16, 17) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.301 | chrf=19.395 | comet=nan | text_eval_sec=3.65\n",
            "========================================================================================\n",
            "step  18320/32000 | avg_loss=2.6627 | train_ppl=14.3356 | lr=0.000137213 | grad_norm=2.8771\n",
            "interval_sec=15.64 | steps/sec=2.5581 | sec/step=0.3909\n",
            "examples_seen=2,344,960 | approx_epochs=1.5740\n",
            "batch_shapes input=(16, 22) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.843 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.853 | chrf=24.265 | comet=nan | text_eval_sec=3.52\n",
            "========================================================================================\n",
            "step  18360/32000 | avg_loss=2.6931 | train_ppl=14.7780 | lr=0.000136686 | grad_norm=3.0555\n",
            "interval_sec=15.58 | steps/sec=2.5675 | sec/step=0.3895\n",
            "examples_seen=2,350,080 | approx_epochs=1.5774\n",
            "batch_shapes input=(16, 40) labels=(16, 32)\n",
            "cuda_mem_alloc_gb=0.859 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.462 | chrf=18.499 | comet=nan | text_eval_sec=3.54\n",
            "========================================================================================\n",
            "step  18400/32000 | avg_loss=2.7111 | train_ppl=15.0459 | lr=0.00013616 | grad_norm=2.7742\n",
            "interval_sec=15.60 | steps/sec=2.5647 | sec/step=0.3899\n",
            "examples_seen=2,355,200 | approx_epochs=1.5808\n",
            "batch_shapes input=(16, 62) labels=(16, 62)\n",
            "cuda_mem_alloc_gb=0.917 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.385 | chrf=20.272 | comet=nan | text_eval_sec=3.47\n",
            "[eval] step=18400 val_loss=2.5811 | val_ppl=13.2117 | bleu=1.572 | chrf=19.627 | comet=nan | text_eval_sec=28.12\n",
            "[best] new best val_loss=2.5811 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step  18440/32000 | avg_loss=2.6873 | train_ppl=14.6923 | lr=0.000135635 | grad_norm=2.8196\n",
            "interval_sec=50.00 | steps/sec=0.7999 | sec/step=1.2501\n",
            "examples_seen=2,360,320 | approx_epochs=1.5843\n",
            "batch_shapes input=(16, 18) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.843 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=2.083 | chrf=22.435 | comet=nan | text_eval_sec=3.54\n",
            "========================================================================================\n",
            "step  18480/32000 | avg_loss=2.6546 | train_ppl=14.2199 | lr=0.00013511 | grad_norm=2.9874\n",
            "interval_sec=15.81 | steps/sec=2.5296 | sec/step=0.3953\n",
            "examples_seen=2,365,440 | approx_epochs=1.5877\n",
            "batch_shapes input=(16, 25) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.844 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.296 | chrf=21.702 | comet=nan | text_eval_sec=3.53\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_018500\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_013500\n",
            "========================================================================================\n",
            "step  18520/32000 | avg_loss=2.6868 | train_ppl=14.6849 | lr=0.000134585 | grad_norm=3.0370\n",
            "interval_sec=16.41 | steps/sec=2.4371 | sec/step=0.4103\n",
            "examples_seen=2,370,560 | approx_epochs=1.5911\n",
            "batch_shapes input=(16, 22) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.549 | chrf=20.951 | comet=nan | text_eval_sec=4.99\n",
            "========================================================================================\n",
            "step  18560/32000 | avg_loss=2.6905 | train_ppl=14.7391 | lr=0.000134061 | grad_norm=2.9397\n",
            "interval_sec=15.53 | steps/sec=2.5764 | sec/step=0.3881\n",
            "examples_seen=2,375,680 | approx_epochs=1.5946\n",
            "batch_shapes input=(16, 18) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.732 | chrf=21.095 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  18600/32000 | avg_loss=2.6629 | train_ppl=14.3384 | lr=0.000133537 | grad_norm=2.6990\n",
            "interval_sec=15.78 | steps/sec=2.5342 | sec/step=0.3946\n",
            "examples_seen=2,380,800 | approx_epochs=1.5980\n",
            "batch_shapes input=(16, 19) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.128 | chrf=17.895 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  18640/32000 | avg_loss=2.6953 | train_ppl=14.8094 | lr=0.000133014 | grad_norm=2.8671\n",
            "interval_sec=15.62 | steps/sec=2.5601 | sec/step=0.3906\n",
            "examples_seen=2,385,920 | approx_epochs=1.6014\n",
            "batch_shapes input=(16, 20) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.984 | chrf=26.689 | comet=nan | text_eval_sec=3.68\n",
            "========================================================================================\n",
            "step  18680/32000 | avg_loss=2.7170 | train_ppl=15.1342 | lr=0.000132491 | grad_norm=3.0855\n",
            "interval_sec=15.59 | steps/sec=2.5659 | sec/step=0.3897\n",
            "examples_seen=2,391,040 | approx_epochs=1.6049\n",
            "batch_shapes input=(16, 16) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.813 | chrf=23.739 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  18720/32000 | avg_loss=2.6512 | train_ppl=14.1707 | lr=0.000131969 | grad_norm=2.8613\n",
            "interval_sec=15.60 | steps/sec=2.5638 | sec/step=0.3900\n",
            "examples_seen=2,396,160 | approx_epochs=1.6083\n",
            "batch_shapes input=(16, 23) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.844 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.829 | chrf=22.182 | comet=nan | text_eval_sec=3.47\n",
            "========================================================================================\n",
            "step  18760/32000 | avg_loss=2.6620 | train_ppl=14.3243 | lr=0.000131448 | grad_norm=2.7782\n",
            "interval_sec=15.62 | steps/sec=2.5615 | sec/step=0.3904\n",
            "examples_seen=2,401,280 | approx_epochs=1.6118\n",
            "batch_shapes input=(16, 17) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.652 | chrf=21.566 | comet=nan | text_eval_sec=3.48\n",
            "========================================================================================\n",
            "step  18800/32000 | avg_loss=2.6507 | train_ppl=14.1637 | lr=0.000130927 | grad_norm=3.0023\n",
            "interval_sec=15.64 | steps/sec=2.5582 | sec/step=0.3909\n",
            "examples_seen=2,406,400 | approx_epochs=1.6152\n",
            "batch_shapes input=(16, 24) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.703 | chrf=22.011 | comet=nan | text_eval_sec=3.61\n",
            "[eval] step=18800 val_loss=2.6045 | val_ppl=13.5239 | bleu=1.378 | chrf=19.508 | comet=nan | text_eval_sec=28.50\n",
            "========================================================================================\n",
            "step  18840/32000 | avg_loss=2.6632 | train_ppl=14.3427 | lr=0.000130406 | grad_norm=2.9188\n",
            "interval_sec=50.29 | steps/sec=0.7954 | sec/step=1.2572\n",
            "examples_seen=2,411,520 | approx_epochs=1.6186\n",
            "batch_shapes input=(16, 16) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.618 | chrf=21.266 | comet=nan | text_eval_sec=3.56\n",
            "========================================================================================\n",
            "step  18880/32000 | avg_loss=2.6849 | train_ppl=14.6567 | lr=0.000129886 | grad_norm=3.0466\n",
            "interval_sec=15.60 | steps/sec=2.5635 | sec/step=0.3901\n",
            "examples_seen=2,416,640 | approx_epochs=1.6221\n",
            "batch_shapes input=(16, 22) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.789 | chrf=23.989 | comet=nan | text_eval_sec=3.61\n",
            "========================================================================================\n",
            "step  18920/32000 | avg_loss=2.6775 | train_ppl=14.5487 | lr=0.000129367 | grad_norm=3.0950\n",
            "interval_sec=15.62 | steps/sec=2.5601 | sec/step=0.3906\n",
            "examples_seen=2,421,760 | approx_epochs=1.6255\n",
            "batch_shapes input=(16, 22) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.392 | chrf=20.062 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step  18960/32000 | avg_loss=2.6556 | train_ppl=14.2337 | lr=0.000128848 | grad_norm=2.8183\n",
            "interval_sec=15.55 | steps/sec=2.5719 | sec/step=0.3888\n",
            "examples_seen=2,426,880 | approx_epochs=1.6289\n",
            "batch_shapes input=(16, 22) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.431 | chrf=18.843 | comet=nan | text_eval_sec=3.56\n",
            "========================================================================================\n",
            "step  19000/32000 | avg_loss=2.7076 | train_ppl=14.9938 | lr=0.00012833 | grad_norm=2.9490\n",
            "interval_sec=15.73 | steps/sec=2.5424 | sec/step=0.3933\n",
            "examples_seen=2,432,000 | approx_epochs=1.6324\n",
            "batch_shapes input=(16, 43) labels=(16, 46)\n",
            "cuda_mem_alloc_gb=0.886 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.665 | chrf=22.231 | comet=nan | text_eval_sec=3.60\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_019000\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_014000\n",
            "========================================================================================\n",
            "step  19040/32000 | avg_loss=2.6492 | train_ppl=14.1432 | lr=0.000127812 | grad_norm=2.8405\n",
            "interval_sec=16.56 | steps/sec=2.4152 | sec/step=0.4140\n",
            "examples_seen=2,437,120 | approx_epochs=1.6358\n",
            "batch_shapes input=(16, 24) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.363 | chrf=20.564 | comet=nan | text_eval_sec=4.82\n",
            "========================================================================================\n",
            "step  19080/32000 | avg_loss=2.6374 | train_ppl=13.9766 | lr=0.000127295 | grad_norm=3.3147\n",
            "interval_sec=15.54 | steps/sec=2.5743 | sec/step=0.3885\n",
            "examples_seen=2,442,240 | approx_epochs=1.6393\n",
            "batch_shapes input=(16, 19) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.608 | chrf=22.100 | comet=nan | text_eval_sec=3.54\n",
            "========================================================================================\n",
            "step  19120/32000 | avg_loss=2.6439 | train_ppl=14.0678 | lr=0.000126778 | grad_norm=2.8609\n",
            "interval_sec=15.68 | steps/sec=2.5502 | sec/step=0.3921\n",
            "examples_seen=2,447,360 | approx_epochs=1.6427\n",
            "batch_shapes input=(16, 17) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.574 | chrf=19.360 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step  19160/32000 | avg_loss=2.6129 | train_ppl=13.6380 | lr=0.000126262 | grad_norm=3.0319\n",
            "interval_sec=15.80 | steps/sec=2.5312 | sec/step=0.3951\n",
            "examples_seen=2,452,480 | approx_epochs=1.6461\n",
            "batch_shapes input=(16, 15) labels=(16, 11)\n",
            "cuda_mem_alloc_gb=0.818 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.247 | chrf=24.850 | comet=nan | text_eval_sec=3.56\n",
            "========================================================================================\n",
            "step  19200/32000 | avg_loss=2.5955 | train_ppl=13.4032 | lr=0.000125747 | grad_norm=3.1284\n",
            "interval_sec=15.72 | steps/sec=2.5441 | sec/step=0.3931\n",
            "examples_seen=2,457,600 | approx_epochs=1.6496\n",
            "batch_shapes input=(16, 16) labels=(16, 11)\n",
            "cuda_mem_alloc_gb=0.818 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.922 | chrf=21.033 | comet=nan | text_eval_sec=3.55\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=19200 val_loss=2.5477 | val_ppl=12.7781 | bleu=1.442 | chrf=20.833 | comet=nan | text_eval_sec=28.08\n",
            "[best] new best val_loss=2.5477 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step  19240/32000 | avg_loss=2.6320 | train_ppl=13.9015 | lr=0.000125233 | grad_norm=2.9164\n",
            "interval_sec=49.85 | steps/sec=0.8023 | sec/step=1.2464\n",
            "examples_seen=2,462,720 | approx_epochs=1.6530\n",
            "batch_shapes input=(16, 14) labels=(16, 12)\n",
            "cuda_mem_alloc_gb=0.820 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.677 | chrf=19.517 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step  19280/32000 | avg_loss=2.6413 | train_ppl=14.0312 | lr=0.000124719 | grad_norm=2.9892\n",
            "interval_sec=15.54 | steps/sec=2.5734 | sec/step=0.3886\n",
            "examples_seen=2,467,840 | approx_epochs=1.6564\n",
            "batch_shapes input=(16, 21) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.771 | chrf=20.007 | comet=nan | text_eval_sec=3.52\n",
            "========================================================================================\n",
            "step  19320/32000 | avg_loss=2.6265 | train_ppl=13.8247 | lr=0.000124205 | grad_norm=2.8199\n",
            "interval_sec=15.60 | steps/sec=2.5643 | sec/step=0.3900\n",
            "examples_seen=2,472,960 | approx_epochs=1.6599\n",
            "batch_shapes input=(16, 19) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.493 | chrf=24.646 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  19360/32000 | avg_loss=2.6643 | train_ppl=14.3580 | lr=0.000123692 | grad_norm=2.9996\n",
            "interval_sec=15.70 | steps/sec=2.5478 | sec/step=0.3925\n",
            "examples_seen=2,478,080 | approx_epochs=1.6633\n",
            "batch_shapes input=(16, 17) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.741 | chrf=22.646 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step  19400/32000 | avg_loss=2.6880 | train_ppl=14.7021 | lr=0.00012318 | grad_norm=3.1452\n",
            "interval_sec=15.65 | steps/sec=2.5565 | sec/step=0.3912\n",
            "examples_seen=2,483,200 | approx_epochs=1.6667\n",
            "batch_shapes input=(16, 22) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.843 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.337 | chrf=18.737 | comet=nan | text_eval_sec=3.48\n",
            "========================================================================================\n",
            "step  19440/32000 | avg_loss=2.6454 | train_ppl=14.0889 | lr=0.000122669 | grad_norm=3.1227\n",
            "interval_sec=15.47 | steps/sec=2.5859 | sec/step=0.3867\n",
            "examples_seen=2,488,320 | approx_epochs=1.6702\n",
            "batch_shapes input=(16, 19) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.892 | chrf=19.683 | comet=nan | text_eval_sec=3.61\n",
            "========================================================================================\n",
            "step  19480/32000 | avg_loss=2.6276 | train_ppl=13.8407 | lr=0.000122158 | grad_norm=2.8785\n",
            "interval_sec=15.70 | steps/sec=2.5478 | sec/step=0.3925\n",
            "examples_seen=2,493,440 | approx_epochs=1.6736\n",
            "batch_shapes input=(16, 28) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.345 | chrf=20.216 | comet=nan | text_eval_sec=4.16\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_019500\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_014500\n",
            "========================================================================================\n",
            "step  19520/32000 | avg_loss=2.6214 | train_ppl=13.7552 | lr=0.000121648 | grad_norm=2.9349\n",
            "interval_sec=16.33 | steps/sec=2.4495 | sec/step=0.4082\n",
            "examples_seen=2,498,560 | approx_epochs=1.6771\n",
            "batch_shapes input=(16, 26) labels=(16, 27)\n",
            "cuda_mem_alloc_gb=0.849 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.650 | chrf=20.532 | comet=nan | text_eval_sec=4.03\n",
            "========================================================================================\n",
            "step  19560/32000 | avg_loss=2.6091 | train_ppl=13.5872 | lr=0.000121139 | grad_norm=2.8912\n",
            "interval_sec=15.52 | steps/sec=2.5769 | sec/step=0.3881\n",
            "examples_seen=2,503,680 | approx_epochs=1.6805\n",
            "batch_shapes input=(16, 21) labels=(16, 26)\n",
            "cuda_mem_alloc_gb=0.847 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.031 | chrf=16.949 | comet=nan | text_eval_sec=3.63\n",
            "========================================================================================\n",
            "step  19600/32000 | avg_loss=2.6365 | train_ppl=13.9637 | lr=0.00012063 | grad_norm=2.8525\n",
            "interval_sec=15.69 | steps/sec=2.5488 | sec/step=0.3923\n",
            "examples_seen=2,508,800 | approx_epochs=1.6839\n",
            "batch_shapes input=(16, 20) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.355 | chrf=20.109 | comet=nan | text_eval_sec=3.52\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=19600 val_loss=2.5137 | val_ppl=12.3503 | bleu=1.577 | chrf=21.282 | comet=nan | text_eval_sec=28.26\n",
            "[best] new best val_loss=2.5137 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step  19640/32000 | avg_loss=2.6399 | train_ppl=14.0122 | lr=0.000120122 | grad_norm=2.8710\n",
            "interval_sec=50.12 | steps/sec=0.7981 | sec/step=1.2530\n",
            "examples_seen=2,513,920 | approx_epochs=1.6874\n",
            "batch_shapes input=(16, 41) labels=(16, 36)\n",
            "cuda_mem_alloc_gb=0.867 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.955 | chrf=21.983 | comet=nan | text_eval_sec=3.56\n",
            "========================================================================================\n",
            "step  19680/32000 | avg_loss=2.6404 | train_ppl=14.0182 | lr=0.000119615 | grad_norm=3.0595\n",
            "interval_sec=15.67 | steps/sec=2.5520 | sec/step=0.3918\n",
            "examples_seen=2,519,040 | approx_epochs=1.6908\n",
            "batch_shapes input=(16, 23) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.845 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.241 | chrf=19.770 | comet=nan | text_eval_sec=3.61\n",
            "========================================================================================\n",
            "step  19720/32000 | avg_loss=2.5828 | train_ppl=13.2347 | lr=0.000119108 | grad_norm=3.0278\n",
            "interval_sec=15.65 | steps/sec=2.5559 | sec/step=0.3912\n",
            "examples_seen=2,524,160 | approx_epochs=1.6942\n",
            "batch_shapes input=(16, 25) labels=(16, 28)\n",
            "cuda_mem_alloc_gb=0.851 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.484 | chrf=20.166 | comet=nan | text_eval_sec=3.61\n",
            "========================================================================================\n",
            "step  19760/32000 | avg_loss=2.6311 | train_ppl=13.8890 | lr=0.000118603 | grad_norm=3.0074\n",
            "interval_sec=15.59 | steps/sec=2.5655 | sec/step=0.3898\n",
            "examples_seen=2,529,280 | approx_epochs=1.6977\n",
            "batch_shapes input=(16, 17) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.993 | chrf=18.474 | comet=nan | text_eval_sec=3.50\n",
            "========================================================================================\n",
            "step  19800/32000 | avg_loss=2.6143 | train_ppl=13.6580 | lr=0.000118098 | grad_norm=2.8681\n",
            "interval_sec=15.67 | steps/sec=2.5534 | sec/step=0.3916\n",
            "examples_seen=2,534,400 | approx_epochs=1.7011\n",
            "batch_shapes input=(16, 19) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.843 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.613 | chrf=20.815 | comet=nan | text_eval_sec=3.59\n",
            "========================================================================================\n",
            "step  19840/32000 | avg_loss=2.6498 | train_ppl=14.1518 | lr=0.000117593 | grad_norm=3.2049\n",
            "interval_sec=15.80 | steps/sec=2.5316 | sec/step=0.3950\n",
            "examples_seen=2,539,520 | approx_epochs=1.7045\n",
            "batch_shapes input=(16, 14) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.418 | chrf=21.486 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  19880/32000 | avg_loss=2.6268 | train_ppl=13.8289 | lr=0.00011709 | grad_norm=3.0653\n",
            "interval_sec=15.67 | steps/sec=2.5524 | sec/step=0.3918\n",
            "examples_seen=2,544,640 | approx_epochs=1.7080\n",
            "batch_shapes input=(16, 22) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.889 | chrf=21.052 | comet=nan | text_eval_sec=3.77\n",
            "========================================================================================\n",
            "step  19920/32000 | avg_loss=2.6273 | train_ppl=13.8361 | lr=0.000116587 | grad_norm=2.9626\n",
            "interval_sec=15.45 | steps/sec=2.5885 | sec/step=0.3863\n",
            "examples_seen=2,549,760 | approx_epochs=1.7114\n",
            "batch_shapes input=(16, 21) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.170 | chrf=16.438 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step  19960/32000 | avg_loss=2.5784 | train_ppl=13.1757 | lr=0.000116085 | grad_norm=2.7197\n",
            "interval_sec=15.58 | steps/sec=2.5675 | sec/step=0.3895\n",
            "examples_seen=2,554,880 | approx_epochs=1.7149\n",
            "batch_shapes input=(16, 22) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.843 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.739 | chrf=23.282 | comet=nan | text_eval_sec=3.65\n",
            "========================================================================================\n",
            "step  20000/32000 | avg_loss=2.5776 | train_ppl=13.1661 | lr=0.000115584 | grad_norm=3.0256\n",
            "interval_sec=15.80 | steps/sec=2.5324 | sec/step=0.3949\n",
            "examples_seen=2,560,000 | approx_epochs=1.7183\n",
            "batch_shapes input=(16, 44) labels=(16, 51)\n",
            "cuda_mem_alloc_gb=0.896 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.376 | chrf=21.347 | comet=nan | text_eval_sec=3.56\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=20000 val_loss=2.5447 | val_ppl=12.7398 | bleu=1.468 | chrf=20.452 | comet=nan | text_eval_sec=28.06\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_020000\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_015000\n",
            "========================================================================================\n",
            "step  20040/32000 | avg_loss=2.6185 | train_ppl=13.7158 | lr=0.000115083 | grad_norm=2.9795\n",
            "interval_sec=50.03 | steps/sec=0.7995 | sec/step=1.2507\n",
            "examples_seen=2,565,120 | approx_epochs=1.7217\n",
            "batch_shapes input=(16, 75) labels=(16, 71)\n",
            "cuda_mem_alloc_gb=0.935 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.541 | chrf=17.574 | comet=nan | text_eval_sec=3.64\n",
            "========================================================================================\n",
            "step  20080/32000 | avg_loss=2.6023 | train_ppl=13.4944 | lr=0.000114583 | grad_norm=2.8929\n",
            "interval_sec=15.58 | steps/sec=2.5668 | sec/step=0.3896\n",
            "examples_seen=2,570,240 | approx_epochs=1.7252\n",
            "batch_shapes input=(16, 24) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.053 | chrf=19.664 | comet=nan | text_eval_sec=3.69\n",
            "========================================================================================\n",
            "step  20120/32000 | avg_loss=2.6358 | train_ppl=13.9540 | lr=0.000114085 | grad_norm=3.1221\n",
            "interval_sec=16.04 | steps/sec=2.4934 | sec/step=0.4011\n",
            "examples_seen=2,575,360 | approx_epochs=1.7286\n",
            "batch_shapes input=(16, 24) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.846 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.455 | chrf=20.162 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step  20160/32000 | avg_loss=2.6573 | train_ppl=14.2573 | lr=0.000113586 | grad_norm=2.8350\n",
            "interval_sec=15.64 | steps/sec=2.5575 | sec/step=0.3910\n",
            "examples_seen=2,580,480 | approx_epochs=1.7320\n",
            "batch_shapes input=(16, 37) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=2.219 | chrf=24.737 | comet=nan | text_eval_sec=3.67\n",
            "========================================================================================\n",
            "step  20200/32000 | avg_loss=2.5916 | train_ppl=13.3515 | lr=0.000113089 | grad_norm=2.9486\n",
            "interval_sec=15.63 | steps/sec=2.5586 | sec/step=0.3908\n",
            "examples_seen=2,585,600 | approx_epochs=1.7355\n",
            "batch_shapes input=(16, 23) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.924 | chrf=18.899 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step  20240/32000 | avg_loss=2.5566 | train_ppl=12.8917 | lr=0.000112593 | grad_norm=2.8725\n",
            "interval_sec=15.70 | steps/sec=2.5479 | sec/step=0.3925\n",
            "examples_seen=2,590,720 | approx_epochs=1.7389\n",
            "batch_shapes input=(16, 32) labels=(16, 32)\n",
            "cuda_mem_alloc_gb=0.859 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.289 | chrf=19.709 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step  20280/32000 | avg_loss=2.6132 | train_ppl=13.6422 | lr=0.000112097 | grad_norm=3.0689\n",
            "interval_sec=15.83 | steps/sec=2.5263 | sec/step=0.3958\n",
            "examples_seen=2,595,840 | approx_epochs=1.7423\n",
            "batch_shapes input=(16, 19) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.644 | chrf=21.617 | comet=nan | text_eval_sec=3.51\n",
            "========================================================================================\n",
            "step  20320/32000 | avg_loss=2.5971 | train_ppl=13.4254 | lr=0.000111602 | grad_norm=2.9384\n",
            "interval_sec=15.72 | steps/sec=2.5451 | sec/step=0.3929\n",
            "examples_seen=2,600,960 | approx_epochs=1.7458\n",
            "batch_shapes input=(16, 17) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.789 | chrf=24.270 | comet=nan | text_eval_sec=3.61\n",
            "========================================================================================\n",
            "step  20360/32000 | avg_loss=2.6165 | train_ppl=13.6877 | lr=0.000111109 | grad_norm=3.2253\n",
            "interval_sec=15.62 | steps/sec=2.5605 | sec/step=0.3906\n",
            "examples_seen=2,606,080 | approx_epochs=1.7492\n",
            "batch_shapes input=(16, 21) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.305 | chrf=19.966 | comet=nan | text_eval_sec=3.56\n",
            "========================================================================================\n",
            "step  20400/32000 | avg_loss=2.6020 | train_ppl=13.4905 | lr=0.000110616 | grad_norm=2.9988\n",
            "interval_sec=15.67 | steps/sec=2.5527 | sec/step=0.3917\n",
            "examples_seen=2,611,200 | approx_epochs=1.7527\n",
            "batch_shapes input=(16, 26) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.415 | chrf=21.240 | comet=nan | text_eval_sec=3.62\n",
            "[eval] step=20400 val_loss=2.5277 | val_ppl=12.5250 | bleu=1.586 | chrf=21.845 | comet=nan | text_eval_sec=28.19\n",
            "========================================================================================\n",
            "step  20440/32000 | avg_loss=2.6058 | train_ppl=13.5421 | lr=0.000110123 | grad_norm=2.9574\n",
            "interval_sec=49.57 | steps/sec=0.8070 | sec/step=1.2392\n",
            "examples_seen=2,616,320 | approx_epochs=1.7561\n",
            "batch_shapes input=(16, 20) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.408 | chrf=21.306 | comet=nan | text_eval_sec=3.74\n",
            "========================================================================================\n",
            "step  20480/32000 | avg_loss=2.5654 | train_ppl=13.0052 | lr=0.000109632 | grad_norm=2.8869\n",
            "interval_sec=16.19 | steps/sec=2.4707 | sec/step=0.4047\n",
            "examples_seen=2,621,440 | approx_epochs=1.7595\n",
            "batch_shapes input=(16, 34) labels=(16, 37)\n",
            "cuda_mem_alloc_gb=0.869 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.924 | chrf=21.134 | comet=nan | text_eval_sec=3.66\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_020500\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_015500\n",
            "========================================================================================\n",
            "step  20520/32000 | avg_loss=2.6272 | train_ppl=13.8356 | lr=0.000109142 | grad_norm=3.1203\n",
            "interval_sec=16.94 | steps/sec=2.3614 | sec/step=0.4235\n",
            "examples_seen=2,626,560 | approx_epochs=1.7630\n",
            "batch_shapes input=(16, 23) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.629 | chrf=19.801 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step  20560/32000 | avg_loss=2.5586 | train_ppl=12.9180 | lr=0.000108652 | grad_norm=3.2708\n",
            "interval_sec=16.05 | steps/sec=2.4921 | sec/step=0.4013\n",
            "examples_seen=2,631,680 | approx_epochs=1.7664\n",
            "batch_shapes input=(16, 16) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=2.083 | chrf=21.993 | comet=nan | text_eval_sec=3.54\n",
            "========================================================================================\n",
            "step  20600/32000 | avg_loss=2.5680 | train_ppl=13.0403 | lr=0.000108163 | grad_norm=3.0902\n",
            "interval_sec=15.81 | steps/sec=2.5303 | sec/step=0.3952\n",
            "examples_seen=2,636,800 | approx_epochs=1.7698\n",
            "batch_shapes input=(16, 24) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.504 | chrf=25.165 | comet=nan | text_eval_sec=3.54\n",
            "========================================================================================\n",
            "step  20640/32000 | avg_loss=2.5539 | train_ppl=12.8566 | lr=0.000107676 | grad_norm=2.9401\n",
            "interval_sec=15.65 | steps/sec=2.5566 | sec/step=0.3911\n",
            "examples_seen=2,641,920 | approx_epochs=1.7733\n",
            "batch_shapes input=(16, 24) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.845 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.241 | chrf=21.917 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step  20680/32000 | avg_loss=2.6501 | train_ppl=14.1555 | lr=0.000107189 | grad_norm=2.9861\n",
            "interval_sec=16.18 | steps/sec=2.4718 | sec/step=0.4046\n",
            "examples_seen=2,647,040 | approx_epochs=1.7767\n",
            "batch_shapes input=(16, 20) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.438 | chrf=24.691 | comet=nan | text_eval_sec=3.56\n",
            "========================================================================================\n",
            "step  20720/32000 | avg_loss=2.6081 | train_ppl=13.5735 | lr=0.000106703 | grad_norm=3.1037\n",
            "interval_sec=15.86 | steps/sec=2.5218 | sec/step=0.3965\n",
            "examples_seen=2,652,160 | approx_epochs=1.7801\n",
            "batch_shapes input=(16, 24) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.406 | chrf=21.285 | comet=nan | text_eval_sec=3.56\n",
            "========================================================================================\n",
            "step  20760/32000 | avg_loss=2.6362 | train_ppl=13.9600 | lr=0.000106218 | grad_norm=2.9733\n",
            "interval_sec=15.76 | steps/sec=2.5373 | sec/step=0.3941\n",
            "examples_seen=2,657,280 | approx_epochs=1.7836\n",
            "batch_shapes input=(16, 113) labels=(16, 122)\n",
            "cuda_mem_alloc_gb=1.033 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.348 | chrf=19.795 | comet=nan | text_eval_sec=3.66\n",
            "========================================================================================\n",
            "step  20800/32000 | avg_loss=2.5896 | train_ppl=13.3240 | lr=0.000105734 | grad_norm=2.9025\n",
            "interval_sec=15.64 | steps/sec=2.5584 | sec/step=0.3909\n",
            "examples_seen=2,662,400 | approx_epochs=1.7870\n",
            "batch_shapes input=(16, 15) labels=(16, 11)\n",
            "cuda_mem_alloc_gb=0.818 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.343 | chrf=18.552 | comet=nan | text_eval_sec=3.67\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=20800 val_loss=2.4947 | val_ppl=12.1182 | bleu=1.512 | chrf=22.196 | comet=nan | text_eval_sec=28.02\n",
            "[best] new best val_loss=2.4947 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step  20840/32000 | avg_loss=2.5746 | train_ppl=13.1260 | lr=0.000105251 | grad_norm=3.0169\n",
            "interval_sec=49.65 | steps/sec=0.8057 | sec/step=1.2412\n",
            "examples_seen=2,667,520 | approx_epochs=1.7905\n",
            "batch_shapes input=(16, 60) labels=(16, 72)\n",
            "cuda_mem_alloc_gb=0.936 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.501 | chrf=22.632 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step  20880/32000 | avg_loss=2.5811 | train_ppl=13.2113 | lr=0.000104769 | grad_norm=2.8647\n",
            "interval_sec=15.85 | steps/sec=2.5244 | sec/step=0.3961\n",
            "examples_seen=2,672,640 | approx_epochs=1.7939\n",
            "batch_shapes input=(16, 19) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.312 | chrf=19.451 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  20920/32000 | avg_loss=2.5841 | train_ppl=13.2519 | lr=0.000104287 | grad_norm=2.9560\n",
            "interval_sec=15.59 | steps/sec=2.5661 | sec/step=0.3897\n",
            "examples_seen=2,677,760 | approx_epochs=1.7973\n",
            "batch_shapes input=(16, 24) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.478 | chrf=21.282 | comet=nan | text_eval_sec=3.65\n",
            "========================================================================================\n",
            "step  20960/32000 | avg_loss=2.6178 | train_ppl=13.7057 | lr=0.000103807 | grad_norm=3.4391\n",
            "interval_sec=15.69 | steps/sec=2.5501 | sec/step=0.3921\n",
            "examples_seen=2,682,880 | approx_epochs=1.8008\n",
            "batch_shapes input=(16, 24) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.844 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.918 | chrf=19.706 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step  21000/32000 | avg_loss=2.5949 | train_ppl=13.3954 | lr=0.000103328 | grad_norm=2.9212\n",
            "interval_sec=15.74 | steps/sec=2.5407 | sec/step=0.3936\n",
            "examples_seen=2,688,000 | approx_epochs=1.8042\n",
            "batch_shapes input=(16, 13) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.804 | chrf=20.456 | comet=nan | text_eval_sec=3.58\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_021000\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_016000\n",
            "========================================================================================\n",
            "step  21040/32000 | avg_loss=2.6254 | train_ppl=13.8106 | lr=0.00010285 | grad_norm=2.8850\n",
            "interval_sec=16.76 | steps/sec=2.3870 | sec/step=0.4189\n",
            "examples_seen=2,693,120 | approx_epochs=1.8076\n",
            "batch_shapes input=(16, 17) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.453 | chrf=20.045 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  21080/32000 | avg_loss=2.5608 | train_ppl=12.9466 | lr=0.000102372 | grad_norm=2.8154\n",
            "interval_sec=15.91 | steps/sec=2.5136 | sec/step=0.3978\n",
            "examples_seen=2,698,240 | approx_epochs=1.8111\n",
            "batch_shapes input=(16, 29) labels=(16, 26)\n",
            "cuda_mem_alloc_gb=0.848 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.184 | chrf=20.922 | comet=nan | text_eval_sec=3.52\n",
            "========================================================================================\n",
            "step  21120/32000 | avg_loss=2.5403 | train_ppl=12.6839 | lr=0.000101896 | grad_norm=2.9898\n",
            "interval_sec=15.59 | steps/sec=2.5653 | sec/step=0.3898\n",
            "examples_seen=2,703,360 | approx_epochs=1.8145\n",
            "batch_shapes input=(16, 23) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.418 | chrf=20.303 | comet=nan | text_eval_sec=3.55\n",
            "========================================================================================\n",
            "step  21160/32000 | avg_loss=2.5955 | train_ppl=13.4039 | lr=0.000101421 | grad_norm=3.2125\n",
            "interval_sec=15.55 | steps/sec=2.5728 | sec/step=0.3887\n",
            "examples_seen=2,708,480 | approx_epochs=1.8180\n",
            "batch_shapes input=(16, 19) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.985 | chrf=17.729 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  21200/32000 | avg_loss=2.6089 | train_ppl=13.5845 | lr=0.000100946 | grad_norm=2.8291\n",
            "interval_sec=15.64 | steps/sec=2.5583 | sec/step=0.3909\n",
            "examples_seen=2,713,600 | approx_epochs=1.8214\n",
            "batch_shapes input=(16, 32) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.844 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.865 | chrf=18.474 | comet=nan | text_eval_sec=3.55\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=21200 val_loss=2.4574 | val_ppl=11.6743 | bleu=1.475 | chrf=22.246 | comet=nan | text_eval_sec=28.22\n",
            "[best] new best val_loss=2.4574 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step  21240/32000 | avg_loss=2.5624 | train_ppl=12.9672 | lr=0.000100473 | grad_norm=3.2244\n",
            "interval_sec=49.96 | steps/sec=0.8007 | sec/step=1.2489\n",
            "examples_seen=2,718,720 | approx_epochs=1.8248\n",
            "batch_shapes input=(16, 16) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.469 | chrf=18.954 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  21280/32000 | avg_loss=2.6005 | train_ppl=13.4707 | lr=0.000100001 | grad_norm=2.8983\n",
            "interval_sec=15.64 | steps/sec=2.5571 | sec/step=0.3911\n",
            "examples_seen=2,723,840 | approx_epochs=1.8283\n",
            "batch_shapes input=(16, 21) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=2.016 | chrf=24.508 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step  21320/32000 | avg_loss=2.5470 | train_ppl=12.7690 | lr=9.95298e-05 | grad_norm=3.0906\n",
            "interval_sec=15.66 | steps/sec=2.5539 | sec/step=0.3916\n",
            "examples_seen=2,728,960 | approx_epochs=1.8317\n",
            "batch_shapes input=(16, 20) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.262 | chrf=21.609 | comet=nan | text_eval_sec=3.59\n",
            "========================================================================================\n",
            "step  21360/32000 | avg_loss=2.5329 | train_ppl=12.5896 | lr=9.90596e-05 | grad_norm=2.8394\n",
            "interval_sec=15.59 | steps/sec=2.5664 | sec/step=0.3897\n",
            "examples_seen=2,734,080 | approx_epochs=1.8351\n",
            "batch_shapes input=(16, 22) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=2.000 | chrf=28.246 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  21400/32000 | avg_loss=2.5566 | train_ppl=12.8919 | lr=9.85905e-05 | grad_norm=3.2847\n",
            "interval_sec=15.53 | steps/sec=2.5760 | sec/step=0.3882\n",
            "examples_seen=2,739,200 | approx_epochs=1.8386\n",
            "batch_shapes input=(16, 18) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.668 | chrf=19.660 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step  21440/32000 | avg_loss=2.5411 | train_ppl=12.6938 | lr=9.81224e-05 | grad_norm=3.1013\n",
            "interval_sec=15.67 | steps/sec=2.5522 | sec/step=0.3918\n",
            "examples_seen=2,744,320 | approx_epochs=1.8420\n",
            "batch_shapes input=(16, 20) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.501 | chrf=20.574 | comet=nan | text_eval_sec=3.55\n",
            "========================================================================================\n",
            "step  21480/32000 | avg_loss=2.5467 | train_ppl=12.7652 | lr=9.76554e-05 | grad_norm=3.1153\n",
            "interval_sec=15.70 | steps/sec=2.5476 | sec/step=0.3925\n",
            "examples_seen=2,749,440 | approx_epochs=1.8454\n",
            "batch_shapes input=(16, 22) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.313 | chrf=22.049 | comet=nan | text_eval_sec=3.52\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_021500\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_016500\n",
            "========================================================================================\n",
            "step  21520/32000 | avg_loss=2.6065 | train_ppl=13.5509 | lr=9.71895e-05 | grad_norm=3.0584\n",
            "interval_sec=16.39 | steps/sec=2.4407 | sec/step=0.4097\n",
            "examples_seen=2,754,560 | approx_epochs=1.8489\n",
            "batch_shapes input=(16, 28) labels=(16, 26)\n",
            "cuda_mem_alloc_gb=0.847 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.268 | chrf=19.854 | comet=nan | text_eval_sec=5.09\n",
            "========================================================================================\n",
            "step  21560/32000 | avg_loss=2.5503 | train_ppl=12.8108 | lr=9.67246e-05 | grad_norm=3.0354\n",
            "interval_sec=15.60 | steps/sec=2.5645 | sec/step=0.3899\n",
            "examples_seen=2,759,680 | approx_epochs=1.8523\n",
            "batch_shapes input=(16, 25) labels=(16, 27)\n",
            "cuda_mem_alloc_gb=0.849 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.731 | chrf=24.453 | comet=nan | text_eval_sec=3.51\n",
            "========================================================================================\n",
            "step  21600/32000 | avg_loss=2.5416 | train_ppl=12.6995 | lr=9.62609e-05 | grad_norm=2.9976\n",
            "interval_sec=15.55 | steps/sec=2.5717 | sec/step=0.3888\n",
            "examples_seen=2,764,800 | approx_epochs=1.8558\n",
            "batch_shapes input=(16, 19) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.562 | chrf=18.167 | comet=nan | text_eval_sec=3.59\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=21600 val_loss=2.4368 | val_ppl=11.4360 | bleu=1.530 | chrf=20.644 | comet=nan | text_eval_sec=28.27\n",
            "[best] new best val_loss=2.4368 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step  21640/32000 | avg_loss=2.5849 | train_ppl=13.2614 | lr=9.57982e-05 | grad_norm=3.0260\n",
            "interval_sec=50.15 | steps/sec=0.7976 | sec/step=1.2537\n",
            "examples_seen=2,769,920 | approx_epochs=1.8592\n",
            "batch_shapes input=(16, 17) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.238 | chrf=17.901 | comet=nan | text_eval_sec=3.59\n",
            "========================================================================================\n",
            "step  21680/32000 | avg_loss=2.5519 | train_ppl=12.8320 | lr=9.53366e-05 | grad_norm=2.8881\n",
            "interval_sec=15.72 | steps/sec=2.5445 | sec/step=0.3930\n",
            "examples_seen=2,775,040 | approx_epochs=1.8626\n",
            "batch_shapes input=(16, 22) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.918 | chrf=21.097 | comet=nan | text_eval_sec=3.65\n",
            "========================================================================================\n",
            "step  21720/32000 | avg_loss=2.5862 | train_ppl=13.2793 | lr=9.48762e-05 | grad_norm=3.1065\n",
            "interval_sec=15.75 | steps/sec=2.5394 | sec/step=0.3938\n",
            "examples_seen=2,780,160 | approx_epochs=1.8661\n",
            "batch_shapes input=(16, 19) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=2.187 | chrf=25.227 | comet=nan | text_eval_sec=3.63\n",
            "========================================================================================\n",
            "step  21760/32000 | avg_loss=2.5102 | train_ppl=12.3069 | lr=9.44168e-05 | grad_norm=2.9647\n",
            "interval_sec=15.77 | steps/sec=2.5366 | sec/step=0.3942\n",
            "examples_seen=2,785,280 | approx_epochs=1.8695\n",
            "batch_shapes input=(16, 20) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.735 | chrf=21.561 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step  21800/32000 | avg_loss=2.5652 | train_ppl=13.0030 | lr=9.39586e-05 | grad_norm=3.0914\n",
            "interval_sec=15.82 | steps/sec=2.5285 | sec/step=0.3955\n",
            "examples_seen=2,790,400 | approx_epochs=1.8729\n",
            "batch_shapes input=(16, 18) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.268 | chrf=21.174 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  21840/32000 | avg_loss=2.5840 | train_ppl=13.2504 | lr=9.35015e-05 | grad_norm=3.1898\n",
            "interval_sec=15.53 | steps/sec=2.5751 | sec/step=0.3883\n",
            "examples_seen=2,795,520 | approx_epochs=1.8764\n",
            "batch_shapes input=(16, 24) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.241 | chrf=19.720 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step  21880/32000 | avg_loss=2.5462 | train_ppl=12.7589 | lr=9.30455e-05 | grad_norm=3.0863\n",
            "interval_sec=15.66 | steps/sec=2.5538 | sec/step=0.3916\n",
            "examples_seen=2,800,640 | approx_epochs=1.8798\n",
            "batch_shapes input=(16, 46) labels=(16, 36)\n",
            "cuda_mem_alloc_gb=0.867 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.408 | chrf=18.798 | comet=nan | text_eval_sec=3.61\n",
            "========================================================================================\n",
            "step  21920/32000 | avg_loss=2.5109 | train_ppl=12.3156 | lr=9.25907e-05 | grad_norm=2.7650\n",
            "interval_sec=16.41 | steps/sec=2.4378 | sec/step=0.4102\n",
            "examples_seen=2,805,760 | approx_epochs=1.8832\n",
            "batch_shapes input=(16, 13) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.281 | chrf=22.268 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  21960/32000 | avg_loss=2.5583 | train_ppl=12.9142 | lr=9.2137e-05 | grad_norm=3.1209\n",
            "interval_sec=15.80 | steps/sec=2.5319 | sec/step=0.3950\n",
            "examples_seen=2,810,880 | approx_epochs=1.8867\n",
            "batch_shapes input=(16, 51) labels=(16, 40)\n",
            "cuda_mem_alloc_gb=0.875 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.698 | chrf=24.839 | comet=nan | text_eval_sec=3.72\n",
            "========================================================================================\n",
            "step  22000/32000 | avg_loss=2.5498 | train_ppl=12.8051 | lr=9.16845e-05 | grad_norm=2.9470\n",
            "interval_sec=15.75 | steps/sec=2.5390 | sec/step=0.3939\n",
            "examples_seen=2,816,000 | approx_epochs=1.8901\n",
            "batch_shapes input=(16, 23) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.357 | chrf=21.936 | comet=nan | text_eval_sec=3.60\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=22000 val_loss=2.4111 | val_ppl=11.1459 | bleu=1.376 | chrf=22.139 | comet=nan | text_eval_sec=28.16\n",
            "[best] new best val_loss=2.4111 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_022000\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_017000\n",
            "========================================================================================\n",
            "step  22040/32000 | avg_loss=2.5459 | train_ppl=12.7546 | lr=9.12332e-05 | grad_norm=3.0379\n",
            "interval_sec=55.50 | steps/sec=0.7207 | sec/step=1.3875\n",
            "examples_seen=2,821,120 | approx_epochs=1.8936\n",
            "batch_shapes input=(16, 23) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.628 | chrf=20.479 | comet=nan | text_eval_sec=5.73\n",
            "========================================================================================\n",
            "step  22080/32000 | avg_loss=2.5641 | train_ppl=12.9892 | lr=9.0783e-05 | grad_norm=2.9749\n",
            "interval_sec=15.67 | steps/sec=2.5526 | sec/step=0.3918\n",
            "examples_seen=2,826,240 | approx_epochs=1.8970\n",
            "batch_shapes input=(16, 27) labels=(16, 28)\n",
            "cuda_mem_alloc_gb=0.851 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.507 | chrf=21.209 | comet=nan | text_eval_sec=3.56\n",
            "========================================================================================\n",
            "step  22120/32000 | avg_loss=2.5417 | train_ppl=12.7010 | lr=9.0334e-05 | grad_norm=2.9173\n",
            "interval_sec=15.80 | steps/sec=2.5313 | sec/step=0.3951\n",
            "examples_seen=2,831,360 | approx_epochs=1.9004\n",
            "batch_shapes input=(16, 18) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.624 | chrf=22.741 | comet=nan | text_eval_sec=3.63\n",
            "========================================================================================\n",
            "step  22160/32000 | avg_loss=2.5793 | train_ppl=13.1885 | lr=8.98862e-05 | grad_norm=2.9839\n",
            "interval_sec=15.74 | steps/sec=2.5420 | sec/step=0.3934\n",
            "examples_seen=2,836,480 | approx_epochs=1.9039\n",
            "batch_shapes input=(16, 20) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.323 | chrf=19.621 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step  22200/32000 | avg_loss=2.5447 | train_ppl=12.7400 | lr=8.94396e-05 | grad_norm=3.6362\n",
            "interval_sec=15.60 | steps/sec=2.5638 | sec/step=0.3901\n",
            "examples_seen=2,841,600 | approx_epochs=1.9073\n",
            "batch_shapes input=(16, 21) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.721 | chrf=25.125 | comet=nan | text_eval_sec=3.56\n",
            "========================================================================================\n",
            "step  22240/32000 | avg_loss=2.5064 | train_ppl=12.2610 | lr=8.89941e-05 | grad_norm=3.3041\n",
            "interval_sec=15.64 | steps/sec=2.5581 | sec/step=0.3909\n",
            "examples_seen=2,846,720 | approx_epochs=1.9107\n",
            "batch_shapes input=(16, 27) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.769 | chrf=27.281 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step  22280/32000 | avg_loss=2.5046 | train_ppl=12.2386 | lr=8.85499e-05 | grad_norm=2.9932\n",
            "interval_sec=15.76 | steps/sec=2.5373 | sec/step=0.3941\n",
            "examples_seen=2,851,840 | approx_epochs=1.9142\n",
            "batch_shapes input=(16, 22) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.281 | chrf=20.308 | comet=nan | text_eval_sec=3.56\n",
            "========================================================================================\n",
            "step  22320/32000 | avg_loss=2.5326 | train_ppl=12.5867 | lr=8.8107e-05 | grad_norm=3.0587\n",
            "interval_sec=15.69 | steps/sec=2.5502 | sec/step=0.3921\n",
            "examples_seen=2,856,960 | approx_epochs=1.9176\n",
            "batch_shapes input=(16, 22) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.867 | chrf=22.736 | comet=nan | text_eval_sec=3.53\n",
            "========================================================================================\n",
            "step  22360/32000 | avg_loss=2.5082 | train_ppl=12.2822 | lr=8.76652e-05 | grad_norm=2.9032\n",
            "interval_sec=15.75 | steps/sec=2.5392 | sec/step=0.3938\n",
            "examples_seen=2,862,080 | approx_epochs=1.9210\n",
            "batch_shapes input=(16, 16) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.729 | chrf=19.429 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step  22400/32000 | avg_loss=2.4907 | train_ppl=12.0696 | lr=8.72246e-05 | grad_norm=3.1859\n",
            "interval_sec=15.68 | steps/sec=2.5503 | sec/step=0.3921\n",
            "examples_seen=2,867,200 | approx_epochs=1.9245\n",
            "batch_shapes input=(16, 17) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.373 | chrf=24.265 | comet=nan | text_eval_sec=3.60\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=22400 val_loss=2.4039 | val_ppl=11.0663 | bleu=1.678 | chrf=21.663 | comet=nan | text_eval_sec=28.26\n",
            "[best] new best val_loss=2.4039 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step  22440/32000 | avg_loss=2.4828 | train_ppl=11.9752 | lr=8.67853e-05 | grad_norm=2.8613\n",
            "interval_sec=50.16 | steps/sec=0.7975 | sec/step=1.2539\n",
            "examples_seen=2,872,320 | approx_epochs=1.9279\n",
            "batch_shapes input=(16, 21) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.253 | chrf=22.360 | comet=nan | text_eval_sec=3.63\n",
            "========================================================================================\n",
            "step  22480/32000 | avg_loss=2.5445 | train_ppl=12.7369 | lr=8.63473e-05 | grad_norm=3.0722\n",
            "interval_sec=15.67 | steps/sec=2.5521 | sec/step=0.3918\n",
            "examples_seen=2,877,440 | approx_epochs=1.9314\n",
            "batch_shapes input=(16, 19) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.841 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.981 | chrf=22.277 | comet=nan | text_eval_sec=3.62\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_022500\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_017500\n",
            "========================================================================================\n",
            "step  22520/32000 | avg_loss=2.5648 | train_ppl=12.9987 | lr=8.59105e-05 | grad_norm=3.0218\n",
            "interval_sec=16.38 | steps/sec=2.4414 | sec/step=0.4096\n",
            "examples_seen=2,882,560 | approx_epochs=1.9348\n",
            "batch_shapes input=(16, 18) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.202 | chrf=21.870 | comet=nan | text_eval_sec=3.59\n",
            "========================================================================================\n",
            "step  22560/32000 | avg_loss=2.5118 | train_ppl=12.3270 | lr=8.54749e-05 | grad_norm=2.9438\n",
            "interval_sec=15.70 | steps/sec=2.5481 | sec/step=0.3924\n",
            "examples_seen=2,887,680 | approx_epochs=1.9382\n",
            "batch_shapes input=(16, 32) labels=(16, 36)\n",
            "cuda_mem_alloc_gb=0.867 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.646 | chrf=21.515 | comet=nan | text_eval_sec=3.65\n",
            "========================================================================================\n",
            "step  22600/32000 | avg_loss=2.5374 | train_ppl=12.6463 | lr=8.50406e-05 | grad_norm=2.9001\n",
            "interval_sec=15.47 | steps/sec=2.5855 | sec/step=0.3868\n",
            "examples_seen=2,892,800 | approx_epochs=1.9417\n",
            "batch_shapes input=(16, 26) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.280 | chrf=19.709 | comet=nan | text_eval_sec=3.51\n",
            "========================================================================================\n",
            "step  22640/32000 | avg_loss=2.5015 | train_ppl=12.2007 | lr=8.46076e-05 | grad_norm=2.9026\n",
            "interval_sec=15.72 | steps/sec=2.5439 | sec/step=0.3931\n",
            "examples_seen=2,897,920 | approx_epochs=1.9451\n",
            "batch_shapes input=(16, 23) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.527 | chrf=21.444 | comet=nan | text_eval_sec=3.61\n",
            "========================================================================================\n",
            "step  22680/32000 | avg_loss=2.5894 | train_ppl=13.3212 | lr=8.41759e-05 | grad_norm=3.3697\n",
            "interval_sec=15.84 | steps/sec=2.5247 | sec/step=0.3961\n",
            "examples_seen=2,903,040 | approx_epochs=1.9485\n",
            "batch_shapes input=(16, 23) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.792 | chrf=21.743 | comet=nan | text_eval_sec=3.67\n",
            "========================================================================================\n",
            "step  22720/32000 | avg_loss=2.5304 | train_ppl=12.5584 | lr=8.37454e-05 | grad_norm=2.9046\n",
            "interval_sec=15.58 | steps/sec=2.5666 | sec/step=0.3896\n",
            "examples_seen=2,908,160 | approx_epochs=1.9520\n",
            "batch_shapes input=(16, 24) labels=(16, 27)\n",
            "cuda_mem_alloc_gb=0.849 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.791 | chrf=22.765 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step  22760/32000 | avg_loss=2.5020 | train_ppl=12.2068 | lr=8.33163e-05 | grad_norm=3.1477\n",
            "interval_sec=15.65 | steps/sec=2.5565 | sec/step=0.3912\n",
            "examples_seen=2,913,280 | approx_epochs=1.9554\n",
            "batch_shapes input=(16, 19) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.971 | chrf=26.009 | comet=nan | text_eval_sec=3.51\n",
            "========================================================================================\n",
            "step  22800/32000 | avg_loss=2.5228 | train_ppl=12.4640 | lr=8.28884e-05 | grad_norm=2.7968\n",
            "interval_sec=15.65 | steps/sec=2.5565 | sec/step=0.3912\n",
            "examples_seen=2,918,400 | approx_epochs=1.9589\n",
            "batch_shapes input=(16, 17) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.809 | chrf=26.317 | comet=nan | text_eval_sec=3.50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=22800 val_loss=2.4063 | val_ppl=11.0926 | bleu=1.656 | chrf=21.604 | comet=nan | text_eval_sec=27.98\n",
            "========================================================================================\n",
            "step  22840/32000 | avg_loss=2.5532 | train_ppl=12.8482 | lr=8.24618e-05 | grad_norm=3.2765\n",
            "interval_sec=49.23 | steps/sec=0.8125 | sec/step=1.2308\n",
            "examples_seen=2,923,520 | approx_epochs=1.9623\n",
            "batch_shapes input=(16, 15) labels=(16, 14)\n",
            "cuda_mem_alloc_gb=0.824 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.527 | chrf=18.339 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step  22880/32000 | avg_loss=2.5392 | train_ppl=12.6692 | lr=8.20366e-05 | grad_norm=3.0768\n",
            "interval_sec=15.72 | steps/sec=2.5445 | sec/step=0.3930\n",
            "examples_seen=2,928,640 | approx_epochs=1.9657\n",
            "batch_shapes input=(16, 19) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.359 | chrf=20.298 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step  22920/32000 | avg_loss=2.5228 | train_ppl=12.4635 | lr=8.16127e-05 | grad_norm=2.9928\n",
            "interval_sec=15.72 | steps/sec=2.5440 | sec/step=0.3931\n",
            "examples_seen=2,933,760 | approx_epochs=1.9692\n",
            "batch_shapes input=(16, 22) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.620 | chrf=21.181 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step  22960/32000 | avg_loss=2.5514 | train_ppl=12.8248 | lr=8.119e-05 | grad_norm=2.9242\n",
            "interval_sec=15.66 | steps/sec=2.5543 | sec/step=0.3915\n",
            "examples_seen=2,938,880 | approx_epochs=1.9726\n",
            "batch_shapes input=(16, 22) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.487 | chrf=22.422 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step  23000/32000 | avg_loss=2.4944 | train_ppl=12.1145 | lr=8.07688e-05 | grad_norm=3.0407\n",
            "interval_sec=15.78 | steps/sec=2.5354 | sec/step=0.3944\n",
            "examples_seen=2,944,000 | approx_epochs=1.9760\n",
            "batch_shapes input=(16, 25) labels=(16, 26)\n",
            "cuda_mem_alloc_gb=0.847 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.888 | chrf=23.195 | comet=nan | text_eval_sec=3.49\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_023000\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_018000\n",
            "========================================================================================\n",
            "step  23040/32000 | avg_loss=2.4692 | train_ppl=11.8133 | lr=8.03488e-05 | grad_norm=3.1530\n",
            "interval_sec=16.43 | steps/sec=2.4346 | sec/step=0.4107\n",
            "examples_seen=2,949,120 | approx_epochs=1.9795\n",
            "batch_shapes input=(16, 20) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.385 | chrf=20.260 | comet=nan | text_eval_sec=3.48\n",
            "========================================================================================\n",
            "step  23080/32000 | avg_loss=2.4764 | train_ppl=11.8978 | lr=7.99303e-05 | grad_norm=3.1368\n",
            "interval_sec=15.65 | steps/sec=2.5556 | sec/step=0.3913\n",
            "examples_seen=2,954,240 | approx_epochs=1.9829\n",
            "batch_shapes input=(16, 24) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.480 | chrf=23.398 | comet=nan | text_eval_sec=3.52\n",
            "========================================================================================\n",
            "step  23120/32000 | avg_loss=2.4969 | train_ppl=12.1447 | lr=7.9513e-05 | grad_norm=2.9603\n",
            "interval_sec=15.54 | steps/sec=2.5738 | sec/step=0.3885\n",
            "examples_seen=2,959,360 | approx_epochs=1.9863\n",
            "batch_shapes input=(16, 22) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.276 | chrf=19.319 | comet=nan | text_eval_sec=3.65\n",
            "========================================================================================\n",
            "step  23160/32000 | avg_loss=2.4933 | train_ppl=12.1012 | lr=7.90971e-05 | grad_norm=3.0152\n",
            "interval_sec=15.82 | steps/sec=2.5290 | sec/step=0.3954\n",
            "examples_seen=2,964,480 | approx_epochs=1.9898\n",
            "batch_shapes input=(16, 20) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.406 | chrf=17.064 | comet=nan | text_eval_sec=3.73\n",
            "========================================================================================\n",
            "step  23200/32000 | avg_loss=2.5050 | train_ppl=12.2438 | lr=7.86826e-05 | grad_norm=3.0323\n",
            "interval_sec=15.73 | steps/sec=2.5424 | sec/step=0.3933\n",
            "examples_seen=2,969,600 | approx_epochs=1.9932\n",
            "batch_shapes input=(16, 23) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.844 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.283 | chrf=18.333 | comet=nan | text_eval_sec=3.71\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=23200 val_loss=2.3892 | val_ppl=10.9045 | bleu=1.562 | chrf=20.285 | comet=nan | text_eval_sec=28.01\n",
            "[best] new best val_loss=2.3892 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step  23240/32000 | avg_loss=2.5234 | train_ppl=12.4708 | lr=7.82695e-05 | grad_norm=3.1301\n",
            "interval_sec=49.84 | steps/sec=0.8025 | sec/step=1.2461\n",
            "examples_seen=2,974,720 | approx_epochs=1.9967\n",
            "batch_shapes input=(16, 22) labels=(16, 13)\n",
            "cuda_mem_alloc_gb=0.822 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.672 | chrf=20.995 | comet=nan | text_eval_sec=3.81\n",
            "========================================================================================\n",
            "step  23280/32000 | avg_loss=2.5039 | train_ppl=12.2296 | lr=7.78577e-05 | grad_norm=2.7718\n",
            "interval_sec=15.64 | steps/sec=2.5582 | sec/step=0.3909\n",
            "examples_seen=2,979,840 | approx_epochs=2.0001\n",
            "batch_shapes input=(16, 19) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.202 | chrf=18.042 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step  23320/32000 | avg_loss=2.3048 | train_ppl=10.0221 | lr=7.74474e-05 | grad_norm=2.5846\n",
            "interval_sec=15.63 | steps/sec=2.5589 | sec/step=0.3908\n",
            "examples_seen=2,984,960 | approx_epochs=2.0035\n",
            "batch_shapes input=(16, 22) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.139 | chrf=19.512 | comet=nan | text_eval_sec=3.64\n",
            "========================================================================================\n",
            "step  23360/32000 | avg_loss=2.3311 | train_ppl=10.2891 | lr=7.70384e-05 | grad_norm=2.8711\n",
            "interval_sec=15.65 | steps/sec=2.5567 | sec/step=0.3911\n",
            "examples_seen=2,990,080 | approx_epochs=2.0070\n",
            "batch_shapes input=(16, 22) labels=(16, 27)\n",
            "cuda_mem_alloc_gb=0.849 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.144 | chrf=21.240 | comet=nan | text_eval_sec=3.63\n",
            "========================================================================================\n",
            "step  23400/32000 | avg_loss=2.3287 | train_ppl=10.2646 | lr=7.66308e-05 | grad_norm=3.0963\n",
            "interval_sec=15.68 | steps/sec=2.5509 | sec/step=0.3920\n",
            "examples_seen=2,995,200 | approx_epochs=2.0104\n",
            "batch_shapes input=(16, 29) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.594 | chrf=19.956 | comet=nan | text_eval_sec=3.63\n",
            "========================================================================================\n",
            "step  23440/32000 | avg_loss=2.3270 | train_ppl=10.2472 | lr=7.62246e-05 | grad_norm=2.9365\n",
            "interval_sec=15.67 | steps/sec=2.5519 | sec/step=0.3919\n",
            "examples_seen=3,000,320 | approx_epochs=2.0138\n",
            "batch_shapes input=(16, 22) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.254 | chrf=19.077 | comet=nan | text_eval_sec=3.67\n",
            "========================================================================================\n",
            "step  23480/32000 | avg_loss=2.2944 | train_ppl=9.9186 | lr=7.58198e-05 | grad_norm=2.9766\n",
            "interval_sec=15.96 | steps/sec=2.5057 | sec/step=0.3991\n",
            "examples_seen=3,005,440 | approx_epochs=2.0173\n",
            "batch_shapes input=(16, 21) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.449 | chrf=21.263 | comet=nan | text_eval_sec=3.82\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_023500\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_018500\n",
            "========================================================================================\n",
            "step  23520/32000 | avg_loss=2.3398 | train_ppl=10.3790 | lr=7.54165e-05 | grad_norm=3.1315\n",
            "interval_sec=17.16 | steps/sec=2.3315 | sec/step=0.4289\n",
            "examples_seen=3,010,560 | approx_epochs=2.0207\n",
            "batch_shapes input=(16, 47) labels=(16, 44)\n",
            "cuda_mem_alloc_gb=0.882 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.339 | chrf=21.247 | comet=nan | text_eval_sec=3.72\n",
            "========================================================================================\n",
            "step  23560/32000 | avg_loss=2.2987 | train_ppl=9.9610 | lr=7.50146e-05 | grad_norm=2.7805\n",
            "interval_sec=15.76 | steps/sec=2.5387 | sec/step=0.3939\n",
            "examples_seen=3,015,680 | approx_epochs=2.0241\n",
            "batch_shapes input=(16, 23) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=2.227 | chrf=24.745 | comet=nan | text_eval_sec=3.63\n",
            "========================================================================================\n",
            "step  23600/32000 | avg_loss=2.3153 | train_ppl=10.1281 | lr=7.46141e-05 | grad_norm=3.0907\n",
            "interval_sec=15.59 | steps/sec=2.5654 | sec/step=0.3898\n",
            "examples_seen=3,020,800 | approx_epochs=2.0276\n",
            "batch_shapes input=(16, 58) labels=(16, 54)\n",
            "cuda_mem_alloc_gb=0.902 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.604 | chrf=22.630 | comet=nan | text_eval_sec=3.60\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=23600 val_loss=2.3467 | val_ppl=10.4511 | bleu=1.712 | chrf=22.258 | comet=nan | text_eval_sec=27.97\n",
            "[best] new best val_loss=2.3467 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step  23640/32000 | avg_loss=2.2964 | train_ppl=9.9387 | lr=7.4215e-05 | grad_norm=2.9590\n",
            "interval_sec=49.84 | steps/sec=0.8025 | sec/step=1.2461\n",
            "examples_seen=3,025,920 | approx_epochs=2.0310\n",
            "batch_shapes input=(16, 26) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.812 | chrf=22.520 | comet=nan | text_eval_sec=3.65\n",
            "========================================================================================\n",
            "step  23680/32000 | avg_loss=2.3529 | train_ppl=10.5163 | lr=7.38174e-05 | grad_norm=2.7582\n",
            "interval_sec=15.67 | steps/sec=2.5534 | sec/step=0.3916\n",
            "examples_seen=3,031,040 | approx_epochs=2.0345\n",
            "batch_shapes input=(16, 18) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.526 | chrf=21.972 | comet=nan | text_eval_sec=3.61\n",
            "========================================================================================\n",
            "step  23720/32000 | avg_loss=2.3135 | train_ppl=10.1100 | lr=7.34212e-05 | grad_norm=2.8522\n",
            "interval_sec=15.74 | steps/sec=2.5416 | sec/step=0.3935\n",
            "examples_seen=3,036,160 | approx_epochs=2.0379\n",
            "batch_shapes input=(16, 25) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.844 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.589 | chrf=21.226 | comet=nan | text_eval_sec=3.71\n",
            "========================================================================================\n",
            "step  23760/32000 | avg_loss=2.3537 | train_ppl=10.5247 | lr=7.30265e-05 | grad_norm=2.7789\n",
            "interval_sec=15.65 | steps/sec=2.5557 | sec/step=0.3913\n",
            "examples_seen=3,041,280 | approx_epochs=2.0413\n",
            "batch_shapes input=(16, 15) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.790 | chrf=21.525 | comet=nan | text_eval_sec=3.61\n",
            "========================================================================================\n",
            "step  23800/32000 | avg_loss=2.3205 | train_ppl=10.1809 | lr=7.26332e-05 | grad_norm=2.8955\n",
            "interval_sec=15.44 | steps/sec=2.5906 | sec/step=0.3860\n",
            "examples_seen=3,046,400 | approx_epochs=2.0448\n",
            "batch_shapes input=(16, 23) labels=(16, 26)\n",
            "cuda_mem_alloc_gb=0.847 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.758 | chrf=23.654 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step  23840/32000 | avg_loss=2.3443 | train_ppl=10.4258 | lr=7.22414e-05 | grad_norm=2.9604\n",
            "interval_sec=15.68 | steps/sec=2.5503 | sec/step=0.3921\n",
            "examples_seen=3,051,520 | approx_epochs=2.0482\n",
            "batch_shapes input=(16, 17) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.336 | chrf=19.451 | comet=nan | text_eval_sec=3.66\n",
            "========================================================================================\n",
            "step  23880/32000 | avg_loss=2.3332 | train_ppl=10.3105 | lr=7.18511e-05 | grad_norm=2.7444\n",
            "interval_sec=15.67 | steps/sec=2.5522 | sec/step=0.3918\n",
            "examples_seen=3,056,640 | approx_epochs=2.0516\n",
            "batch_shapes input=(16, 31) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=2.044 | chrf=28.891 | comet=nan | text_eval_sec=3.59\n",
            "========================================================================================\n",
            "step  23920/32000 | avg_loss=2.3407 | train_ppl=10.3889 | lr=7.14623e-05 | grad_norm=2.9370\n",
            "interval_sec=15.67 | steps/sec=2.5532 | sec/step=0.3917\n",
            "examples_seen=3,061,760 | approx_epochs=2.0551\n",
            "batch_shapes input=(16, 60) labels=(16, 62)\n",
            "cuda_mem_alloc_gb=0.917 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.580 | chrf=23.312 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  23960/32000 | avg_loss=2.3289 | train_ppl=10.2664 | lr=7.1075e-05 | grad_norm=3.0677\n",
            "interval_sec=15.64 | steps/sec=2.5569 | sec/step=0.3911\n",
            "examples_seen=3,066,880 | approx_epochs=2.0585\n",
            "batch_shapes input=(16, 32) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.857 | chrf=24.768 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  24000/32000 | avg_loss=2.3021 | train_ppl=9.9949 | lr=7.06891e-05 | grad_norm=2.8121\n",
            "interval_sec=15.52 | steps/sec=2.5768 | sec/step=0.3881\n",
            "examples_seen=3,072,000 | approx_epochs=2.0619\n",
            "batch_shapes input=(16, 15) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.655 | chrf=24.739 | comet=nan | text_eval_sec=3.61\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=24000 val_loss=2.3777 | val_ppl=10.7802 | bleu=1.656 | chrf=23.877 | comet=nan | text_eval_sec=27.95\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_024000\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_019000\n",
            "========================================================================================\n",
            "step  24040/32000 | avg_loss=2.3564 | train_ppl=10.5525 | lr=7.03048e-05 | grad_norm=2.7478\n",
            "interval_sec=52.37 | steps/sec=0.7638 | sec/step=1.3093\n",
            "examples_seen=3,077,120 | approx_epochs=2.0654\n",
            "batch_shapes input=(16, 14) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.212 | chrf=20.849 | comet=nan | text_eval_sec=3.66\n",
            "========================================================================================\n",
            "step  24080/32000 | avg_loss=2.2971 | train_ppl=9.9454 | lr=6.99219e-05 | grad_norm=3.0899\n",
            "interval_sec=15.69 | steps/sec=2.5499 | sec/step=0.3922\n",
            "examples_seen=3,082,240 | approx_epochs=2.0688\n",
            "batch_shapes input=(16, 15) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=2.084 | chrf=25.491 | comet=nan | text_eval_sec=3.56\n",
            "========================================================================================\n",
            "step  24120/32000 | avg_loss=2.2910 | train_ppl=9.8849 | lr=6.95406e-05 | grad_norm=3.3199\n",
            "interval_sec=15.70 | steps/sec=2.5472 | sec/step=0.3926\n",
            "examples_seen=3,087,360 | approx_epochs=2.0723\n",
            "batch_shapes input=(16, 19) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.841 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.297 | chrf=20.215 | comet=nan | text_eval_sec=3.59\n",
            "========================================================================================\n",
            "step  24160/32000 | avg_loss=2.2831 | train_ppl=9.8071 | lr=6.91608e-05 | grad_norm=2.8028\n",
            "interval_sec=15.66 | steps/sec=2.5536 | sec/step=0.3916\n",
            "examples_seen=3,092,480 | approx_epochs=2.0757\n",
            "batch_shapes input=(16, 32) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.844 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.574 | chrf=22.176 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step  24200/32000 | avg_loss=2.3242 | train_ppl=10.2180 | lr=6.87825e-05 | grad_norm=3.0337\n",
            "interval_sec=15.63 | steps/sec=2.5590 | sec/step=0.3908\n",
            "examples_seen=3,097,600 | approx_epochs=2.0791\n",
            "batch_shapes input=(16, 21) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=2.078 | chrf=23.297 | comet=nan | text_eval_sec=3.70\n",
            "========================================================================================\n",
            "step  24240/32000 | avg_loss=2.3558 | train_ppl=10.5466 | lr=6.84057e-05 | grad_norm=2.9030\n",
            "interval_sec=15.65 | steps/sec=2.5557 | sec/step=0.3913\n",
            "examples_seen=3,102,720 | approx_epochs=2.0826\n",
            "batch_shapes input=(16, 19) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.396 | chrf=22.571 | comet=nan | text_eval_sec=3.55\n",
            "========================================================================================\n",
            "step  24280/32000 | avg_loss=2.3233 | train_ppl=10.2090 | lr=6.80305e-05 | grad_norm=2.9680\n",
            "interval_sec=15.70 | steps/sec=2.5473 | sec/step=0.3926\n",
            "examples_seen=3,107,840 | approx_epochs=2.0860\n",
            "batch_shapes input=(16, 20) labels=(16, 28)\n",
            "cuda_mem_alloc_gb=0.851 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.276 | chrf=23.652 | comet=nan | text_eval_sec=3.61\n",
            "========================================================================================\n",
            "step  24320/32000 | avg_loss=2.3467 | train_ppl=10.4509 | lr=6.76568e-05 | grad_norm=2.7734\n",
            "interval_sec=15.82 | steps/sec=2.5291 | sec/step=0.3954\n",
            "examples_seen=3,112,960 | approx_epochs=2.0894\n",
            "batch_shapes input=(16, 27) labels=(16, 29)\n",
            "cuda_mem_alloc_gb=0.853 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.391 | chrf=19.401 | comet=nan | text_eval_sec=3.55\n",
            "========================================================================================\n",
            "step  24360/32000 | avg_loss=2.3306 | train_ppl=10.2836 | lr=6.72847e-05 | grad_norm=2.7941\n",
            "interval_sec=15.76 | steps/sec=2.5387 | sec/step=0.3939\n",
            "examples_seen=3,118,080 | approx_epochs=2.0929\n",
            "batch_shapes input=(16, 28) labels=(16, 28)\n",
            "cuda_mem_alloc_gb=0.851 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=2.038 | chrf=24.783 | comet=nan | text_eval_sec=3.59\n",
            "========================================================================================\n",
            "step  24400/32000 | avg_loss=2.3028 | train_ppl=10.0022 | lr=6.69141e-05 | grad_norm=2.8433\n",
            "interval_sec=15.66 | steps/sec=2.5546 | sec/step=0.3915\n",
            "examples_seen=3,123,200 | approx_epochs=2.0963\n",
            "batch_shapes input=(16, 23) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.608 | chrf=20.605 | comet=nan | text_eval_sec=3.63\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=24400 val_loss=2.4058 | val_ppl=11.0877 | bleu=1.735 | chrf=21.748 | comet=nan | text_eval_sec=28.07\n",
            "========================================================================================\n",
            "step  24440/32000 | avg_loss=2.3533 | train_ppl=10.5205 | lr=6.6545e-05 | grad_norm=2.9774\n",
            "interval_sec=49.27 | steps/sec=0.8119 | sec/step=1.2317\n",
            "examples_seen=3,128,320 | approx_epochs=2.0998\n",
            "batch_shapes input=(16, 21) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.844 | chrf=22.653 | comet=nan | text_eval_sec=3.54\n",
            "========================================================================================\n",
            "step  24480/32000 | avg_loss=2.3514 | train_ppl=10.4999 | lr=6.61776e-05 | grad_norm=3.3226\n",
            "interval_sec=15.72 | steps/sec=2.5439 | sec/step=0.3931\n",
            "examples_seen=3,133,440 | approx_epochs=2.1032\n",
            "batch_shapes input=(16, 20) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.797 | chrf=23.035 | comet=nan | text_eval_sec=3.67\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_024500\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_019500\n",
            "========================================================================================\n",
            "step  24520/32000 | avg_loss=2.3108 | train_ppl=10.0825 | lr=6.58117e-05 | grad_norm=3.1845\n",
            "interval_sec=16.79 | steps/sec=2.3818 | sec/step=0.4199\n",
            "examples_seen=3,138,560 | approx_epochs=2.1066\n",
            "batch_shapes input=(16, 18) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.344 | chrf=21.584 | comet=nan | text_eval_sec=3.79\n",
            "========================================================================================\n",
            "step  24560/32000 | avg_loss=2.3374 | train_ppl=10.3538 | lr=6.54474e-05 | grad_norm=2.9482\n",
            "interval_sec=15.95 | steps/sec=2.5073 | sec/step=0.3988\n",
            "examples_seen=3,143,680 | approx_epochs=2.1101\n",
            "batch_shapes input=(16, 36) labels=(16, 41)\n",
            "cuda_mem_alloc_gb=0.876 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.304 | chrf=18.898 | comet=nan | text_eval_sec=3.72\n",
            "========================================================================================\n",
            "step  24600/32000 | avg_loss=2.3516 | train_ppl=10.5023 | lr=6.50846e-05 | grad_norm=3.0943\n",
            "interval_sec=15.92 | steps/sec=2.5131 | sec/step=0.3979\n",
            "examples_seen=3,148,800 | approx_epochs=2.1135\n",
            "batch_shapes input=(16, 25) labels=(16, 29)\n",
            "cuda_mem_alloc_gb=0.853 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.132 | chrf=23.283 | comet=nan | text_eval_sec=3.59\n",
            "========================================================================================\n",
            "step  24640/32000 | avg_loss=2.3315 | train_ppl=10.2932 | lr=6.47235e-05 | grad_norm=2.7818\n",
            "interval_sec=16.08 | steps/sec=2.4882 | sec/step=0.4019\n",
            "examples_seen=3,153,920 | approx_epochs=2.1169\n",
            "batch_shapes input=(16, 17) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.944 | chrf=25.960 | comet=nan | text_eval_sec=3.64\n",
            "========================================================================================\n",
            "step  24680/32000 | avg_loss=2.2817 | train_ppl=9.7936 | lr=6.43639e-05 | grad_norm=3.1649\n",
            "interval_sec=15.75 | steps/sec=2.5401 | sec/step=0.3937\n",
            "examples_seen=3,159,040 | approx_epochs=2.1204\n",
            "batch_shapes input=(16, 20) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.912 | chrf=23.196 | comet=nan | text_eval_sec=3.66\n",
            "========================================================================================\n",
            "step  24720/32000 | avg_loss=2.3066 | train_ppl=10.0398 | lr=6.4006e-05 | grad_norm=2.9711\n",
            "interval_sec=15.61 | steps/sec=2.5622 | sec/step=0.3903\n",
            "examples_seen=3,164,160 | approx_epochs=2.1238\n",
            "batch_shapes input=(16, 19) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.665 | chrf=21.361 | comet=nan | text_eval_sec=3.66\n",
            "========================================================================================\n",
            "step  24760/32000 | avg_loss=2.2768 | train_ppl=9.7450 | lr=6.36496e-05 | grad_norm=2.8100\n",
            "interval_sec=15.86 | steps/sec=2.5223 | sec/step=0.3965\n",
            "examples_seen=3,169,280 | approx_epochs=2.1272\n",
            "batch_shapes input=(16, 22) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.455 | chrf=19.676 | comet=nan | text_eval_sec=3.65\n",
            "========================================================================================\n",
            "step  24800/32000 | avg_loss=2.3538 | train_ppl=10.5260 | lr=6.32949e-05 | grad_norm=2.7553\n",
            "interval_sec=15.66 | steps/sec=2.5538 | sec/step=0.3916\n",
            "examples_seen=3,174,400 | approx_epochs=2.1307\n",
            "batch_shapes input=(16, 21) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=2.022 | chrf=22.342 | comet=nan | text_eval_sec=3.71\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=24800 val_loss=2.3231 | val_ppl=10.2070 | bleu=1.595 | chrf=22.697 | comet=nan | text_eval_sec=28.56\n",
            "[best] new best val_loss=2.3231 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step  24840/32000 | avg_loss=2.3451 | train_ppl=10.4342 | lr=6.29418e-05 | grad_norm=3.2101\n",
            "interval_sec=50.70 | steps/sec=0.7889 | sec/step=1.2676\n",
            "examples_seen=3,179,520 | approx_epochs=2.1341\n",
            "batch_shapes input=(16, 18) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.642 | chrf=23.001 | comet=nan | text_eval_sec=3.53\n",
            "========================================================================================\n",
            "step  24880/32000 | avg_loss=2.3153 | train_ppl=10.1281 | lr=6.25903e-05 | grad_norm=2.9690\n",
            "interval_sec=15.72 | steps/sec=2.5446 | sec/step=0.3930\n",
            "examples_seen=3,184,640 | approx_epochs=2.1376\n",
            "batch_shapes input=(16, 21) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.321 | chrf=21.929 | comet=nan | text_eval_sec=3.71\n",
            "========================================================================================\n",
            "step  24920/32000 | avg_loss=2.3317 | train_ppl=10.2950 | lr=6.22404e-05 | grad_norm=2.9685\n",
            "interval_sec=15.70 | steps/sec=2.5473 | sec/step=0.3926\n",
            "examples_seen=3,189,760 | approx_epochs=2.1410\n",
            "batch_shapes input=(16, 20) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.229 | chrf=21.277 | comet=nan | text_eval_sec=3.54\n",
            "========================================================================================\n",
            "step  24960/32000 | avg_loss=2.3223 | train_ppl=10.1988 | lr=6.18922e-05 | grad_norm=3.0629\n",
            "interval_sec=15.79 | steps/sec=2.5332 | sec/step=0.3948\n",
            "examples_seen=3,194,880 | approx_epochs=2.1444\n",
            "batch_shapes input=(16, 21) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.845 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.756 | chrf=24.368 | comet=nan | text_eval_sec=3.70\n",
            "========================================================================================\n",
            "step  25000/32000 | avg_loss=2.3093 | train_ppl=10.0672 | lr=6.15456e-05 | grad_norm=2.8887\n",
            "interval_sec=15.52 | steps/sec=2.5768 | sec/step=0.3881\n",
            "examples_seen=3,200,000 | approx_epochs=2.1479\n",
            "batch_shapes input=(16, 65) labels=(16, 63)\n",
            "cuda_mem_alloc_gb=0.919 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.482 | chrf=21.340 | comet=nan | text_eval_sec=3.66\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_025000\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_020000\n",
            "========================================================================================\n",
            "step  25040/32000 | avg_loss=2.3389 | train_ppl=10.3702 | lr=6.12006e-05 | grad_norm=2.8186\n",
            "interval_sec=16.45 | steps/sec=2.4312 | sec/step=0.4113\n",
            "examples_seen=3,205,120 | approx_epochs=2.1513\n",
            "batch_shapes input=(16, 15) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.597 | chrf=20.858 | comet=nan | text_eval_sec=3.63\n",
            "========================================================================================\n",
            "step  25080/32000 | avg_loss=2.3331 | train_ppl=10.3093 | lr=6.08573e-05 | grad_norm=2.9330\n",
            "interval_sec=15.83 | steps/sec=2.5265 | sec/step=0.3958\n",
            "examples_seen=3,210,240 | approx_epochs=2.1547\n",
            "batch_shapes input=(16, 17) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.350 | chrf=20.749 | comet=nan | text_eval_sec=3.83\n",
            "========================================================================================\n",
            "step  25120/32000 | avg_loss=2.3538 | train_ppl=10.5256 | lr=6.05157e-05 | grad_norm=3.3523\n",
            "interval_sec=15.68 | steps/sec=2.5512 | sec/step=0.3920\n",
            "examples_seen=3,215,360 | approx_epochs=2.1582\n",
            "batch_shapes input=(16, 61) labels=(16, 71)\n",
            "cuda_mem_alloc_gb=0.934 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.983 | chrf=25.311 | comet=nan | text_eval_sec=3.64\n",
            "========================================================================================\n",
            "step  25160/32000 | avg_loss=2.2959 | train_ppl=9.9333 | lr=6.01757e-05 | grad_norm=3.4622\n",
            "interval_sec=15.67 | steps/sec=2.5520 | sec/step=0.3919\n",
            "examples_seen=3,220,480 | approx_epochs=2.1616\n",
            "batch_shapes input=(16, 33) labels=(16, 32)\n",
            "cuda_mem_alloc_gb=0.859 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.248 | chrf=17.790 | comet=nan | text_eval_sec=3.70\n",
            "========================================================================================\n",
            "step  25200/32000 | avg_loss=2.2742 | train_ppl=9.7206 | lr=5.98374e-05 | grad_norm=3.1718\n",
            "interval_sec=15.62 | steps/sec=2.5613 | sec/step=0.3904\n",
            "examples_seen=3,225,600 | approx_epochs=2.1650\n",
            "batch_shapes input=(16, 17) labels=(16, 13)\n",
            "cuda_mem_alloc_gb=0.822 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.699 | chrf=19.973 | comet=nan | text_eval_sec=3.49\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=25200 val_loss=2.3543 | val_ppl=10.5307 | bleu=1.506 | chrf=20.881 | comet=nan | text_eval_sec=28.89\n",
            "========================================================================================\n",
            "step  25240/32000 | avg_loss=2.3258 | train_ppl=10.2353 | lr=5.95007e-05 | grad_norm=3.1674\n",
            "interval_sec=50.22 | steps/sec=0.7966 | sec/step=1.2554\n",
            "examples_seen=3,230,720 | approx_epochs=2.1685\n",
            "batch_shapes input=(16, 27) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.844 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.363 | chrf=20.721 | comet=nan | text_eval_sec=3.78\n",
            "========================================================================================\n",
            "step  25280/32000 | avg_loss=2.3041 | train_ppl=10.0154 | lr=5.91657e-05 | grad_norm=3.0013\n",
            "interval_sec=15.97 | steps/sec=2.5043 | sec/step=0.3993\n",
            "examples_seen=3,235,840 | approx_epochs=2.1719\n",
            "batch_shapes input=(16, 22) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.762 | chrf=23.572 | comet=nan | text_eval_sec=3.67\n",
            "========================================================================================\n",
            "step  25320/32000 | avg_loss=2.3668 | train_ppl=10.6634 | lr=5.88324e-05 | grad_norm=3.4385\n",
            "interval_sec=15.65 | steps/sec=2.5554 | sec/step=0.3913\n",
            "examples_seen=3,240,960 | approx_epochs=2.1754\n",
            "batch_shapes input=(16, 25) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.657 | chrf=21.507 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step  25360/32000 | avg_loss=2.3348 | train_ppl=10.3271 | lr=5.85008e-05 | grad_norm=3.1802\n",
            "interval_sec=15.61 | steps/sec=2.5621 | sec/step=0.3903\n",
            "examples_seen=3,246,080 | approx_epochs=2.1788\n",
            "batch_shapes input=(16, 23) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=2.045 | chrf=25.199 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  25400/32000 | avg_loss=2.2785 | train_ppl=9.7622 | lr=5.81709e-05 | grad_norm=3.0106\n",
            "interval_sec=15.61 | steps/sec=2.5624 | sec/step=0.3903\n",
            "examples_seen=3,251,200 | approx_epochs=2.1822\n",
            "batch_shapes input=(16, 16) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.755 | chrf=22.528 | comet=nan | text_eval_sec=3.68\n",
            "========================================================================================\n",
            "step  25440/32000 | avg_loss=2.2930 | train_ppl=9.9043 | lr=5.78427e-05 | grad_norm=3.2719\n",
            "interval_sec=15.68 | steps/sec=2.5514 | sec/step=0.3919\n",
            "examples_seen=3,256,320 | approx_epochs=2.1857\n",
            "batch_shapes input=(16, 18) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.638 | chrf=23.840 | comet=nan | text_eval_sec=3.48\n",
            "========================================================================================\n",
            "step  25480/32000 | avg_loss=2.3447 | train_ppl=10.4301 | lr=5.75162e-05 | grad_norm=2.9170\n",
            "interval_sec=15.79 | steps/sec=2.5338 | sec/step=0.3947\n",
            "examples_seen=3,261,440 | approx_epochs=2.1891\n",
            "batch_shapes input=(16, 26) labels=(16, 28)\n",
            "cuda_mem_alloc_gb=0.851 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.673 | chrf=22.221 | comet=nan | text_eval_sec=3.65\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_025500\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_020500\n",
            "========================================================================================\n",
            "step  25520/32000 | avg_loss=2.3063 | train_ppl=10.0373 | lr=5.71914e-05 | grad_norm=3.1006\n",
            "interval_sec=18.20 | steps/sec=2.1981 | sec/step=0.4549\n",
            "examples_seen=3,266,560 | approx_epochs=2.1925\n",
            "batch_shapes input=(16, 18) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.495 | chrf=21.633 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step  25560/32000 | avg_loss=2.2614 | train_ppl=9.5967 | lr=5.68683e-05 | grad_norm=3.1890\n",
            "interval_sec=15.53 | steps/sec=2.5762 | sec/step=0.3882\n",
            "examples_seen=3,271,680 | approx_epochs=2.1960\n",
            "batch_shapes input=(16, 25) labels=(16, 28)\n",
            "cuda_mem_alloc_gb=0.851 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.706 | chrf=23.916 | comet=nan | text_eval_sec=3.51\n",
            "========================================================================================\n",
            "step  25600/32000 | avg_loss=2.3079 | train_ppl=10.0532 | lr=5.65469e-05 | grad_norm=2.9047\n",
            "interval_sec=15.45 | steps/sec=2.5890 | sec/step=0.3863\n",
            "examples_seen=3,276,800 | approx_epochs=2.1994\n",
            "batch_shapes input=(16, 15) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.569 | chrf=23.257 | comet=nan | text_eval_sec=3.63\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=25600 val_loss=2.3689 | val_ppl=10.6858 | bleu=1.790 | chrf=22.621 | comet=nan | text_eval_sec=27.80\n",
            "========================================================================================\n",
            "step  25640/32000 | avg_loss=2.3653 | train_ppl=10.6472 | lr=5.62273e-05 | grad_norm=3.3048\n",
            "interval_sec=48.81 | steps/sec=0.8196 | sec/step=1.2202\n",
            "examples_seen=3,281,920 | approx_epochs=2.2028\n",
            "batch_shapes input=(16, 21) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.334 | chrf=19.600 | comet=nan | text_eval_sec=3.52\n",
            "========================================================================================\n",
            "step  25680/32000 | avg_loss=2.3660 | train_ppl=10.6543 | lr=5.59094e-05 | grad_norm=2.9556\n",
            "interval_sec=15.63 | steps/sec=2.5593 | sec/step=0.3907\n",
            "examples_seen=3,287,040 | approx_epochs=2.2063\n",
            "batch_shapes input=(16, 21) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.197 | chrf=19.690 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step  25720/32000 | avg_loss=2.3044 | train_ppl=10.0181 | lr=5.55932e-05 | grad_norm=2.9269\n",
            "interval_sec=15.68 | steps/sec=2.5514 | sec/step=0.3919\n",
            "examples_seen=3,292,160 | approx_epochs=2.2097\n",
            "batch_shapes input=(16, 31) labels=(16, 26)\n",
            "cuda_mem_alloc_gb=0.848 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=2.052 | chrf=22.971 | comet=nan | text_eval_sec=3.50\n",
            "========================================================================================\n",
            "step  25760/32000 | avg_loss=2.3396 | train_ppl=10.3776 | lr=5.52788e-05 | grad_norm=3.0989\n",
            "interval_sec=15.60 | steps/sec=2.5646 | sec/step=0.3899\n",
            "examples_seen=3,297,280 | approx_epochs=2.2132\n",
            "batch_shapes input=(16, 20) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.894 | chrf=22.706 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step  25800/32000 | avg_loss=2.2893 | train_ppl=9.8685 | lr=5.49661e-05 | grad_norm=2.9640\n",
            "interval_sec=15.63 | steps/sec=2.5600 | sec/step=0.3906\n",
            "examples_seen=3,302,400 | approx_epochs=2.2166\n",
            "batch_shapes input=(16, 25) labels=(16, 30)\n",
            "cuda_mem_alloc_gb=0.855 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.512 | chrf=21.280 | comet=nan | text_eval_sec=3.77\n",
            "========================================================================================\n",
            "step  25840/32000 | avg_loss=2.3266 | train_ppl=10.2434 | lr=5.46551e-05 | grad_norm=3.1171\n",
            "interval_sec=15.72 | steps/sec=2.5442 | sec/step=0.3931\n",
            "examples_seen=3,307,520 | approx_epochs=2.2200\n",
            "batch_shapes input=(16, 23) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.718 | chrf=23.892 | comet=nan | text_eval_sec=3.55\n",
            "========================================================================================\n",
            "step  25880/32000 | avg_loss=2.3410 | train_ppl=10.3914 | lr=5.43459e-05 | grad_norm=3.1927\n",
            "interval_sec=15.70 | steps/sec=2.5470 | sec/step=0.3926\n",
            "examples_seen=3,312,640 | approx_epochs=2.2235\n",
            "batch_shapes input=(16, 21) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.835 | chrf=23.228 | comet=nan | text_eval_sec=3.56\n",
            "========================================================================================\n",
            "step  25920/32000 | avg_loss=2.3085 | train_ppl=10.0589 | lr=5.40385e-05 | grad_norm=3.0311\n",
            "interval_sec=15.81 | steps/sec=2.5296 | sec/step=0.3953\n",
            "examples_seen=3,317,760 | approx_epochs=2.2269\n",
            "batch_shapes input=(16, 22) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.440 | chrf=19.096 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step  25960/32000 | avg_loss=2.3139 | train_ppl=10.1140 | lr=5.37328e-05 | grad_norm=3.0286\n",
            "interval_sec=16.15 | steps/sec=2.4774 | sec/step=0.4036\n",
            "examples_seen=3,322,880 | approx_epochs=2.2303\n",
            "batch_shapes input=(16, 19) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.829 | chrf=21.469 | comet=nan | text_eval_sec=3.63\n",
            "========================================================================================\n",
            "step  26000/32000 | avg_loss=2.2976 | train_ppl=9.9504 | lr=5.34289e-05 | grad_norm=2.9825\n",
            "interval_sec=15.93 | steps/sec=2.5107 | sec/step=0.3983\n",
            "examples_seen=3,328,000 | approx_epochs=2.2338\n",
            "batch_shapes input=(16, 20) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.405 | chrf=19.854 | comet=nan | text_eval_sec=3.56\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=26000 val_loss=2.3498 | val_ppl=10.4830 | bleu=1.654 | chrf=23.793 | comet=nan | text_eval_sec=28.13\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_026000\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_021000\n",
            "========================================================================================\n",
            "step  26040/32000 | avg_loss=2.2896 | train_ppl=9.8712 | lr=5.31268e-05 | grad_norm=2.9299\n",
            "interval_sec=50.16 | steps/sec=0.7975 | sec/step=1.2539\n",
            "examples_seen=3,333,120 | approx_epochs=2.2372\n",
            "batch_shapes input=(16, 23) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.844 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.973 | chrf=21.861 | comet=nan | text_eval_sec=3.54\n",
            "========================================================================================\n",
            "step  26080/32000 | avg_loss=2.2855 | train_ppl=9.8301 | lr=5.28264e-05 | grad_norm=3.1200\n",
            "interval_sec=15.66 | steps/sec=2.5547 | sec/step=0.3914\n",
            "examples_seen=3,338,240 | approx_epochs=2.2407\n",
            "batch_shapes input=(16, 21) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.275 | chrf=19.434 | comet=nan | text_eval_sec=3.61\n",
            "========================================================================================\n",
            "step  26120/32000 | avg_loss=2.2572 | train_ppl=9.5567 | lr=5.25278e-05 | grad_norm=2.9347\n",
            "interval_sec=15.71 | steps/sec=2.5464 | sec/step=0.3927\n",
            "examples_seen=3,343,360 | approx_epochs=2.2441\n",
            "batch_shapes input=(16, 20) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=2.205 | chrf=24.762 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step  26160/32000 | avg_loss=2.2704 | train_ppl=9.6836 | lr=5.22311e-05 | grad_norm=2.8098\n",
            "interval_sec=15.79 | steps/sec=2.5336 | sec/step=0.3947\n",
            "examples_seen=3,348,480 | approx_epochs=2.2475\n",
            "batch_shapes input=(16, 16) labels=(16, 14)\n",
            "cuda_mem_alloc_gb=0.824 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.516 | chrf=19.761 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step  26200/32000 | avg_loss=2.2641 | train_ppl=9.6223 | lr=5.19361e-05 | grad_norm=2.9347\n",
            "interval_sec=15.59 | steps/sec=2.5660 | sec/step=0.3897\n",
            "examples_seen=3,353,600 | approx_epochs=2.2510\n",
            "batch_shapes input=(16, 19) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.653 | chrf=23.051 | comet=nan | text_eval_sec=3.53\n",
            "========================================================================================\n",
            "step  26240/32000 | avg_loss=2.2840 | train_ppl=9.8156 | lr=5.16429e-05 | grad_norm=3.1179\n",
            "interval_sec=15.74 | steps/sec=2.5408 | sec/step=0.3936\n",
            "examples_seen=3,358,720 | approx_epochs=2.2544\n",
            "batch_shapes input=(16, 21) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.581 | chrf=19.816 | comet=nan | text_eval_sec=3.61\n",
            "========================================================================================\n",
            "step  26280/32000 | avg_loss=2.3004 | train_ppl=9.9779 | lr=5.13515e-05 | grad_norm=3.0962\n",
            "interval_sec=15.61 | steps/sec=2.5625 | sec/step=0.3902\n",
            "examples_seen=3,363,840 | approx_epochs=2.2578\n",
            "batch_shapes input=(16, 19) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.754 | chrf=22.713 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step  26320/32000 | avg_loss=2.2867 | train_ppl=9.8421 | lr=5.10619e-05 | grad_norm=3.0045\n",
            "interval_sec=15.82 | steps/sec=2.5291 | sec/step=0.3954\n",
            "examples_seen=3,368,960 | approx_epochs=2.2613\n",
            "batch_shapes input=(16, 21) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.486 | chrf=22.562 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  26360/32000 | avg_loss=2.2720 | train_ppl=9.6983 | lr=5.07741e-05 | grad_norm=3.3179\n",
            "interval_sec=15.64 | steps/sec=2.5567 | sec/step=0.3911\n",
            "examples_seen=3,374,080 | approx_epochs=2.2647\n",
            "batch_shapes input=(16, 32) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.846 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.747 | chrf=23.568 | comet=nan | text_eval_sec=3.61\n",
            "========================================================================================\n",
            "step  26400/32000 | avg_loss=2.3137 | train_ppl=10.1122 | lr=5.04882e-05 | grad_norm=3.0519\n",
            "interval_sec=15.62 | steps/sec=2.5615 | sec/step=0.3904\n",
            "examples_seen=3,379,200 | approx_epochs=2.2681\n",
            "batch_shapes input=(16, 28) labels=(16, 28)\n",
            "cuda_mem_alloc_gb=0.851 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.567 | chrf=23.101 | comet=nan | text_eval_sec=3.52\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=26400 val_loss=2.3495 | val_ppl=10.4799 | bleu=1.570 | chrf=21.052 | comet=nan | text_eval_sec=28.19\n",
            "========================================================================================\n",
            "step  26440/32000 | avg_loss=2.2470 | train_ppl=9.4594 | lr=5.0204e-05 | grad_norm=2.9227\n",
            "interval_sec=49.23 | steps/sec=0.8125 | sec/step=1.2307\n",
            "examples_seen=3,384,320 | approx_epochs=2.2716\n",
            "batch_shapes input=(16, 26) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=2.097 | chrf=27.137 | comet=nan | text_eval_sec=3.59\n",
            "========================================================================================\n",
            "step  26480/32000 | avg_loss=2.3231 | train_ppl=10.2073 | lr=4.99217e-05 | grad_norm=3.0860\n",
            "interval_sec=15.59 | steps/sec=2.5660 | sec/step=0.3897\n",
            "examples_seen=3,389,440 | approx_epochs=2.2750\n",
            "batch_shapes input=(16, 18) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.625 | chrf=22.436 | comet=nan | text_eval_sec=3.63\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_026500\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_021500\n",
            "========================================================================================\n",
            "step  26520/32000 | avg_loss=2.3290 | train_ppl=10.2676 | lr=4.96412e-05 | grad_norm=3.2509\n",
            "interval_sec=16.31 | steps/sec=2.4532 | sec/step=0.4076\n",
            "examples_seen=3,394,560 | approx_epochs=2.2785\n",
            "batch_shapes input=(16, 26) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.381 | chrf=21.365 | comet=nan | text_eval_sec=3.69\n",
            "========================================================================================\n",
            "step  26560/32000 | avg_loss=2.2883 | train_ppl=9.8581 | lr=4.93626e-05 | grad_norm=3.0224\n",
            "interval_sec=15.57 | steps/sec=2.5690 | sec/step=0.3893\n",
            "examples_seen=3,399,680 | approx_epochs=2.2819\n",
            "batch_shapes input=(16, 20) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.764 | chrf=19.751 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step  26600/32000 | avg_loss=2.2800 | train_ppl=9.7764 | lr=4.90858e-05 | grad_norm=2.9257\n",
            "interval_sec=15.59 | steps/sec=2.5657 | sec/step=0.3898\n",
            "examples_seen=3,404,800 | approx_epochs=2.2853\n",
            "batch_shapes input=(16, 22) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=0.801 | chrf=16.891 | comet=nan | text_eval_sec=3.49\n",
            "========================================================================================\n",
            "step  26640/32000 | avg_loss=2.3304 | train_ppl=10.2825 | lr=4.88108e-05 | grad_norm=2.8716\n",
            "interval_sec=15.68 | steps/sec=2.5508 | sec/step=0.3920\n",
            "examples_seen=3,409,920 | approx_epochs=2.2888\n",
            "batch_shapes input=(16, 47) labels=(16, 48)\n",
            "cuda_mem_alloc_gb=0.890 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.663 | chrf=22.854 | comet=nan | text_eval_sec=3.54\n",
            "========================================================================================\n",
            "step  26680/32000 | avg_loss=2.3391 | train_ppl=10.3718 | lr=4.85377e-05 | grad_norm=3.1484\n",
            "interval_sec=15.66 | steps/sec=2.5540 | sec/step=0.3915\n",
            "examples_seen=3,415,040 | approx_epochs=2.2922\n",
            "batch_shapes input=(16, 18) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.726 | chrf=22.746 | comet=nan | text_eval_sec=3.61\n",
            "========================================================================================\n",
            "step  26720/32000 | avg_loss=2.2884 | train_ppl=9.8592 | lr=4.82664e-05 | grad_norm=3.0234\n",
            "interval_sec=15.72 | steps/sec=2.5453 | sec/step=0.3929\n",
            "examples_seen=3,420,160 | approx_epochs=2.2956\n",
            "batch_shapes input=(16, 22) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.845 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.356 | chrf=20.176 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  26760/32000 | avg_loss=2.2782 | train_ppl=9.7587 | lr=4.7997e-05 | grad_norm=2.8773\n",
            "interval_sec=15.72 | steps/sec=2.5437 | sec/step=0.3931\n",
            "examples_seen=3,425,280 | approx_epochs=2.2991\n",
            "batch_shapes input=(16, 25) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.845 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.773 | chrf=19.784 | comet=nan | text_eval_sec=3.65\n",
            "========================================================================================\n",
            "step  26800/32000 | avg_loss=2.3169 | train_ppl=10.1446 | lr=4.77294e-05 | grad_norm=3.0467\n",
            "interval_sec=15.76 | steps/sec=2.5387 | sec/step=0.3939\n",
            "examples_seen=3,430,400 | approx_epochs=2.3025\n",
            "batch_shapes input=(16, 22) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.126 | chrf=22.189 | comet=nan | text_eval_sec=3.56\n",
            "[eval] step=26800 val_loss=2.3195 | val_ppl=10.1704 | bleu=1.523 | chrf=20.669 | comet=nan | text_eval_sec=28.18\n",
            "[best] new best val_loss=2.3195 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step  26840/32000 | avg_loss=2.2819 | train_ppl=9.7951 | lr=4.74637e-05 | grad_norm=2.9815\n",
            "interval_sec=49.95 | steps/sec=0.8008 | sec/step=1.2488\n",
            "examples_seen=3,435,520 | approx_epochs=2.3059\n",
            "batch_shapes input=(16, 42) labels=(16, 47)\n",
            "cuda_mem_alloc_gb=0.888 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.675 | chrf=17.368 | comet=nan | text_eval_sec=3.65\n",
            "========================================================================================\n",
            "step  26880/32000 | avg_loss=2.2810 | train_ppl=9.7868 | lr=4.71999e-05 | grad_norm=3.0008\n",
            "interval_sec=15.70 | steps/sec=2.5476 | sec/step=0.3925\n",
            "examples_seen=3,440,640 | approx_epochs=2.3094\n",
            "batch_shapes input=(16, 28) labels=(16, 26)\n",
            "cuda_mem_alloc_gb=0.847 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.331 | chrf=18.222 | comet=nan | text_eval_sec=3.55\n",
            "========================================================================================\n",
            "step  26920/32000 | avg_loss=2.3159 | train_ppl=10.1345 | lr=4.69379e-05 | grad_norm=3.1803\n",
            "interval_sec=15.58 | steps/sec=2.5680 | sec/step=0.3894\n",
            "examples_seen=3,445,760 | approx_epochs=2.3128\n",
            "batch_shapes input=(16, 20) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.843 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.560 | chrf=21.104 | comet=nan | text_eval_sec=3.73\n",
            "========================================================================================\n",
            "step  26960/32000 | avg_loss=2.3354 | train_ppl=10.3337 | lr=4.66779e-05 | grad_norm=3.1117\n",
            "interval_sec=16.03 | steps/sec=2.4959 | sec/step=0.4007\n",
            "examples_seen=3,450,880 | approx_epochs=2.3163\n",
            "batch_shapes input=(16, 28) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.846 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.830 | chrf=21.288 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step  27000/32000 | avg_loss=2.2924 | train_ppl=9.8982 | lr=4.64197e-05 | grad_norm=3.6544\n",
            "interval_sec=15.69 | steps/sec=2.5487 | sec/step=0.3924\n",
            "examples_seen=3,456,000 | approx_epochs=2.3197\n",
            "batch_shapes input=(16, 22) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.806 | chrf=27.317 | comet=nan | text_eval_sec=3.53\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_027000\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_022000\n",
            "========================================================================================\n",
            "step  27040/32000 | avg_loss=2.3231 | train_ppl=10.2072 | lr=4.61634e-05 | grad_norm=3.5468\n",
            "interval_sec=16.50 | steps/sec=2.4236 | sec/step=0.4126\n",
            "examples_seen=3,461,120 | approx_epochs=2.3231\n",
            "batch_shapes input=(16, 22) labels=(16, 28)\n",
            "cuda_mem_alloc_gb=0.851 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.631 | chrf=19.272 | comet=nan | text_eval_sec=3.56\n",
            "========================================================================================\n",
            "step  27080/32000 | avg_loss=2.3166 | train_ppl=10.1416 | lr=4.59089e-05 | grad_norm=3.0357\n",
            "interval_sec=15.53 | steps/sec=2.5750 | sec/step=0.3883\n",
            "examples_seen=3,466,240 | approx_epochs=2.3266\n",
            "batch_shapes input=(16, 21) labels=(16, 30)\n",
            "cuda_mem_alloc_gb=0.855 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.562 | chrf=22.987 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  27120/32000 | avg_loss=2.2631 | train_ppl=9.6128 | lr=4.56564e-05 | grad_norm=2.8827\n",
            "interval_sec=15.76 | steps/sec=2.5386 | sec/step=0.3939\n",
            "examples_seen=3,471,360 | approx_epochs=2.3300\n",
            "batch_shapes input=(16, 20) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.794 | chrf=22.636 | comet=nan | text_eval_sec=3.65\n",
            "========================================================================================\n",
            "step  27160/32000 | avg_loss=2.3092 | train_ppl=10.0660 | lr=4.54058e-05 | grad_norm=3.1784\n",
            "interval_sec=15.69 | steps/sec=2.5499 | sec/step=0.3922\n",
            "examples_seen=3,476,480 | approx_epochs=2.3334\n",
            "batch_shapes input=(16, 63) labels=(16, 55)\n",
            "cuda_mem_alloc_gb=0.904 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.769 | chrf=20.133 | comet=nan | text_eval_sec=3.50\n",
            "========================================================================================\n",
            "step  27200/32000 | avg_loss=2.3400 | train_ppl=10.3812 | lr=4.5157e-05 | grad_norm=2.8860\n",
            "interval_sec=15.85 | steps/sec=2.5233 | sec/step=0.3963\n",
            "examples_seen=3,481,600 | approx_epochs=2.3369\n",
            "batch_shapes input=(16, 21) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.843 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.439 | chrf=20.981 | comet=nan | text_eval_sec=3.64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=27200 val_loss=2.2932 | val_ppl=9.9069 | bleu=1.579 | chrf=20.917 | comet=nan | text_eval_sec=28.54\n",
            "[best] new best val_loss=2.2932 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step  27240/32000 | avg_loss=2.2675 | train_ppl=9.6552 | lr=4.49102e-05 | grad_norm=3.1957\n",
            "interval_sec=51.21 | steps/sec=0.7811 | sec/step=1.2803\n",
            "examples_seen=3,486,720 | approx_epochs=2.3403\n",
            "batch_shapes input=(16, 22) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.538 | chrf=24.149 | comet=nan | text_eval_sec=3.55\n",
            "========================================================================================\n",
            "step  27280/32000 | avg_loss=2.3110 | train_ppl=10.0841 | lr=4.46653e-05 | grad_norm=3.2587\n",
            "interval_sec=16.01 | steps/sec=2.4985 | sec/step=0.4002\n",
            "examples_seen=3,491,840 | approx_epochs=2.3437\n",
            "batch_shapes input=(16, 19) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.317 | chrf=20.902 | comet=nan | text_eval_sec=3.63\n",
            "========================================================================================\n",
            "step  27320/32000 | avg_loss=2.2582 | train_ppl=9.5661 | lr=4.44223e-05 | grad_norm=3.3532\n",
            "interval_sec=15.64 | steps/sec=2.5581 | sec/step=0.3909\n",
            "examples_seen=3,496,960 | approx_epochs=2.3472\n",
            "batch_shapes input=(16, 20) labels=(16, 14)\n",
            "cuda_mem_alloc_gb=0.824 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.955 | chrf=23.979 | comet=nan | text_eval_sec=4.74\n",
            "========================================================================================\n",
            "step  27360/32000 | avg_loss=2.2870 | train_ppl=9.8454 | lr=4.41812e-05 | grad_norm=2.8985\n",
            "interval_sec=15.55 | steps/sec=2.5729 | sec/step=0.3887\n",
            "examples_seen=3,502,080 | approx_epochs=2.3506\n",
            "batch_shapes input=(16, 18) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=2.055 | chrf=25.350 | comet=nan | text_eval_sec=3.46\n",
            "========================================================================================\n",
            "step  27400/32000 | avg_loss=2.2722 | train_ppl=9.7003 | lr=4.3942e-05 | grad_norm=3.2128\n",
            "interval_sec=15.95 | steps/sec=2.5084 | sec/step=0.3987\n",
            "examples_seen=3,507,200 | approx_epochs=2.3541\n",
            "batch_shapes input=(16, 18) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.378 | chrf=22.986 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step  27440/32000 | avg_loss=2.2655 | train_ppl=9.6363 | lr=4.37048e-05 | grad_norm=2.8797\n",
            "interval_sec=15.68 | steps/sec=2.5512 | sec/step=0.3920\n",
            "examples_seen=3,512,320 | approx_epochs=2.3575\n",
            "batch_shapes input=(16, 30) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.844 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.530 | chrf=19.781 | comet=nan | text_eval_sec=3.55\n",
            "========================================================================================\n",
            "step  27480/32000 | avg_loss=2.2508 | train_ppl=9.4957 | lr=4.34695e-05 | grad_norm=3.2062\n",
            "interval_sec=15.71 | steps/sec=2.5457 | sec/step=0.3928\n",
            "examples_seen=3,517,440 | approx_epochs=2.3609\n",
            "batch_shapes input=(16, 83) labels=(16, 86)\n",
            "cuda_mem_alloc_gb=0.964 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.427 | chrf=21.722 | comet=nan | text_eval_sec=3.59\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_027500\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_022500\n",
            "========================================================================================\n",
            "step  27520/32000 | avg_loss=2.2715 | train_ppl=9.6940 | lr=4.32361e-05 | grad_norm=3.3777\n",
            "interval_sec=16.34 | steps/sec=2.4483 | sec/step=0.4085\n",
            "examples_seen=3,522,560 | approx_epochs=2.3644\n",
            "batch_shapes input=(16, 17) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.654 | chrf=19.537 | comet=nan | text_eval_sec=3.82\n",
            "========================================================================================\n",
            "step  27560/32000 | avg_loss=2.3045 | train_ppl=10.0187 | lr=4.30046e-05 | grad_norm=2.7457\n",
            "interval_sec=15.66 | steps/sec=2.5549 | sec/step=0.3914\n",
            "examples_seen=3,527,680 | approx_epochs=2.3678\n",
            "batch_shapes input=(16, 17) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.532 | chrf=21.516 | comet=nan | text_eval_sec=3.55\n",
            "========================================================================================\n",
            "step  27600/32000 | avg_loss=2.2662 | train_ppl=9.6424 | lr=4.27751e-05 | grad_norm=3.1925\n",
            "interval_sec=15.66 | steps/sec=2.5541 | sec/step=0.3915\n",
            "examples_seen=3,532,800 | approx_epochs=2.3712\n",
            "batch_shapes input=(16, 25) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=2.268 | chrf=23.237 | comet=nan | text_eval_sec=3.54\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=27600 val_loss=2.3381 | val_ppl=10.3616 | bleu=1.746 | chrf=20.960 | comet=nan | text_eval_sec=27.93\n",
            "========================================================================================\n",
            "step  27640/32000 | avg_loss=2.2702 | train_ppl=9.6817 | lr=4.25475e-05 | grad_norm=3.1738\n",
            "interval_sec=48.91 | steps/sec=0.8178 | sec/step=1.2229\n",
            "examples_seen=3,537,920 | approx_epochs=2.3747\n",
            "batch_shapes input=(16, 23) labels=(16, 29)\n",
            "cuda_mem_alloc_gb=0.853 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.068 | chrf=18.503 | comet=nan | text_eval_sec=3.56\n",
            "========================================================================================\n",
            "step  27680/32000 | avg_loss=2.3038 | train_ppl=10.0117 | lr=4.23219e-05 | grad_norm=3.2166\n",
            "interval_sec=15.66 | steps/sec=2.5538 | sec/step=0.3916\n",
            "examples_seen=3,543,040 | approx_epochs=2.3781\n",
            "batch_shapes input=(16, 20) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.382 | chrf=19.397 | comet=nan | text_eval_sec=3.55\n",
            "========================================================================================\n",
            "step  27720/32000 | avg_loss=2.2544 | train_ppl=9.5293 | lr=4.20983e-05 | grad_norm=3.0462\n",
            "interval_sec=15.47 | steps/sec=2.5850 | sec/step=0.3869\n",
            "examples_seen=3,548,160 | approx_epochs=2.3816\n",
            "batch_shapes input=(16, 16) labels=(16, 14)\n",
            "cuda_mem_alloc_gb=0.824 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.444 | chrf=21.809 | comet=nan | text_eval_sec=3.73\n",
            "========================================================================================\n",
            "step  27760/32000 | avg_loss=2.2819 | train_ppl=9.7955 | lr=4.18765e-05 | grad_norm=2.8075\n",
            "interval_sec=15.92 | steps/sec=2.5118 | sec/step=0.3981\n",
            "examples_seen=3,553,280 | approx_epochs=2.3850\n",
            "batch_shapes input=(16, 18) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.681 | chrf=21.700 | comet=nan | text_eval_sec=3.64\n",
            "========================================================================================\n",
            "step  27800/32000 | avg_loss=2.3270 | train_ppl=10.2474 | lr=4.16568e-05 | grad_norm=3.3381\n",
            "interval_sec=15.66 | steps/sec=2.5548 | sec/step=0.3914\n",
            "examples_seen=3,558,400 | approx_epochs=2.3884\n",
            "batch_shapes input=(16, 79) labels=(16, 86)\n",
            "cuda_mem_alloc_gb=0.963 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.771 | chrf=22.570 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step  27840/32000 | avg_loss=2.2358 | train_ppl=9.3539 | lr=4.1439e-05 | grad_norm=2.8990\n",
            "interval_sec=15.64 | steps/sec=2.5580 | sec/step=0.3909\n",
            "examples_seen=3,563,520 | approx_epochs=2.3919\n",
            "batch_shapes input=(16, 19) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.776 | chrf=21.609 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step  27880/32000 | avg_loss=2.2734 | train_ppl=9.7124 | lr=4.12231e-05 | grad_norm=2.9666\n",
            "interval_sec=15.82 | steps/sec=2.5277 | sec/step=0.3956\n",
            "examples_seen=3,568,640 | approx_epochs=2.3953\n",
            "batch_shapes input=(16, 43) labels=(16, 31)\n",
            "cuda_mem_alloc_gb=0.857 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.617 | chrf=19.660 | comet=nan | text_eval_sec=3.84\n",
            "========================================================================================\n",
            "step  27920/32000 | avg_loss=2.2689 | train_ppl=9.6686 | lr=4.10093e-05 | grad_norm=2.8739\n",
            "interval_sec=16.62 | steps/sec=2.4064 | sec/step=0.4156\n",
            "examples_seen=3,573,760 | approx_epochs=2.3987\n",
            "batch_shapes input=(16, 17) labels=(16, 27)\n",
            "cuda_mem_alloc_gb=0.849 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.532 | chrf=22.386 | comet=nan | text_eval_sec=3.69\n",
            "========================================================================================\n",
            "step  27960/32000 | avg_loss=2.2567 | train_ppl=9.5514 | lr=4.07974e-05 | grad_norm=3.0398\n",
            "interval_sec=15.68 | steps/sec=2.5516 | sec/step=0.3919\n",
            "examples_seen=3,578,880 | approx_epochs=2.4022\n",
            "batch_shapes input=(16, 24) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.844 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=2.029 | chrf=20.789 | comet=nan | text_eval_sec=3.61\n",
            "========================================================================================\n",
            "step  28000/32000 | avg_loss=2.2544 | train_ppl=9.5295 | lr=4.05874e-05 | grad_norm=3.0746\n",
            "interval_sec=15.56 | steps/sec=2.5703 | sec/step=0.3891\n",
            "examples_seen=3,584,000 | approx_epochs=2.4056\n",
            "batch_shapes input=(16, 15) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.570 | chrf=22.121 | comet=nan | text_eval_sec=3.56\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=28000 val_loss=2.3219 | val_ppl=10.1952 | bleu=1.528 | chrf=20.966 | comet=nan | text_eval_sec=28.05\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_028000\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_023000\n",
            "========================================================================================\n",
            "step  28040/32000 | avg_loss=2.2347 | train_ppl=9.3437 | lr=4.03795e-05 | grad_norm=3.0151\n",
            "interval_sec=49.97 | steps/sec=0.8005 | sec/step=1.2493\n",
            "examples_seen=3,589,120 | approx_epochs=2.4090\n",
            "batch_shapes input=(16, 16) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.803 | chrf=22.568 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  28080/32000 | avg_loss=2.2773 | train_ppl=9.7507 | lr=4.01735e-05 | grad_norm=3.0165\n",
            "interval_sec=15.59 | steps/sec=2.5663 | sec/step=0.3897\n",
            "examples_seen=3,594,240 | approx_epochs=2.4125\n",
            "batch_shapes input=(16, 19) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.246 | chrf=19.804 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step  28120/32000 | avg_loss=2.3011 | train_ppl=9.9853 | lr=3.99695e-05 | grad_norm=2.8382\n",
            "interval_sec=15.67 | steps/sec=2.5529 | sec/step=0.3917\n",
            "examples_seen=3,599,360 | approx_epochs=2.4159\n",
            "batch_shapes input=(16, 25) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.902 | chrf=22.748 | comet=nan | text_eval_sec=3.56\n",
            "========================================================================================\n",
            "step  28160/32000 | avg_loss=2.2462 | train_ppl=9.4518 | lr=3.97675e-05 | grad_norm=2.7412\n",
            "interval_sec=15.64 | steps/sec=2.5569 | sec/step=0.3911\n",
            "examples_seen=3,604,480 | approx_epochs=2.4194\n",
            "batch_shapes input=(16, 17) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.485 | chrf=20.635 | comet=nan | text_eval_sec=3.66\n",
            "========================================================================================\n",
            "step  28200/32000 | avg_loss=2.3056 | train_ppl=10.0301 | lr=3.95675e-05 | grad_norm=2.8902\n",
            "interval_sec=16.27 | steps/sec=2.4586 | sec/step=0.4067\n",
            "examples_seen=3,609,600 | approx_epochs=2.4228\n",
            "batch_shapes input=(16, 23) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.844 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.833 | chrf=24.618 | comet=nan | text_eval_sec=3.65\n",
            "========================================================================================\n",
            "step  28240/32000 | avg_loss=2.2478 | train_ppl=9.4670 | lr=3.93695e-05 | grad_norm=2.8625\n",
            "interval_sec=15.68 | steps/sec=2.5516 | sec/step=0.3919\n",
            "examples_seen=3,614,720 | approx_epochs=2.4262\n",
            "batch_shapes input=(16, 29) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.846 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.775 | chrf=24.328 | comet=nan | text_eval_sec=3.96\n",
            "========================================================================================\n",
            "step  28280/32000 | avg_loss=2.2513 | train_ppl=9.5005 | lr=3.91735e-05 | grad_norm=3.0459\n",
            "interval_sec=15.84 | steps/sec=2.5254 | sec/step=0.3960\n",
            "examples_seen=3,619,840 | approx_epochs=2.4297\n",
            "batch_shapes input=(16, 18) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.489 | chrf=20.481 | comet=nan | text_eval_sec=3.59\n",
            "========================================================================================\n",
            "step  28320/32000 | avg_loss=2.2524 | train_ppl=9.5109 | lr=3.89795e-05 | grad_norm=2.9492\n",
            "interval_sec=15.75 | steps/sec=2.5399 | sec/step=0.3937\n",
            "examples_seen=3,624,960 | approx_epochs=2.4331\n",
            "batch_shapes input=(16, 18) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.493 | chrf=19.038 | comet=nan | text_eval_sec=3.48\n",
            "========================================================================================\n",
            "step  28360/32000 | avg_loss=2.3038 | train_ppl=10.0121 | lr=3.87875e-05 | grad_norm=3.1233\n",
            "interval_sec=15.66 | steps/sec=2.5549 | sec/step=0.3914\n",
            "examples_seen=3,630,080 | approx_epochs=2.4365\n",
            "batch_shapes input=(16, 87) labels=(16, 86)\n",
            "cuda_mem_alloc_gb=0.964 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=2.048 | chrf=23.160 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step  28400/32000 | avg_loss=2.3491 | train_ppl=10.4764 | lr=3.85975e-05 | grad_norm=2.8880\n",
            "interval_sec=15.88 | steps/sec=2.5188 | sec/step=0.3970\n",
            "examples_seen=3,635,200 | approx_epochs=2.4400\n",
            "batch_shapes input=(16, 22) labels=(16, 28)\n",
            "cuda_mem_alloc_gb=0.851 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.677 | chrf=20.947 | comet=nan | text_eval_sec=3.55\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=28400 val_loss=2.3035 | val_ppl=10.0087 | bleu=1.664 | chrf=21.057 | comet=nan | text_eval_sec=28.04\n",
            "========================================================================================\n",
            "step  28440/32000 | avg_loss=2.2620 | train_ppl=9.6021 | lr=3.84095e-05 | grad_norm=3.1614\n",
            "interval_sec=49.05 | steps/sec=0.8156 | sec/step=1.2262\n",
            "examples_seen=3,640,320 | approx_epochs=2.4434\n",
            "batch_shapes input=(16, 23) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.813 | chrf=22.619 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step  28480/32000 | avg_loss=2.2755 | train_ppl=9.7326 | lr=3.82235e-05 | grad_norm=2.9375\n",
            "interval_sec=15.68 | steps/sec=2.5516 | sec/step=0.3919\n",
            "examples_seen=3,645,440 | approx_epochs=2.4468\n",
            "batch_shapes input=(16, 49) labels=(16, 54)\n",
            "cuda_mem_alloc_gb=0.902 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.496 | chrf=22.503 | comet=nan | text_eval_sec=3.53\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_028500\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_023500\n",
            "========================================================================================\n",
            "step  28520/32000 | avg_loss=2.3022 | train_ppl=9.9965 | lr=3.80395e-05 | grad_norm=3.2159\n",
            "interval_sec=16.45 | steps/sec=2.4314 | sec/step=0.4113\n",
            "examples_seen=3,650,560 | approx_epochs=2.4503\n",
            "batch_shapes input=(16, 17) labels=(16, 16)\n",
            "cuda_mem_alloc_gb=0.828 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.506 | chrf=21.289 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step  28560/32000 | avg_loss=2.2635 | train_ppl=9.6167 | lr=3.78576e-05 | grad_norm=3.0021\n",
            "interval_sec=15.49 | steps/sec=2.5822 | sec/step=0.3873\n",
            "examples_seen=3,655,680 | approx_epochs=2.4537\n",
            "batch_shapes input=(16, 23) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.401 | chrf=18.439 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step  28600/32000 | avg_loss=2.3389 | train_ppl=10.3699 | lr=3.76777e-05 | grad_norm=2.8561\n",
            "interval_sec=15.60 | steps/sec=2.5642 | sec/step=0.3900\n",
            "examples_seen=3,660,800 | approx_epochs=2.4572\n",
            "batch_shapes input=(16, 21) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.565 | chrf=24.409 | comet=nan | text_eval_sec=3.61\n",
            "========================================================================================\n",
            "step  28640/32000 | avg_loss=2.2333 | train_ppl=9.3310 | lr=3.74998e-05 | grad_norm=3.0931\n",
            "interval_sec=15.63 | steps/sec=2.5595 | sec/step=0.3907\n",
            "examples_seen=3,665,920 | approx_epochs=2.4606\n",
            "batch_shapes input=(16, 22) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.771 | chrf=23.323 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step  28680/32000 | avg_loss=2.2873 | train_ppl=9.8482 | lr=3.73239e-05 | grad_norm=3.1327\n",
            "interval_sec=15.48 | steps/sec=2.5839 | sec/step=0.3870\n",
            "examples_seen=3,671,040 | approx_epochs=2.4640\n",
            "batch_shapes input=(16, 19) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.729 | chrf=25.456 | comet=nan | text_eval_sec=3.50\n",
            "========================================================================================\n",
            "step  28720/32000 | avg_loss=2.2274 | train_ppl=9.2758 | lr=3.715e-05 | grad_norm=3.2593\n",
            "interval_sec=15.56 | steps/sec=2.5700 | sec/step=0.3891\n",
            "examples_seen=3,676,160 | approx_epochs=2.4675\n",
            "batch_shapes input=(16, 18) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.875 | chrf=24.624 | comet=nan | text_eval_sec=3.51\n",
            "========================================================================================\n",
            "step  28760/32000 | avg_loss=2.2845 | train_ppl=9.8208 | lr=3.69782e-05 | grad_norm=3.0921\n",
            "interval_sec=16.03 | steps/sec=2.4953 | sec/step=0.4007\n",
            "examples_seen=3,681,280 | approx_epochs=2.4709\n",
            "batch_shapes input=(16, 25) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.241 | chrf=20.903 | comet=nan | text_eval_sec=3.72\n",
            "========================================================================================\n",
            "step  28800/32000 | avg_loss=2.2829 | train_ppl=9.8046 | lr=3.68084e-05 | grad_norm=2.9309\n",
            "interval_sec=15.97 | steps/sec=2.5044 | sec/step=0.3993\n",
            "examples_seen=3,686,400 | approx_epochs=2.4743\n",
            "batch_shapes input=(16, 16) labels=(16, 13)\n",
            "cuda_mem_alloc_gb=0.822 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.687 | chrf=25.564 | comet=nan | text_eval_sec=3.70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=28800 val_loss=2.2825 | val_ppl=9.8010 | bleu=1.715 | chrf=21.967 | comet=nan | text_eval_sec=28.18\n",
            "[best] new best val_loss=2.2825 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step  28840/32000 | avg_loss=2.2655 | train_ppl=9.6358 | lr=3.66407e-05 | grad_norm=2.8339\n",
            "interval_sec=50.12 | steps/sec=0.7980 | sec/step=1.2531\n",
            "examples_seen=3,691,520 | approx_epochs=2.4778\n",
            "batch_shapes input=(16, 90) labels=(16, 74)\n",
            "cuda_mem_alloc_gb=0.941 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=2.231 | chrf=25.123 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step  28880/32000 | avg_loss=2.3101 | train_ppl=10.0750 | lr=3.6475e-05 | grad_norm=2.8301\n",
            "interval_sec=15.63 | steps/sec=2.5588 | sec/step=0.3908\n",
            "examples_seen=3,696,640 | approx_epochs=2.4812\n",
            "batch_shapes input=(16, 28) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.346 | chrf=22.293 | comet=nan | text_eval_sec=3.56\n",
            "========================================================================================\n",
            "step  28920/32000 | avg_loss=2.2786 | train_ppl=9.7631 | lr=3.63113e-05 | grad_norm=2.8760\n",
            "interval_sec=15.61 | steps/sec=2.5626 | sec/step=0.3902\n",
            "examples_seen=3,701,760 | approx_epochs=2.4846\n",
            "batch_shapes input=(16, 21) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.474 | chrf=20.966 | comet=nan | text_eval_sec=3.55\n",
            "========================================================================================\n",
            "step  28960/32000 | avg_loss=2.3276 | train_ppl=10.2535 | lr=3.61497e-05 | grad_norm=3.0371\n",
            "interval_sec=15.63 | steps/sec=2.5599 | sec/step=0.3906\n",
            "examples_seen=3,706,880 | approx_epochs=2.4881\n",
            "batch_shapes input=(16, 32) labels=(16, 27)\n",
            "cuda_mem_alloc_gb=0.850 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.608 | chrf=20.892 | comet=nan | text_eval_sec=3.64\n",
            "========================================================================================\n",
            "step  29000/32000 | avg_loss=2.2955 | train_ppl=9.9290 | lr=3.59901e-05 | grad_norm=3.1839\n",
            "interval_sec=15.76 | steps/sec=2.5384 | sec/step=0.3940\n",
            "examples_seen=3,712,000 | approx_epochs=2.4915\n",
            "batch_shapes input=(16, 53) labels=(16, 59)\n",
            "cuda_mem_alloc_gb=0.911 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.524 | chrf=20.679 | comet=nan | text_eval_sec=3.62\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_029000\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_024000\n",
            "========================================================================================\n",
            "step  29040/32000 | avg_loss=2.2533 | train_ppl=9.5192 | lr=3.58326e-05 | grad_norm=3.0811\n",
            "interval_sec=16.43 | steps/sec=2.4348 | sec/step=0.4107\n",
            "examples_seen=3,717,120 | approx_epochs=2.4950\n",
            "batch_shapes input=(16, 18) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.247 | chrf=18.830 | comet=nan | text_eval_sec=3.54\n",
            "========================================================================================\n",
            "step  29080/32000 | avg_loss=2.3067 | train_ppl=10.0409 | lr=3.56771e-05 | grad_norm=3.0958\n",
            "interval_sec=15.73 | steps/sec=2.5428 | sec/step=0.3933\n",
            "examples_seen=3,722,240 | approx_epochs=2.4984\n",
            "batch_shapes input=(16, 26) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.591 | chrf=20.217 | comet=nan | text_eval_sec=4.38\n",
            "========================================================================================\n",
            "step  29120/32000 | avg_loss=2.2653 | train_ppl=9.6345 | lr=3.55237e-05 | grad_norm=3.0707\n",
            "interval_sec=15.57 | steps/sec=2.5697 | sec/step=0.3891\n",
            "examples_seen=3,727,360 | approx_epochs=2.5018\n",
            "batch_shapes input=(16, 22) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.898 | chrf=22.490 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step  29160/32000 | avg_loss=2.2546 | train_ppl=9.5315 | lr=3.53724e-05 | grad_norm=3.1658\n",
            "interval_sec=15.63 | steps/sec=2.5599 | sec/step=0.3906\n",
            "examples_seen=3,732,480 | approx_epochs=2.5053\n",
            "batch_shapes input=(16, 18) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.508 | chrf=22.145 | comet=nan | text_eval_sec=3.54\n",
            "========================================================================================\n",
            "step  29200/32000 | avg_loss=2.2740 | train_ppl=9.7181 | lr=3.52231e-05 | grad_norm=2.8827\n",
            "interval_sec=15.78 | steps/sec=2.5353 | sec/step=0.3944\n",
            "examples_seen=3,737,600 | approx_epochs=2.5087\n",
            "batch_shapes input=(16, 22) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.845 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.576 | chrf=22.295 | comet=nan | text_eval_sec=3.54\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=29200 val_loss=2.2382 | val_ppl=9.3769 | bleu=1.731 | chrf=22.494 | comet=nan | text_eval_sec=28.78\n",
            "[best] new best val_loss=2.2382 saved to c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n",
            "========================================================================================\n",
            "step  29240/32000 | avg_loss=2.2482 | train_ppl=9.4711 | lr=3.50759e-05 | grad_norm=2.9873\n",
            "interval_sec=50.90 | steps/sec=0.7858 | sec/step=1.2725\n",
            "examples_seen=3,742,720 | approx_epochs=2.5121\n",
            "batch_shapes input=(16, 33) labels=(16, 39)\n",
            "cuda_mem_alloc_gb=0.873 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.490 | chrf=20.481 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step  29280/32000 | avg_loss=2.2539 | train_ppl=9.5251 | lr=3.49307e-05 | grad_norm=3.1096\n",
            "interval_sec=15.85 | steps/sec=2.5242 | sec/step=0.3962\n",
            "examples_seen=3,747,840 | approx_epochs=2.5156\n",
            "batch_shapes input=(16, 47) labels=(16, 48)\n",
            "cuda_mem_alloc_gb=0.890 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.060 | chrf=18.453 | comet=nan | text_eval_sec=3.55\n",
            "========================================================================================\n",
            "step  29320/32000 | avg_loss=2.2477 | train_ppl=9.4659 | lr=3.47876e-05 | grad_norm=3.0001\n",
            "interval_sec=15.83 | steps/sec=2.5272 | sec/step=0.3957\n",
            "examples_seen=3,752,960 | approx_epochs=2.5190\n",
            "batch_shapes input=(16, 22) labels=(16, 26)\n",
            "cuda_mem_alloc_gb=0.847 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.441 | chrf=21.232 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step  29360/32000 | avg_loss=2.2866 | train_ppl=9.8418 | lr=3.46466e-05 | grad_norm=3.0927\n",
            "interval_sec=15.85 | steps/sec=2.5243 | sec/step=0.3962\n",
            "examples_seen=3,758,080 | approx_epochs=2.5225\n",
            "batch_shapes input=(16, 24) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.379 | chrf=20.378 | comet=nan | text_eval_sec=3.81\n",
            "========================================================================================\n",
            "step  29400/32000 | avg_loss=2.2523 | train_ppl=9.5099 | lr=3.45076e-05 | grad_norm=3.1806\n",
            "interval_sec=16.08 | steps/sec=2.4880 | sec/step=0.4019\n",
            "examples_seen=3,763,200 | approx_epochs=2.5259\n",
            "batch_shapes input=(16, 23) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.625 | chrf=21.156 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step  29440/32000 | avg_loss=2.2525 | train_ppl=9.5115 | lr=3.43707e-05 | grad_norm=2.9143\n",
            "interval_sec=15.69 | steps/sec=2.5490 | sec/step=0.3923\n",
            "examples_seen=3,768,320 | approx_epochs=2.5293\n",
            "batch_shapes input=(16, 26) labels=(16, 26)\n",
            "cuda_mem_alloc_gb=0.847 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.307 | chrf=23.650 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step  29480/32000 | avg_loss=2.2736 | train_ppl=9.7139 | lr=3.42359e-05 | grad_norm=2.8261\n",
            "interval_sec=15.81 | steps/sec=2.5299 | sec/step=0.3953\n",
            "examples_seen=3,773,440 | approx_epochs=2.5328\n",
            "batch_shapes input=(16, 24) labels=(16, 26)\n",
            "cuda_mem_alloc_gb=0.847 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.139 | chrf=19.506 | comet=nan | text_eval_sec=3.61\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_029500\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_024500\n",
            "========================================================================================\n",
            "step  29520/32000 | avg_loss=2.2810 | train_ppl=9.7863 | lr=3.41032e-05 | grad_norm=3.1729\n",
            "interval_sec=16.51 | steps/sec=2.4235 | sec/step=0.4126\n",
            "examples_seen=3,778,560 | approx_epochs=2.5362\n",
            "batch_shapes input=(16, 32) labels=(16, 27)\n",
            "cuda_mem_alloc_gb=0.850 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.082 | chrf=22.195 | comet=nan | text_eval_sec=4.76\n",
            "========================================================================================\n",
            "step  29560/32000 | avg_loss=2.2848 | train_ppl=9.8232 | lr=3.39725e-05 | grad_norm=3.3882\n",
            "interval_sec=15.69 | steps/sec=2.5495 | sec/step=0.3922\n",
            "examples_seen=3,783,680 | approx_epochs=2.5396\n",
            "batch_shapes input=(16, 18) labels=(16, 14)\n",
            "cuda_mem_alloc_gb=0.824 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.872 | chrf=23.306 | comet=nan | text_eval_sec=3.56\n",
            "========================================================================================\n",
            "step  29600/32000 | avg_loss=2.2609 | train_ppl=9.5920 | lr=3.3844e-05 | grad_norm=3.1561\n",
            "interval_sec=15.80 | steps/sec=2.5314 | sec/step=0.3950\n",
            "examples_seen=3,788,800 | approx_epochs=2.5431\n",
            "batch_shapes input=(16, 24) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.439 | chrf=20.429 | comet=nan | text_eval_sec=3.61\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=29600 val_loss=2.2855 | val_ppl=9.8307 | bleu=1.717 | chrf=22.153 | comet=nan | text_eval_sec=28.35\n",
            "========================================================================================\n",
            "step  29640/32000 | avg_loss=2.2801 | train_ppl=9.7779 | lr=3.37175e-05 | grad_norm=3.3854\n",
            "interval_sec=49.65 | steps/sec=0.8056 | sec/step=1.2414\n",
            "examples_seen=3,793,920 | approx_epochs=2.5465\n",
            "batch_shapes input=(16, 28) labels=(16, 31)\n",
            "cuda_mem_alloc_gb=0.857 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.457 | chrf=20.193 | comet=nan | text_eval_sec=3.84\n",
            "========================================================================================\n",
            "step  29680/32000 | avg_loss=2.2711 | train_ppl=9.6902 | lr=3.35931e-05 | grad_norm=3.2272\n",
            "interval_sec=16.04 | steps/sec=2.4940 | sec/step=0.4010\n",
            "examples_seen=3,799,040 | approx_epochs=2.5499\n",
            "batch_shapes input=(16, 25) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.844 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.549 | chrf=21.205 | comet=nan | text_eval_sec=3.61\n",
            "========================================================================================\n",
            "step  29720/32000 | avg_loss=2.2395 | train_ppl=9.3884 | lr=3.34708e-05 | grad_norm=2.9697\n",
            "interval_sec=15.67 | steps/sec=2.5526 | sec/step=0.3918\n",
            "examples_seen=3,804,160 | approx_epochs=2.5534\n",
            "batch_shapes input=(16, 18) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.694 | chrf=22.570 | comet=nan | text_eval_sec=3.61\n",
            "========================================================================================\n",
            "step  29760/32000 | avg_loss=2.2493 | train_ppl=9.4811 | lr=3.33506e-05 | grad_norm=3.1000\n",
            "interval_sec=15.65 | steps/sec=2.5562 | sec/step=0.3912\n",
            "examples_seen=3,809,280 | approx_epochs=2.5568\n",
            "batch_shapes input=(16, 18) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.930 | chrf=24.292 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step  29800/32000 | avg_loss=2.2647 | train_ppl=9.6286 | lr=3.32325e-05 | grad_norm=2.9964\n",
            "interval_sec=15.60 | steps/sec=2.5647 | sec/step=0.3899\n",
            "examples_seen=3,814,400 | approx_epochs=2.5603\n",
            "batch_shapes input=(16, 15) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.258 | chrf=21.827 | comet=nan | text_eval_sec=3.49\n",
            "========================================================================================\n",
            "step  29840/32000 | avg_loss=2.2743 | train_ppl=9.7212 | lr=3.31165e-05 | grad_norm=3.0613\n",
            "interval_sec=15.49 | steps/sec=2.5830 | sec/step=0.3871\n",
            "examples_seen=3,819,520 | approx_epochs=2.5637\n",
            "batch_shapes input=(16, 22) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.845 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.698 | chrf=21.080 | comet=nan | text_eval_sec=3.54\n",
            "========================================================================================\n",
            "step  29880/32000 | avg_loss=2.2814 | train_ppl=9.7900 | lr=3.30025e-05 | grad_norm=3.2040\n",
            "interval_sec=15.62 | steps/sec=2.5616 | sec/step=0.3904\n",
            "examples_seen=3,824,640 | approx_epochs=2.5671\n",
            "batch_shapes input=(16, 21) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.787 | chrf=22.287 | comet=nan | text_eval_sec=3.59\n",
            "========================================================================================\n",
            "step  29920/32000 | avg_loss=2.2611 | train_ppl=9.5937 | lr=3.28907e-05 | grad_norm=3.1070\n",
            "interval_sec=15.67 | steps/sec=2.5519 | sec/step=0.3919\n",
            "examples_seen=3,829,760 | approx_epochs=2.5706\n",
            "batch_shapes input=(16, 28) labels=(16, 25)\n",
            "cuda_mem_alloc_gb=0.846 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.542 | chrf=25.964 | comet=nan | text_eval_sec=3.61\n",
            "========================================================================================\n",
            "step  29960/32000 | avg_loss=2.2388 | train_ppl=9.3822 | lr=3.2781e-05 | grad_norm=3.0451\n",
            "interval_sec=15.58 | steps/sec=2.5682 | sec/step=0.3894\n",
            "examples_seen=3,834,880 | approx_epochs=2.5740\n",
            "batch_shapes input=(16, 22) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.830 | chrf=21.663 | comet=nan | text_eval_sec=3.67\n",
            "========================================================================================\n",
            "step  30000/32000 | avg_loss=2.2385 | train_ppl=9.3796 | lr=3.26733e-05 | grad_norm=3.2518\n",
            "interval_sec=15.67 | steps/sec=2.5526 | sec/step=0.3918\n",
            "examples_seen=3,840,000 | approx_epochs=2.5774\n",
            "batch_shapes input=(16, 15) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.723 | chrf=25.605 | comet=nan | text_eval_sec=3.52\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=30000 val_loss=2.2831 | val_ppl=9.8075 | bleu=1.681 | chrf=22.360 | comet=nan | text_eval_sec=28.04\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_030000\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_025000\n",
            "========================================================================================\n",
            "step  30040/32000 | avg_loss=2.2421 | train_ppl=9.4128 | lr=3.25678e-05 | grad_norm=2.9627\n",
            "interval_sec=49.76 | steps/sec=0.8039 | sec/step=1.2439\n",
            "examples_seen=3,845,120 | approx_epochs=2.5809\n",
            "batch_shapes input=(16, 56) labels=(16, 64)\n",
            "cuda_mem_alloc_gb=0.921 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.648 | chrf=21.189 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step  30080/32000 | avg_loss=2.2900 | train_ppl=9.8749 | lr=3.24644e-05 | grad_norm=3.1481\n",
            "interval_sec=15.69 | steps/sec=2.5489 | sec/step=0.3923\n",
            "examples_seen=3,850,240 | approx_epochs=2.5843\n",
            "batch_shapes input=(16, 21) labels=(16, 27)\n",
            "cuda_mem_alloc_gb=0.849 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.901 | chrf=28.513 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step  30120/32000 | avg_loss=2.3355 | train_ppl=10.3348 | lr=3.23631e-05 | grad_norm=3.2143\n",
            "interval_sec=15.48 | steps/sec=2.5841 | sec/step=0.3870\n",
            "examples_seen=3,855,360 | approx_epochs=2.5877\n",
            "batch_shapes input=(16, 18) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.549 | chrf=22.886 | comet=nan | text_eval_sec=3.59\n",
            "========================================================================================\n",
            "step  30160/32000 | avg_loss=2.3023 | train_ppl=9.9970 | lr=3.22639e-05 | grad_norm=3.0833\n",
            "interval_sec=15.52 | steps/sec=2.5768 | sec/step=0.3881\n",
            "examples_seen=3,860,480 | approx_epochs=2.5912\n",
            "batch_shapes input=(16, 23) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.213 | chrf=21.570 | comet=nan | text_eval_sec=3.55\n",
            "========================================================================================\n",
            "step  30200/32000 | avg_loss=2.2705 | train_ppl=9.6842 | lr=3.21668e-05 | grad_norm=3.1467\n",
            "interval_sec=15.58 | steps/sec=2.5674 | sec/step=0.3895\n",
            "examples_seen=3,865,600 | approx_epochs=2.5946\n",
            "batch_shapes input=(16, 18) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.644 | chrf=22.525 | comet=nan | text_eval_sec=3.56\n",
            "========================================================================================\n",
            "step  30240/32000 | avg_loss=2.2999 | train_ppl=9.9733 | lr=3.20718e-05 | grad_norm=3.0271\n",
            "interval_sec=15.62 | steps/sec=2.5615 | sec/step=0.3904\n",
            "examples_seen=3,870,720 | approx_epochs=2.5981\n",
            "batch_shapes input=(16, 18) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=2.007 | chrf=24.587 | comet=nan | text_eval_sec=3.54\n",
            "========================================================================================\n",
            "step  30280/32000 | avg_loss=2.2531 | train_ppl=9.5174 | lr=3.19789e-05 | grad_norm=3.3059\n",
            "interval_sec=15.68 | steps/sec=2.5506 | sec/step=0.3921\n",
            "examples_seen=3,875,840 | approx_epochs=2.6015\n",
            "batch_shapes input=(16, 14) labels=(16, 13)\n",
            "cuda_mem_alloc_gb=0.822 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.524 | chrf=22.889 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  30320/32000 | avg_loss=2.2529 | train_ppl=9.5154 | lr=3.18881e-05 | grad_norm=3.0642\n",
            "interval_sec=15.63 | steps/sec=2.5594 | sec/step=0.3907\n",
            "examples_seen=3,880,960 | approx_epochs=2.6049\n",
            "batch_shapes input=(16, 17) labels=(16, 12)\n",
            "cuda_mem_alloc_gb=0.820 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.965 | chrf=23.991 | comet=nan | text_eval_sec=3.54\n",
            "========================================================================================\n",
            "step  30360/32000 | avg_loss=2.2695 | train_ppl=9.6749 | lr=3.17995e-05 | grad_norm=3.6940\n",
            "interval_sec=15.89 | steps/sec=2.5169 | sec/step=0.3973\n",
            "examples_seen=3,886,080 | approx_epochs=2.6084\n",
            "batch_shapes input=(16, 19) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.930 | chrf=26.981 | comet=nan | text_eval_sec=3.67\n",
            "========================================================================================\n",
            "step  30400/32000 | avg_loss=2.2556 | train_ppl=9.5406 | lr=3.1713e-05 | grad_norm=3.4197\n",
            "interval_sec=15.70 | steps/sec=2.5480 | sec/step=0.3925\n",
            "examples_seen=3,891,200 | approx_epochs=2.6118\n",
            "batch_shapes input=(16, 18) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.365 | chrf=18.281 | comet=nan | text_eval_sec=3.58\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=30400 val_loss=2.2828 | val_ppl=9.8037 | bleu=1.725 | chrf=23.403 | comet=nan | text_eval_sec=28.22\n",
            "========================================================================================\n",
            "step  30440/32000 | avg_loss=2.2502 | train_ppl=9.4895 | lr=3.16286e-05 | grad_norm=2.9956\n",
            "interval_sec=49.33 | steps/sec=0.8109 | sec/step=1.2332\n",
            "examples_seen=3,896,320 | approx_epochs=2.6152\n",
            "batch_shapes input=(16, 20) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=2.251 | chrf=23.808 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step  30480/32000 | avg_loss=2.2990 | train_ppl=9.9644 | lr=3.15463e-05 | grad_norm=3.1165\n",
            "interval_sec=15.85 | steps/sec=2.5232 | sec/step=0.3963\n",
            "examples_seen=3,901,440 | approx_epochs=2.6187\n",
            "batch_shapes input=(16, 21) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.964 | chrf=24.197 | comet=nan | text_eval_sec=3.56\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_030500\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_025500\n",
            "========================================================================================\n",
            "step  30520/32000 | avg_loss=2.2120 | train_ppl=9.1337 | lr=3.14661e-05 | grad_norm=3.2097\n",
            "interval_sec=16.41 | steps/sec=2.4380 | sec/step=0.4102\n",
            "examples_seen=3,906,560 | approx_epochs=2.6221\n",
            "batch_shapes input=(16, 17) labels=(16, 15)\n",
            "cuda_mem_alloc_gb=0.826 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.968 | chrf=26.801 | comet=nan | text_eval_sec=4.28\n",
            "========================================================================================\n",
            "step  30560/32000 | avg_loss=2.2764 | train_ppl=9.7417 | lr=3.13881e-05 | grad_norm=2.8994\n",
            "interval_sec=15.77 | steps/sec=2.5367 | sec/step=0.3942\n",
            "examples_seen=3,911,680 | approx_epochs=2.6255\n",
            "batch_shapes input=(16, 23) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.483 | chrf=23.674 | comet=nan | text_eval_sec=3.62\n",
            "========================================================================================\n",
            "step  30600/32000 | avg_loss=2.2832 | train_ppl=9.8080 | lr=3.13121e-05 | grad_norm=2.9102\n",
            "interval_sec=15.73 | steps/sec=2.5430 | sec/step=0.3932\n",
            "examples_seen=3,916,800 | approx_epochs=2.6290\n",
            "batch_shapes input=(16, 14) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.693 | chrf=23.109 | comet=nan | text_eval_sec=3.57\n",
            "========================================================================================\n",
            "step  30640/32000 | avg_loss=2.2273 | train_ppl=9.2748 | lr=3.12384e-05 | grad_norm=3.1253\n",
            "interval_sec=15.54 | steps/sec=2.5732 | sec/step=0.3886\n",
            "examples_seen=3,921,920 | approx_epochs=2.6324\n",
            "batch_shapes input=(16, 22) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.799 | chrf=23.399 | comet=nan | text_eval_sec=3.63\n",
            "========================================================================================\n",
            "step  30680/32000 | avg_loss=2.2469 | train_ppl=9.4581 | lr=3.11667e-05 | grad_norm=3.1825\n",
            "interval_sec=15.71 | steps/sec=2.5461 | sec/step=0.3928\n",
            "examples_seen=3,927,040 | approx_epochs=2.6359\n",
            "batch_shapes input=(16, 53) labels=(16, 54)\n",
            "cuda_mem_alloc_gb=0.902 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.607 | chrf=25.170 | comet=nan | text_eval_sec=3.53\n",
            "========================================================================================\n",
            "step  30720/32000 | avg_loss=2.2914 | train_ppl=9.8886 | lr=3.10971e-05 | grad_norm=3.0554\n",
            "interval_sec=15.56 | steps/sec=2.5705 | sec/step=0.3890\n",
            "examples_seen=3,932,160 | approx_epochs=2.6393\n",
            "batch_shapes input=(16, 26) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.876 | chrf=23.161 | comet=nan | text_eval_sec=3.58\n",
            "========================================================================================\n",
            "step  30760/32000 | avg_loss=2.2721 | train_ppl=9.7000 | lr=3.10297e-05 | grad_norm=3.1209\n",
            "interval_sec=15.75 | steps/sec=2.5400 | sec/step=0.3937\n",
            "examples_seen=3,937,280 | approx_epochs=2.6427\n",
            "batch_shapes input=(16, 19) labels=(16, 13)\n",
            "cuda_mem_alloc_gb=0.822 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.254 | chrf=22.705 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  30800/32000 | avg_loss=2.2816 | train_ppl=9.7921 | lr=3.09644e-05 | grad_norm=3.0510\n",
            "interval_sec=15.64 | steps/sec=2.5570 | sec/step=0.3911\n",
            "examples_seen=3,942,400 | approx_epochs=2.6462\n",
            "batch_shapes input=(16, 26) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.767 | chrf=22.863 | comet=nan | text_eval_sec=3.56\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=30800 val_loss=2.2744 | val_ppl=9.7221 | bleu=1.716 | chrf=23.496 | comet=nan | text_eval_sec=28.28\n",
            "========================================================================================\n",
            "step  30840/32000 | avg_loss=2.2264 | train_ppl=9.2663 | lr=3.09013e-05 | grad_norm=3.1392\n",
            "interval_sec=49.43 | steps/sec=0.8093 | sec/step=1.2357\n",
            "examples_seen=3,947,520 | approx_epochs=2.6496\n",
            "batch_shapes input=(16, 28) labels=(16, 22)\n",
            "cuda_mem_alloc_gb=0.840 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.366 | chrf=19.333 | comet=nan | text_eval_sec=3.64\n",
            "========================================================================================\n",
            "step  30880/32000 | avg_loss=2.2764 | train_ppl=9.7411 | lr=3.08403e-05 | grad_norm=2.9009\n",
            "interval_sec=16.07 | steps/sec=2.4898 | sec/step=0.4016\n",
            "examples_seen=3,952,640 | approx_epochs=2.6530\n",
            "batch_shapes input=(16, 24) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.009 | chrf=19.310 | comet=nan | text_eval_sec=3.55\n",
            "========================================================================================\n",
            "step  30920/32000 | avg_loss=2.2514 | train_ppl=9.5008 | lr=3.07814e-05 | grad_norm=2.8375\n",
            "interval_sec=15.90 | steps/sec=2.5164 | sec/step=0.3974\n",
            "examples_seen=3,957,760 | approx_epochs=2.6565\n",
            "batch_shapes input=(16, 21) labels=(16, 19)\n",
            "cuda_mem_alloc_gb=0.834 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.269 | chrf=22.673 | comet=nan | text_eval_sec=3.56\n",
            "========================================================================================\n",
            "step  30960/32000 | avg_loss=2.2846 | train_ppl=9.8219 | lr=3.07246e-05 | grad_norm=2.9412\n",
            "interval_sec=15.80 | steps/sec=2.5323 | sec/step=0.3949\n",
            "examples_seen=3,962,880 | approx_epochs=2.6599\n",
            "batch_shapes input=(16, 28) labels=(16, 29)\n",
            "cuda_mem_alloc_gb=0.853 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.409 | chrf=21.104 | comet=nan | text_eval_sec=3.63\n",
            "========================================================================================\n",
            "step  31000/32000 | avg_loss=2.2596 | train_ppl=9.5795 | lr=3.067e-05 | grad_norm=3.3016\n",
            "interval_sec=15.71 | steps/sec=2.5467 | sec/step=0.3927\n",
            "examples_seen=3,968,000 | approx_epochs=2.6634\n",
            "batch_shapes input=(16, 25) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.714 | chrf=23.603 | comet=nan | text_eval_sec=3.62\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_031000\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_026000\n",
            "========================================================================================\n",
            "step  31040/32000 | avg_loss=2.2159 | train_ppl=9.1697 | lr=3.06175e-05 | grad_norm=3.0091\n",
            "interval_sec=16.52 | steps/sec=2.4208 | sec/step=0.4131\n",
            "examples_seen=3,973,120 | approx_epochs=2.6668\n",
            "batch_shapes input=(16, 22) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=2.133 | chrf=27.783 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  31080/32000 | avg_loss=2.2389 | train_ppl=9.3831 | lr=3.05672e-05 | grad_norm=3.3774\n",
            "interval_sec=15.62 | steps/sec=2.5614 | sec/step=0.3904\n",
            "examples_seen=3,978,240 | approx_epochs=2.6702\n",
            "batch_shapes input=(16, 21) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.647 | chrf=21.600 | comet=nan | text_eval_sec=3.65\n",
            "========================================================================================\n",
            "step  31120/32000 | avg_loss=2.3234 | train_ppl=10.2102 | lr=3.05189e-05 | grad_norm=2.8973\n",
            "interval_sec=15.68 | steps/sec=2.5516 | sec/step=0.3919\n",
            "examples_seen=3,983,360 | approx_epochs=2.6737\n",
            "batch_shapes input=(16, 103) labels=(16, 120)\n",
            "cuda_mem_alloc_gb=1.029 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.572 | chrf=22.204 | comet=nan | text_eval_sec=3.52\n",
            "========================================================================================\n",
            "step  31160/32000 | avg_loss=2.2559 | train_ppl=9.5439 | lr=3.04729e-05 | grad_norm=3.1714\n",
            "interval_sec=15.68 | steps/sec=2.5512 | sec/step=0.3920\n",
            "examples_seen=3,988,480 | approx_epochs=2.6771\n",
            "batch_shapes input=(16, 29) labels=(16, 29)\n",
            "cuda_mem_alloc_gb=0.853 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.601 | chrf=23.186 | comet=nan | text_eval_sec=3.69\n",
            "========================================================================================\n",
            "step  31200/32000 | avg_loss=2.2569 | train_ppl=9.5532 | lr=3.04289e-05 | grad_norm=2.9322\n",
            "interval_sec=15.71 | steps/sec=2.5463 | sec/step=0.3927\n",
            "examples_seen=3,993,600 | approx_epochs=2.6805\n",
            "batch_shapes input=(16, 29) labels=(16, 26)\n",
            "cuda_mem_alloc_gb=0.848 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.840 | chrf=24.020 | comet=nan | text_eval_sec=3.48\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=31200 val_loss=2.2841 | val_ppl=9.8172 | bleu=1.644 | chrf=22.100 | comet=nan | text_eval_sec=28.73\n",
            "========================================================================================\n",
            "step  31240/32000 | avg_loss=2.2659 | train_ppl=9.6396 | lr=3.03871e-05 | grad_norm=3.1359\n",
            "interval_sec=49.94 | steps/sec=0.8010 | sec/step=1.2484\n",
            "examples_seen=3,998,720 | approx_epochs=2.6840\n",
            "batch_shapes input=(16, 18) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.293 | chrf=22.397 | comet=nan | text_eval_sec=3.70\n",
            "========================================================================================\n",
            "step  31280/32000 | avg_loss=2.2468 | train_ppl=9.4575 | lr=3.03475e-05 | grad_norm=3.0402\n",
            "interval_sec=15.90 | steps/sec=2.5164 | sec/step=0.3974\n",
            "examples_seen=4,003,840 | approx_epochs=2.6874\n",
            "batch_shapes input=(16, 15) labels=(16, 13)\n",
            "cuda_mem_alloc_gb=0.822 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.025 | chrf=20.692 | comet=nan | text_eval_sec=3.59\n",
            "========================================================================================\n",
            "step  31320/32000 | avg_loss=2.1998 | train_ppl=9.0229 | lr=3.03099e-05 | grad_norm=2.9317\n",
            "interval_sec=15.78 | steps/sec=2.5344 | sec/step=0.3946\n",
            "examples_seen=4,008,960 | approx_epochs=2.6908\n",
            "batch_shapes input=(16, 13) labels=(16, 12)\n",
            "cuda_mem_alloc_gb=0.820 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.302 | chrf=19.945 | comet=nan | text_eval_sec=3.64\n",
            "========================================================================================\n",
            "step  31360/32000 | avg_loss=2.2779 | train_ppl=9.7559 | lr=3.02746e-05 | grad_norm=2.9994\n",
            "interval_sec=15.87 | steps/sec=2.5202 | sec/step=0.3968\n",
            "examples_seen=4,014,080 | approx_epochs=2.6943\n",
            "batch_shapes input=(16, 24) labels=(16, 26)\n",
            "cuda_mem_alloc_gb=0.847 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.816 | chrf=25.652 | comet=nan | text_eval_sec=3.70\n",
            "========================================================================================\n",
            "step  31400/32000 | avg_loss=2.2703 | train_ppl=9.6819 | lr=3.02413e-05 | grad_norm=3.1527\n",
            "interval_sec=16.06 | steps/sec=2.4903 | sec/step=0.4016\n",
            "examples_seen=4,019,200 | approx_epochs=2.6977\n",
            "batch_shapes input=(16, 23) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.548 | chrf=20.430 | comet=nan | text_eval_sec=3.67\n",
            "========================================================================================\n",
            "step  31440/32000 | avg_loss=2.2247 | train_ppl=9.2504 | lr=3.02102e-05 | grad_norm=3.1682\n",
            "interval_sec=15.84 | steps/sec=2.5252 | sec/step=0.3960\n",
            "examples_seen=4,024,320 | approx_epochs=2.7012\n",
            "batch_shapes input=(16, 25) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.721 | chrf=20.956 | comet=nan | text_eval_sec=3.60\n",
            "========================================================================================\n",
            "step  31480/32000 | avg_loss=2.2449 | train_ppl=9.4392 | lr=3.01813e-05 | grad_norm=2.8713\n",
            "interval_sec=15.86 | steps/sec=2.5220 | sec/step=0.3965\n",
            "examples_seen=4,029,440 | approx_epochs=2.7046\n",
            "batch_shapes input=(16, 32) labels=(16, 26)\n",
            "cuda_mem_alloc_gb=0.848 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.516 | chrf=22.826 | comet=nan | text_eval_sec=3.64\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_031500\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_026500\n",
            "========================================================================================\n",
            "step  31520/32000 | avg_loss=2.2275 | train_ppl=9.2769 | lr=3.01545e-05 | grad_norm=3.0049\n",
            "interval_sec=16.49 | steps/sec=2.4260 | sec/step=0.4122\n",
            "examples_seen=4,034,560 | approx_epochs=2.7080\n",
            "batch_shapes input=(16, 21) labels=(16, 21)\n",
            "cuda_mem_alloc_gb=0.838 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.644 | chrf=22.360 | comet=nan | text_eval_sec=5.48\n",
            "========================================================================================\n",
            "step  31560/32000 | avg_loss=2.2572 | train_ppl=9.5567 | lr=3.01298e-05 | grad_norm=3.1358\n",
            "interval_sec=15.69 | steps/sec=2.5502 | sec/step=0.3921\n",
            "examples_seen=4,039,680 | approx_epochs=2.7115\n",
            "batch_shapes input=(16, 21) labels=(16, 23)\n",
            "cuda_mem_alloc_gb=0.842 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.411 | chrf=19.631 | comet=nan | text_eval_sec=3.55\n",
            "========================================================================================\n",
            "step  31600/32000 | avg_loss=2.2599 | train_ppl=9.5825 | lr=3.01073e-05 | grad_norm=2.8624\n",
            "interval_sec=15.94 | steps/sec=2.5090 | sec/step=0.3986\n",
            "examples_seen=4,044,800 | approx_epochs=2.7149\n",
            "batch_shapes input=(16, 71) labels=(16, 60)\n",
            "cuda_mem_alloc_gb=0.914 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.705 | chrf=22.080 | comet=nan | text_eval_sec=3.72\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=31600 val_loss=2.2744 | val_ppl=9.7222 | bleu=1.567 | chrf=23.015 | comet=nan | text_eval_sec=28.66\n",
            "========================================================================================\n",
            "step  31640/32000 | avg_loss=2.2783 | train_ppl=9.7603 | lr=3.00869e-05 | grad_norm=3.2365\n",
            "interval_sec=50.03 | steps/sec=0.7996 | sec/step=1.2507\n",
            "examples_seen=4,049,920 | approx_epochs=2.7183\n",
            "batch_shapes input=(16, 22) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.114 | chrf=22.381 | comet=nan | text_eval_sec=3.55\n",
            "========================================================================================\n",
            "step  31680/32000 | avg_loss=2.2295 | train_ppl=9.2949 | lr=3.00687e-05 | grad_norm=3.2213\n",
            "interval_sec=15.61 | steps/sec=2.5629 | sec/step=0.3902\n",
            "examples_seen=4,055,040 | approx_epochs=2.7218\n",
            "batch_shapes input=(16, 22) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.843 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.578 | chrf=24.901 | comet=nan | text_eval_sec=3.61\n",
            "========================================================================================\n",
            "step  31720/32000 | avg_loss=2.2698 | train_ppl=9.6777 | lr=3.00526e-05 | grad_norm=3.1235\n",
            "interval_sec=15.82 | steps/sec=2.5290 | sec/step=0.3954\n",
            "examples_seen=4,060,160 | approx_epochs=2.7252\n",
            "batch_shapes input=(16, 27) labels=(16, 26)\n",
            "cuda_mem_alloc_gb=0.847 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.723 | chrf=24.180 | comet=nan | text_eval_sec=3.61\n",
            "========================================================================================\n",
            "step  31760/32000 | avg_loss=2.2631 | train_ppl=9.6131 | lr=3.00386e-05 | grad_norm=2.9055\n",
            "interval_sec=15.59 | steps/sec=2.5649 | sec/step=0.3899\n",
            "examples_seen=4,065,280 | approx_epochs=2.7286\n",
            "batch_shapes input=(16, 19) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.865 | chrf=23.176 | comet=nan | text_eval_sec=3.65\n",
            "========================================================================================\n",
            "step  31800/32000 | avg_loss=2.2571 | train_ppl=9.5557 | lr=3.00268e-05 | grad_norm=3.2405\n",
            "interval_sec=15.92 | steps/sec=2.5122 | sec/step=0.3981\n",
            "examples_seen=4,070,400 | approx_epochs=2.7321\n",
            "batch_shapes input=(16, 18) labels=(16, 11)\n",
            "cuda_mem_alloc_gb=0.819 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.274 | chrf=21.433 | comet=nan | text_eval_sec=3.59\n",
            "========================================================================================\n",
            "step  31840/32000 | avg_loss=2.2733 | train_ppl=9.7110 | lr=3.00172e-05 | grad_norm=3.1170\n",
            "interval_sec=15.96 | steps/sec=2.5064 | sec/step=0.3990\n",
            "examples_seen=4,075,520 | approx_epochs=2.7355\n",
            "batch_shapes input=(16, 16) labels=(16, 18)\n",
            "cuda_mem_alloc_gb=0.832 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.436 | chrf=20.900 | comet=nan | text_eval_sec=3.73\n",
            "========================================================================================\n",
            "step  31880/32000 | avg_loss=2.2990 | train_ppl=9.9642 | lr=3.00097e-05 | grad_norm=3.0992\n",
            "interval_sec=15.81 | steps/sec=2.5302 | sec/step=0.3952\n",
            "examples_seen=4,080,640 | approx_epochs=2.7390\n",
            "batch_shapes input=(16, 18) labels=(16, 20)\n",
            "cuda_mem_alloc_gb=0.836 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.673 | chrf=26.692 | comet=nan | text_eval_sec=3.68\n",
            "========================================================================================\n",
            "step  31920/32000 | avg_loss=2.2212 | train_ppl=9.2185 | lr=3.00043e-05 | grad_norm=3.2666\n",
            "interval_sec=15.81 | steps/sec=2.5306 | sec/step=0.3952\n",
            "examples_seen=4,085,760 | approx_epochs=2.7424\n",
            "batch_shapes input=(16, 18) labels=(16, 17)\n",
            "cuda_mem_alloc_gb=0.830 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=2.053 | chrf=25.149 | comet=nan | text_eval_sec=3.64\n",
            "========================================================================================\n",
            "step  31960/32000 | avg_loss=2.2651 | train_ppl=9.6321 | lr=3.00011e-05 | grad_norm=3.0037\n",
            "interval_sec=15.89 | steps/sec=2.5177 | sec/step=0.3972\n",
            "examples_seen=4,090,880 | approx_epochs=2.7458\n",
            "batch_shapes input=(16, 27) labels=(16, 24)\n",
            "cuda_mem_alloc_gb=0.844 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.798 | chrf=24.382 | comet=nan | text_eval_sec=3.78\n",
            "========================================================================================\n",
            "step  32000/32000 | avg_loss=2.2555 | train_ppl=9.5397 | lr=3e-05 | grad_norm=3.6615\n",
            "interval_sec=15.92 | steps/sec=2.5122 | sec/step=0.3981\n",
            "examples_seen=4,096,000 | approx_epochs=2.7493\n",
            "batch_shapes input=(16, 30) labels=(16, 29)\n",
            "cuda_mem_alloc_gb=0.853 | cuda_mem_peak_gb=2.754\n",
            "[text@log] samples=64 | bleu=1.319 | chrf=19.957 | comet=nan | text_eval_sec=3.66\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[eval] step=32000 val_loss=2.2853 | val_ppl=9.8289 | bleu=1.468 | chrf=21.602 | comet=nan | text_eval_sec=29.02\n",
            "[ckpt] saved: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_032000\n",
            "[ckpt] removed old checkpoint: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\step_027000\n",
            "========================================================================================\n",
            "Training finished at step 32000/32000\n",
            "Total elapsed sec: 19240.52\n",
            "Best val loss: 2.2382458706696826\n",
            "Train metrics CSV: c:\\My Projects\\en-ar-translation\\artifacts\\runs\\20260218_003516\\train_metrics.csv\n",
            "Eval metrics CSV: c:\\My Projects\\en-ar-translation\\artifacts\\runs\\20260218_003516\\eval_metrics.csv\n",
            "Best model dir: c:\\My Projects\\en-ar-translation\\checkpoints\\20260218_003516\\best_model\n"
          ]
        }
      ],
      "source": [
        "# Training state\n",
        "model.train()\n",
        "optimizer.zero_grad(set_to_none=True)\n",
        "train_metrics_log = []\n",
        "eval_metrics_log = []\n",
        "best_val_loss = float(\"inf\")\n",
        "optimizer_steps_done = 0\n",
        "micro_step = 0\n",
        "start_time = time.time()\n",
        "interval_start = start_time\n",
        "interval_raw_loss_sum = 0.0\n",
        "interval_micro_steps = 0\n",
        "interval_opt_steps = 0\n",
        "saved_ckpts = []\n",
        "\n",
        "while optimizer_steps_done < max_steps:\n",
        "    for batch in train_loader:\n",
        "        if optimizer_steps_done >= max_steps:\n",
        "            break\n",
        "\n",
        "        micro_step += 1\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        if use_fp16:\n",
        "            with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "                out = model(**batch)\n",
        "                scaled_loss = out.loss / gradient_accumulation_steps\n",
        "            grad_scaler.scale(scaled_loss).backward()\n",
        "        else:\n",
        "            out = model(**batch)\n",
        "            scaled_loss = out.loss / gradient_accumulation_steps\n",
        "            scaled_loss.backward()\n",
        "\n",
        "        raw_loss = float(out.loss.detach().cpu().item())\n",
        "        interval_raw_loss_sum += raw_loss\n",
        "        interval_micro_steps += 1\n",
        "\n",
        "        # One optimizer update happens after N micro-batches (gradient accumulation).\n",
        "        if micro_step % gradient_accumulation_steps == 0:\n",
        "            if use_fp16:\n",
        "                grad_scaler.unscale_(optimizer)\n",
        "            grad_norm = float(torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0).detach().cpu().item())\n",
        "\n",
        "            if use_fp16:\n",
        "                grad_scaler.step(optimizer)\n",
        "                grad_scaler.update()\n",
        "            else:\n",
        "                optimizer.step()\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            lr_scheduler.step()\n",
        "            optimizer_steps_done += 1\n",
        "            interval_opt_steps += 1\n",
        "\n",
        "            current_lr = float(optimizer.param_groups[0][\"lr\"])\n",
        "            examples_seen = int(optimizer_steps_done * per_device_train_batch_size * gradient_accumulation_steps)\n",
        "            approx_epochs = examples_seen / len(train_df) if len(train_df) else float(\"nan\")\n",
        "\n",
        "            # Interval logging is based on optimizer steps (not micro-steps).\n",
        "            if optimizer_steps_done % log_every_steps == 0 or optimizer_steps_done == 1:\n",
        "                interval_elapsed = time.time() - interval_start\n",
        "                avg_interval_loss = interval_raw_loss_sum / max(1, interval_micro_steps)\n",
        "                train_ppl = _loss_to_ppl(avg_interval_loss)\n",
        "                steps_per_sec = interval_opt_steps / interval_elapsed if interval_elapsed > 0 else 0.0\n",
        "                sec_per_step = 1.0 / steps_per_sec if steps_per_sec > 0 else float(\"inf\")\n",
        "\n",
        "                mem_alloc_gb = None\n",
        "                mem_peak_gb = None\n",
        "                if torch.cuda.is_available():\n",
        "                    mem_alloc_gb = torch.cuda.memory_allocated(device) / (1024**3)\n",
        "                    mem_peak_gb = torch.cuda.max_memory_allocated(device) / (1024**3)\n",
        "\n",
        "                print(\"=\" * 88)\n",
        "                print(f\"step {optimizer_steps_done:>6}/{max_steps} | avg_loss={avg_interval_loss:.4f} | train_ppl={train_ppl:.4f} | lr={current_lr:.6g} | grad_norm={grad_norm:.4f}\")\n",
        "                print(f\"interval_sec={interval_elapsed:.2f} | steps/sec={steps_per_sec:.4f} | sec/step={sec_per_step:.4f}\")\n",
        "                print(f\"examples_seen={examples_seen:,} | approx_epochs={approx_epochs:.4f}\")\n",
        "                print(f\"batch_shapes input={tuple(batch['input_ids'].shape)} labels={tuple(batch['labels'].shape)}\")\n",
        "                if mem_alloc_gb is not None:\n",
        "                    print(f\"cuda_mem_alloc_gb={mem_alloc_gb:.3f} | cuda_mem_peak_gb={mem_peak_gb:.3f}\")\n",
        "\n",
        "                text_log_t0 = time.time()\n",
        "                text_metrics_log = _eval_text_metrics(\n",
        "                    model,\n",
        "                    hf_tokenizer,\n",
        "                    val_df,\n",
        "                    device,\n",
        "                    per_device_train_batch_size,\n",
        "                    max_text_metric_batches_log,\n",
        "                    text_metric_num_beams,\n",
        "                    _seed=RANDOM_SEED + optimizer_steps_done,\n",
        "                    _bleu_metric=bleu_metric,\n",
        "                    _chrf_metric=chrf_metric,\n",
        "                    _comet_metric=comet_metric,\n",
        "                )\n",
        "                text_log_sec = time.time() - text_log_t0\n",
        "                print(\n",
        "                    f\"[text@log] samples={text_metrics_log['num_samples']} | bleu={text_metrics_log['bleu']:.3f} \"\n",
        "                    f\"| chrf={text_metrics_log['chrf']:.3f} | comet={text_metrics_log['comet']:.3f} \"\n",
        "                    f\"| text_eval_sec={text_log_sec:.2f}\"\n",
        "                )\n",
        "\n",
        "                train_metrics_log.append({\n",
        "                    \"step\": int(optimizer_steps_done),\n",
        "                    \"avg_interval_loss\": float(avg_interval_loss),\n",
        "                    \"train_ppl\": float(train_ppl),\n",
        "                    \"lr\": current_lr,\n",
        "                    \"grad_norm\": grad_norm,\n",
        "                    \"interval_sec\": float(interval_elapsed),\n",
        "                    \"steps_per_sec\": float(steps_per_sec),\n",
        "                    \"sec_per_step\": float(sec_per_step),\n",
        "                    \"examples_seen\": int(examples_seen),\n",
        "                    \"approx_epochs\": float(approx_epochs),\n",
        "                    \"batch_input_len\": int(batch[\"input_ids\"].shape[1]),\n",
        "                    \"batch_label_len\": int(batch[\"labels\"].shape[1]),\n",
        "                    \"cuda_mem_alloc_gb\": float(mem_alloc_gb) if mem_alloc_gb is not None else None,\n",
        "                    \"cuda_mem_peak_gb\": float(mem_peak_gb) if mem_peak_gb is not None else None,\n",
        "                    \"text_eval_sec\": float(text_log_sec),\n",
        "                    \"text_samples\": int(text_metrics_log[\"num_samples\"]),\n",
        "                    \"text_batches\": int(text_metrics_log[\"num_batches\"]),\n",
        "                    \"bleu\": float(text_metrics_log[\"bleu\"]),\n",
        "                    \"chrf\": float(text_metrics_log[\"chrf\"]),\n",
        "                    \"comet\": float(text_metrics_log[\"comet\"]),\n",
        "                    \"bleu_en_to_ar\": float(text_metrics_log[\"bleu_en_to_ar\"]),\n",
        "                    \"bleu_ar_to_en\": float(text_metrics_log[\"bleu_ar_to_en\"]),\n",
        "                    \"chrf_en_to_ar\": float(text_metrics_log[\"chrf_en_to_ar\"]),\n",
        "                    \"chrf_ar_to_en\": float(text_metrics_log[\"chrf_ar_to_en\"]),\n",
        "                })\n",
        "                pd.DataFrame(train_metrics_log).to_csv(train_metrics_path, index=False)\n",
        "\n",
        "                interval_start = time.time()\n",
        "                interval_raw_loss_sum = 0.0\n",
        "                interval_micro_steps = 0\n",
        "                interval_opt_steps = 0\n",
        "\n",
        "            # Run validation periodically and always at the final step.\n",
        "            should_eval = (optimizer_steps_done % eval_every_steps == 0) or (optimizer_steps_done == max_steps)\n",
        "            if should_eval:\n",
        "                val_loss = _eval_val_loss(model, val_loader, device, use_fp16, _max_batches=max_val_batches)\n",
        "                val_ppl = _loss_to_ppl(val_loss)\n",
        "                text_eval_t0 = time.time()\n",
        "                text_metrics_eval = _eval_text_metrics(\n",
        "                    model,\n",
        "                    hf_tokenizer,\n",
        "                    val_df,\n",
        "                    device,\n",
        "                    per_device_train_batch_size,\n",
        "                    max_text_metric_batches_eval,\n",
        "                    text_metric_num_beams,\n",
        "                    _seed=RANDOM_SEED + 10_000 + optimizer_steps_done,\n",
        "                    _bleu_metric=bleu_metric,\n",
        "                    _chrf_metric=chrf_metric,\n",
        "                    _comet_metric=comet_metric,\n",
        "                )\n",
        "                text_eval_sec = time.time() - text_eval_t0\n",
        "                print(\n",
        "                    f\"[eval] step={optimizer_steps_done} val_loss={val_loss:.4f} | val_ppl={val_ppl:.4f} | \"\n",
        "                    f\"bleu={text_metrics_eval['bleu']:.3f} | chrf={text_metrics_eval['chrf']:.3f} | \"\n",
        "                    f\"comet={text_metrics_eval['comet']:.3f} | text_eval_sec={text_eval_sec:.2f}\"\n",
        "                )\n",
        "                eval_metrics_log.append({\n",
        "                    \"step\": int(optimizer_steps_done),\n",
        "                    \"val_loss\": float(val_loss),\n",
        "                    \"val_ppl\": float(val_ppl),\n",
        "                    \"lr\": current_lr,\n",
        "                    \"text_eval_sec\": float(text_eval_sec),\n",
        "                    \"text_samples\": int(text_metrics_eval[\"num_samples\"]),\n",
        "                    \"text_batches\": int(text_metrics_eval[\"num_batches\"]),\n",
        "                    \"bleu\": float(text_metrics_eval[\"bleu\"]),\n",
        "                    \"chrf\": float(text_metrics_eval[\"chrf\"]),\n",
        "                    \"comet\": float(text_metrics_eval[\"comet\"]),\n",
        "                    \"bleu_en_to_ar\": float(text_metrics_eval[\"bleu_en_to_ar\"]),\n",
        "                    \"bleu_ar_to_en\": float(text_metrics_eval[\"bleu_ar_to_en\"]),\n",
        "                    \"chrf_en_to_ar\": float(text_metrics_eval[\"chrf_en_to_ar\"]),\n",
        "                    \"chrf_ar_to_en\": float(text_metrics_eval[\"chrf_ar_to_en\"]),\n",
        "                })\n",
        "                pd.DataFrame(eval_metrics_log).to_csv(eval_metrics_path, index=False)\n",
        "\n",
        "                if np.isfinite(val_loss) and val_loss < best_val_loss:\n",
        "                    best_val_loss = float(val_loss)\n",
        "                    _save_checkpoint(model, hf_tokenizer, optimizer, lr_scheduler, grad_scaler, optimizer_steps_done, val_loss, best_dir)\n",
        "                    print(f\"[best] new best val_loss={best_val_loss:.4f} saved to {best_dir}\")\n",
        "\n",
        "            # Save periodic checkpoints independently from validation schedule.\n",
        "            should_save = (optimizer_steps_done % save_every_steps == 0) or (optimizer_steps_done == max_steps)\n",
        "            if should_save:\n",
        "                ckpt_dir = ckpt_root / f\"step_{optimizer_steps_done:06d}\"\n",
        "                _save_checkpoint(model, hf_tokenizer, optimizer, lr_scheduler, grad_scaler, optimizer_steps_done, None, ckpt_dir)\n",
        "                saved_ckpts.append(ckpt_dir)\n",
        "                print(f\"[ckpt] saved: {ckpt_dir}\")\n",
        "\n",
        "                # Optional retention policy to limit disk usage.\n",
        "                if keep_last_n_checkpoints is not None and keep_last_n_checkpoints > 0:\n",
        "                    while len(saved_ckpts) > keep_last_n_checkpoints:\n",
        "                        old = saved_ckpts.pop(0)\n",
        "                        if old.exists():\n",
        "                            shutil.rmtree(old)\n",
        "                            print(f\"[ckpt] removed old checkpoint: {old}\")\n",
        "\n",
        "total_elapsed = time.time() - start_time\n",
        "print(\"=\" * 88)\n",
        "print(f\"Training finished at step {optimizer_steps_done}/{max_steps}\")\n",
        "print(f\"Total elapsed sec: {total_elapsed:.2f}\")\n",
        "print(f\"Best val loss: {best_val_loss if np.isfinite(best_val_loss) else None}\")\n",
        "print(f\"Train metrics CSV: {train_metrics_path}\")\n",
        "print(f\"Eval metrics CSV: {eval_metrics_path}\")\n",
        "print(f\"Best model dir: {best_dir}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
